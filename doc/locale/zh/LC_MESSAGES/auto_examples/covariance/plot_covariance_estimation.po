# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:8
msgid "Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood"
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:10
msgid ""
"When working with covariance estimation, the usual approach is to use a "
"maximum likelihood estimator, such as the "
":class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it "
"converges to the true (population) covariance when given many "
"observations. However, it can also be beneficial to regularize it, in "
"order to reduce its variance; this, in turn, introduces some bias. This "
"example illustrates the simple regularization used in "
":ref:`shrunk_covariance` estimators. In particular, it focuses on how to "
"set the amount of regularization, i.e. how to choose the bias-variance "
"trade-off."
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:21
msgid "Here we compare 3 approaches:"
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:23
msgid ""
"Setting the parameter by cross-validating the likelihood on three folds "
"according to a grid of potential shrinkage parameters."
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:26
msgid ""
"A close formula proposed by Ledoit and Wolf to compute the asymptotically"
" optimal regularization parameter (minimizing a MSE criterion), yielding "
"the :class:`sklearn.covariance.LedoitWolf` covariance estimate."
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:31
msgid ""
"An improvement of the Ledoit-Wolf shrinkage, the "
":class:`sklearn.covariance.OAS`, proposed by Chen et al. Its convergence "
"is significantly better under the assumption that the data are Gaussian, "
"in particular for small samples."
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:36
msgid ""
"To quantify estimation error, we plot the likelihood of unseen data for "
"different values of the shrinkage parameter. We also show the choices by "
"cross-validation, or with the LedoitWolf and OAS estimates."
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:40
msgid ""
"Note that the maximum likelihood estimate corresponds to no shrinkage, "
"and thus performs poorly. The Ledoit-Wolf estimate performs really well, "
"as it is close to the optimal and is computational not costly. In this "
"example, the OAS estimate is a bit further away. Interestingly, both "
"approaches outperform cross-validation, which is significantly most "
"computationally costly."
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:56
msgid ""
"**Python source code:** :download:`plot_covariance_estimation.py "
"<plot_covariance_estimation.py>`"
msgstr ""

#: ../../auto_examples/covariance/plot_covariance_estimation.rst:61
msgid ""
"**Total running time of the example:**  0.89 seconds ( 0 minutes  0.89 "
"seconds)"
msgstr ""

