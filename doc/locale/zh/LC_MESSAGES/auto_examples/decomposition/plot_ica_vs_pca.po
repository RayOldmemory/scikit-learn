# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:8
msgid "FastICA on 2D point clouds"
msgstr ""

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:10
msgid ""
"This example illustrates visually in the feature space a comparison by "
"results using two different component analysis techniques."
msgstr ""

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:13
msgid ":ref:`ICA` vs :ref:`PCA`."
msgstr ""

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:15
msgid ""
"Representing ICA in the feature space gives the view of 'geometric ICA': "
"ICA is an algorithm that finds directions in the feature space "
"corresponding to projections with high non-Gaussianity. These directions "
"need not be orthogonal in the original feature space, but they are "
"orthogonal in the whitened feature space, in which all directions "
"correspond to the same variance."
msgstr ""

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:22
msgid ""
"PCA, on the other hand, finds orthogonal directions in the raw feature "
"space that correspond to directions accounting for maximum variance."
msgstr ""

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:25
msgid ""
"Here we simulate independent sources using a highly non-Gaussian process,"
" 2 student T with a low number of degrees of freedom (top left figure). "
"We mix them to create observations (top right figure). In this raw "
"observation space, directions identified by PCA are represented by orange"
" vectors. We represent the signal in the PCA space, after whitening by "
"the variance corresponding to the PCA vectors (lower left). Running ICA "
"corresponds to finding a rotation in this space to identify the "
"directions of largest non-Gaussianity (lower right)."
msgstr ""

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:42
msgid ""
"**Python source code:** :download:`plot_ica_vs_pca.py "
"<plot_ica_vs_pca.py>`"
msgstr ""

#: ../../auto_examples/decomposition/plot_ica_vs_pca.rst:47
msgid ""
"**Total running time of the example:**  1.13 seconds ( 0 minutes  1.13 "
"seconds)"
msgstr ""

