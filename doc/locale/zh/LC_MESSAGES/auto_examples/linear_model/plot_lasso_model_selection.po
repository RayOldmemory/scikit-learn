# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:8
msgid "Lasso model selection: Cross-Validation / AIC / BIC"
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:10
msgid ""
"Use the Akaike information criterion (AIC), the Bayes Information "
"criterion (BIC) and cross-validation to select an optimal value of the "
"regularization parameter alpha of the :ref:`lasso` estimator."
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:14
msgid "Results obtained with LassoLarsIC are based on AIC/BIC criteria."
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:16
msgid ""
"Information-criterion based model selection is very fast, but it relies "
"on a proper estimation of degrees of freedom, are derived for large "
"samples (asymptotic results) and assume the model is correct, i.e. that "
"the data are actually generated by this model. They also tend to break "
"when the problem is badly conditioned (more features than samples)."
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:23
msgid ""
"For cross-validation, we use 20-fold with 2 algorithms to compute the "
"Lasso path: coordinate descent, as implemented by the LassoCV class, and "
"Lars (least angle regression) as implemented by the LassoLarsCV class. "
"Both algorithms give roughly the same results. They differ with regards "
"to their execution speed and sources of numerical errors."
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:29
msgid ""
"Lars computes a path solution only for each kink in the path. As a "
"result, it is very efficient when there are only of few kinks, which is "
"the case if there are few features or samples. Also, it is able to "
"compute the full path without setting any meta parameter. On the "
"opposite, coordinate descent compute the path points on a pre-specified "
"grid (here we use the default). Thus it is more efficient if the number "
"of grid points is smaller than the number of kinks in the path. Such a "
"strategy can be interesting if the number of features is really large and"
" there are enough samples to select a large amount. In terms of numerical"
" errors, for heavily correlated variables, Lars will accumulate more "
"errors, while the coordinate descent algorithm will only sample the path "
"on a grid."
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:42
msgid ""
"Note how the optimal value of alpha varies for each fold. This "
"illustrates why nested-cross validation is necessary when trying to "
"evaluate the performance of a method for which a parameter is chosen by "
"cross-validation: this choice of parameter may not be optimal for unseen "
"data."
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:69
msgid "**Script output**::"
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:76
msgid ""
"**Python source code:** :download:`plot_lasso_model_selection.py "
"<plot_lasso_model_selection.py>`"
msgstr ""

#: ../../auto_examples/linear_model/plot_lasso_model_selection.rst:81
msgid ""
"**Total running time of the example:**  1.69 seconds ( 0 minutes  1.69 "
"seconds)"
msgstr ""

