# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../auto_examples/model_selection/plot_precision_recall.rst:8
msgid "Precision-Recall"
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:10
msgid "Example of Precision-Recall metric to evaluate classifier output quality."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:12
msgid ""
"In information retrieval, precision is a measure of result relevancy, "
"while recall is a measure of how many truly relevant results are "
"returned. A high area under the curve represents both high recall and "
"high precision, where high precision relates to a low false positive "
"rate, and high recall relates to a low false negative rate. High scores "
"for both show that the classifier is returning accurate results (high "
"precision), as well as returning a majority of all positive results (high"
" recall)."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:20
msgid ""
"A system with high recall but low precision returns many results, but "
"most of its predicted labels are incorrect when compared to the training "
"labels. A system with high precision but low recall is just the opposite,"
" returning very few results, but most of its predicted labels are correct"
" when compared to the training labels. An ideal system with high "
"precision and high recall will return many results, with all results "
"labeled correctly."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:27
msgid ""
"Precision (:math:`P`) is defined as the number of true positives "
"(:math:`T_p`) over the number of true positives plus the number of false "
"positives (:math:`F_p`)."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:31
msgid ":math:`P = \\frac{T_p}{T_p+F_p}`"
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:33
msgid ""
"Recall (:math:`R`) is defined as the number of true positives "
"(:math:`T_p`) over the number of true positives plus the number of false "
"negatives (:math:`F_n`)."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:37
msgid ":math:`R = \\frac{T_p}{T_p + F_n}`"
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:39
msgid ""
"These quantities are also related to the (:math:`F_1`) score, which is "
"defined as the harmonic mean of precision and recall."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:42
msgid ":math:`F1 = 2\\frac{P \\times R}{P+R}`"
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:44
msgid ""
"It is important to note that the precision may not decrease with recall. "
"The definition of precision (:math:`\\frac{T_p}{T_p + F_p}`) shows that "
"lowering the threshold of a classifier may increase the denominator, by "
"increasing the number of results returned. If the threshold was "
"previously set too high, the new results may all be true positives, which"
" will increase precision. If the previous threshold was about right or "
"too low, further lowering the threshold will introduce false positives, "
"decreasing precision."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:52
msgid ""
"Recall is defined as :math:`\\frac{T_p}{T_p+F_n}`, where :math:`T_p+F_n` "
"does not depend on the classifier threshold. This means that lowering the"
" classifier threshold may increase recall, by increasing the number of "
"true positive results. It is also possible that lowering the threshold "
"may leave recall unchanged, while the precision fluctuates."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:58
msgid ""
"The relationship between recall and precision can be observed in the "
"stairstep area of the plot - at the edges of these steps a small change "
"in the threshold considerably reduces precision, with only a minor gain "
"in recall. See the corner at recall = .59, precision = .8 for an example "
"of this phenomenon."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:64
msgid ""
"Precision-recall curves are typically used in binary classification to "
"study the output of a classifier. In order to extend Precision-recall "
"curve and average precision to multi-class or multi-label classification,"
" it is necessary to binarize the output. One curve can be drawn per "
"label, but one can also draw a precision-recall curve by considering each"
" element of the label indicator matrix as a binary prediction (micro-"
"averaging)."
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:75
msgid "See also :func:`sklearn.metrics.average_precision_score`,"
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:74
msgid ""
":func:`sklearn.metrics.recall_score`, "
":func:`sklearn.metrics.precision_score`, :func:`sklearn.metrics.f1_score`"
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:86
msgid ""
"**Python source code:** :download:`plot_precision_recall.py "
"<plot_precision_recall.py>`"
msgstr ""

#: ../../auto_examples/model_selection/plot_precision_recall.rst:91
msgid ""
"**Total running time of the example:**  0.47 seconds ( 0 minutes  0.47 "
"seconds)"
msgstr ""

