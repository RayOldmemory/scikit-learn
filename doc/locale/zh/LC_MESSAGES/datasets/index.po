# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../datasets/index.rst:5
msgid "Dataset loading utilities"
msgstr ""

#: ../../datasets/index.rst:9
msgid ""
"The ``sklearn.datasets`` package embeds some small toy datasets as "
"introduced in the :ref:`Getting Started <loading_example_dataset>` "
"section."
msgstr ""

#: ../../datasets/index.rst:12
msgid ""
"To evaluate the impact of the scale of the dataset (``n_samples`` and "
"``n_features``) while controlling the statistical properties of the data "
"(typically the correlation and informativeness of the features), it is "
"also possible to generate synthetic data."
msgstr ""

#: ../../datasets/index.rst:17
msgid ""
"This package also features helpers to fetch larger datasets commonly used"
" by the machine learning community to benchmark algorithm on data that "
"comes from the 'real world'."
msgstr ""

#: ../../datasets/index.rst:22
msgid "General dataset API"
msgstr ""

#: ../../datasets/index.rst:24
msgid ""
"There are three distinct kinds of dataset interfaces for different types "
"of datasets. The simplest one is the interface for sample images, which "
"is described below in the :ref:`sample_images` section."
msgstr ""

#: ../../datasets/index.rst:29
msgid ""
"The dataset generation functions and the svmlight loader share a "
"simplistic interface, returning a tuple ``(X, y)`` consisting of a "
"``n_samples`` * ``n_features`` numpy array ``X`` and an array of length "
"``n_samples`` containing the targets ``y``."
msgstr ""

#: ../../datasets/index.rst:34
msgid ""
"The toy datasets as well as the 'real world' datasets and the datasets "
"fetched from mldata.org have more sophisticated structure. These "
"functions return a dictionary-like object holding at least two items: an "
"array of shape ``n_samples`` * ``n_features`` with key ``data`` (except "
"for 20newsgroups) and a numpy array of length ``n_samples``, containing "
"the target values, with key ``target``."
msgstr ""

#: ../../datasets/index.rst:42
msgid ""
"The datasets also contain a description in ``DESCR`` and some contain "
"``feature_names`` and ``target_names``. See the dataset descriptions "
"below for details."
msgstr ""

#: ../../datasets/index.rst:48
msgid "Toy datasets"
msgstr ""

#: ../../datasets/index.rst:50
msgid ""
"scikit-learn comes with a few small standard datasets that do not require"
" to download any file from some external website."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`load_boston <sklearn.datasets.load_boston>`\\ ()"
msgstr ""

#: ../../<autosummary>:1
msgid "Load and return the boston house-prices dataset (regression)."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`load_iris <sklearn.datasets.load_iris>`\\ ()"
msgstr ""

#: ../../<autosummary>:1
msgid "Load and return the iris dataset (classification)."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`load_diabetes <sklearn.datasets.load_diabetes>`\\ ()"
msgstr ""

#: ../../<autosummary>:1
msgid "Load and return the diabetes dataset (regression)."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`load_digits <sklearn.datasets.load_digits>`\\ ([n_class])"
msgstr ""

#: ../../<autosummary>:1
msgid "Load and return the digits dataset (classification)."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`load_linnerud <sklearn.datasets.load_linnerud>`\\ ()"
msgstr ""

#: ../../<autosummary>:1
msgid "Load and return the linnerud dataset (multivariate regression)."
msgstr ""

#: ../../datasets/index.rst:64
msgid ""
"These datasets are useful to quickly illustrate the behavior of the "
"various algorithms implemented in the scikit. They are however often too "
"small to be representative of real world machine learning tasks."
msgstr ""

#: ../../datasets/index.rst:71
msgid "Sample images"
msgstr ""

#: ../../datasets/index.rst:73
msgid ""
"The scikit also embed a couple of sample JPEG images published under "
"Creative Commons license by their authors. Those image can be useful to "
"test algorithms and pipeline on 2D data."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`load_sample_images <sklearn.datasets.load_sample_images>`\\ ()"
msgstr ""

#: ../../<autosummary>:1
msgid "Load sample images for image manipulation."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`load_sample_image <sklearn.datasets.load_sample_image>`\\ "
"(image_name)"
msgstr ""

#: ../../<autosummary>:1
msgid "Load the numpy array of a single sample image"
msgstr ""

#: ../../datasets/index.rst:93
msgid ""
"The default coding of images is based on the ``uint8`` dtype to spare "
"memory.  Often machine learning algorithms work best if the input is "
"converted to a floating point representation first.  Also, if you plan to"
" use ``pylab.imshow`` don't forget to scale to the range 0 - 1 as done in"
" the following example."
msgstr ""

#: ../../datasets/index.rst
msgid "Examples:"
msgstr ""

#: ../../datasets/index.rst:101
msgid ":ref:`example_cluster_plot_color_quantization.py`"
msgstr ""

#: ../../datasets/index.rst:107
msgid "Sample generators"
msgstr ""

#: ../../datasets/index.rst:109
msgid ""
"In addition, scikit-learn includes various random sample generators that "
"can be used to build artificial datasets of controlled size and "
"complexity."
msgstr ""

#: ../../datasets/index.rst:113
msgid "Generators for classification and clustering"
msgstr ""

#: ../../datasets/index.rst:115
msgid ""
"These generators produce a matrix of features and corresponding discrete "
"targets."
msgstr ""

#: ../../datasets/index.rst:119
msgid "Single label"
msgstr ""

#: ../../datasets/index.rst:121
msgid ""
"Both :func:`make_blobs` and :func:`make_classification` create multiclass"
" datasets by allocating each class one or more normally-distributed "
"clusters of points.  :func:`make_blobs` provides greater control "
"regarding the centers and standard deviations of each cluster, and is "
"used to demonstrate clustering. :func:`make_classification` specialises "
"in introducing noise by way of: correlated, redundant and uninformative "
"features; multiple Gaussian clusters per class; and linear "
"transformations of the feature space."
msgstr ""

#: ../../datasets/index.rst:129
msgid ""
":func:`make_gaussian_quantiles` divides a single Gaussian cluster into "
"near-equal-size classes separated by concentric hyperspheres. "
":func:`make_hastie_10_2` generates a similar binary, 10-dimensional "
"problem."
msgstr ""

#: ../../datasets/index.rst:138
msgid ""
":func:`make_circles` and :func:`make_moons` generate 2d binary "
"classification datasets that are challenging to certain algorithms (e.g. "
"centroid-based clustering or linear classification), including optional "
"Gaussian noise. They are useful for visualisation. produces Gaussian data"
" with a spherical decision boundary for binary classification."
msgstr ""

#: ../../datasets/index.rst:145
msgid "Multilabel"
msgstr ""

#: ../../datasets/index.rst:147
msgid ""
":func:`make_multilabel_classification` generates random samples with "
"multiple labels, reflecting a bag of words drawn from a mixture of "
"topics. The number of topics for each document is drawn from a Poisson "
"distribution, and the topics themselves are drawn from a fixed random "
"distribution. Similarly, the number of words is drawn from Poisson, with "
"words drawn from a multinomial, where each topic defines a probability "
"distribution over words. Simplifications with respect to true bag-of-"
"words mixtures include:"
msgstr ""

#: ../../datasets/index.rst:155
msgid ""
"Per-topic word distributions are independently drawn, where in reality "
"all would be affected by a sparse base distribution, and would be "
"correlated."
msgstr ""

#: ../../datasets/index.rst:157
msgid ""
"For a document generated from multiple topics, all topics are weighted "
"equally in generating its bag of words."
msgstr ""

#: ../../datasets/index.rst:159
msgid ""
"Documents without labels words at random, rather than from a base "
"distribution."
msgstr ""

#: ../../datasets/index.rst:168
msgid "Biclustering"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_biclusters <sklearn.datasets.make_biclusters>`\\ (shape, "
"n_clusters[, noise, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate an array with constant block diagonal structure for biclustering."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_checkerboard <sklearn.datasets.make_checkerboard>`\\ (shape, "
"n_clusters[, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate an array with block checkerboard structure for biclustering."
msgstr ""

#: ../../datasets/index.rst:180
msgid "Generators for regression"
msgstr ""

#: ../../datasets/index.rst:182
msgid ""
":func:`make_regression` produces regression targets as an optionally-"
"sparse random linear combination of random features, with noise. Its "
"informative features may be uncorrelated, or low rank (few features "
"account for most of the variance)."
msgstr ""

#: ../../datasets/index.rst:187
msgid ""
"Other regression generators generate functions deterministically from "
"randomized features.  :func:`make_sparse_uncorrelated` produces a target "
"as a linear combination of four features with fixed coefficients. Others "
"encode explicitly non-linear relations: :func:`make_friedman1` is related"
" by polynomial and sine transforms; :func:`make_friedman2` includes "
"feature multiplication and reciprocation; and :func:`make_friedman3` is "
"similar with an arctan transformation on the target."
msgstr ""

#: ../../datasets/index.rst:196
msgid "Generators for manifold learning"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_s_curve <sklearn.datasets.make_s_curve>`\\ ([n_samples, noise,"
" random_state])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate an S curve dataset."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_swiss_roll <sklearn.datasets.make_swiss_roll>`\\ ([n_samples, "
"noise, random_state])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate a swiss roll dataset."
msgstr ""

#: ../../datasets/index.rst:207
msgid "Generators for decomposition"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_low_rank_matrix <sklearn.datasets.make_low_rank_matrix>`\\ "
"([n_samples, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate a mostly low rank matrix with bell-shaped singular values"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_sparse_coded_signal "
"<sklearn.datasets.make_sparse_coded_signal>`\\ (n_samples, ...[, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate a signal as a sparse combination of dictionary elements."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_spd_matrix <sklearn.datasets.make_spd_matrix>`\\ (n_dim[, "
"random_state])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate a random symmetric, positive-definite matrix."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`make_sparse_spd_matrix <sklearn.datasets.make_sparse_spd_matrix>`\\"
" ([dim, alpha, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Generate a sparse symmetric definite positive matrix."
msgstr ""

#: ../../datasets/index.rst:223
msgid "Datasets in svmlight / libsvm format"
msgstr ""

#: ../../datasets/index.rst:225
msgid ""
"scikit-learn includes utility functions for loading datasets in the "
"svmlight / libsvm format. In this format, each line takes the form "
"``<label> <feature-id>:<feature-value> <feature-id>:<feature-value> "
"...``. This format is especially suitable for sparse datasets. In this "
"module, scipy sparse CSR matrices are used for ``X`` and numpy arrays are"
" used for ``y``."
msgstr ""

#: ../../datasets/index.rst:231
msgid "You may load a dataset like as follows::"
msgstr ""

#: ../../datasets/index.rst:237
msgid "You may also load two (or more) datasets at once::"
msgstr ""

#: ../../datasets/index.rst:243
msgid ""
"In this case, ``X_train`` and ``X_test`` are guaranteed to have the same "
"number of features. Another way to achieve the same result is to fix the "
"number of features::"
msgstr ""

#: ../../datasets/index.rst
msgid "Related links:"
msgstr ""

#: ../../datasets/index.rst:253
msgid ""
"_`Public datasets in svmlight / libsvm format`: "
"http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/"
msgstr ""

#: ../../datasets/index.rst:255
msgid ""
"_`Faster API-compatible implementation`: https://github.com/mblondel"
"/svmlight-loader"
msgstr ""

#: ../../datasets/olivetti_faces.rst:5
msgid "The Olivetti faces dataset"
msgstr ""

#: ../../datasets/olivetti_faces.rst:8
msgid ""
"This dataset contains a set of face images taken between April 1992 and "
"April 1994 at AT&T Laboratories Cambridge. The website describing the "
"original dataset is now defunct, but archived copies can be accessed "
"through `the Internet Archive's Wayback Machine`_. The "
":func:`sklearn.datasets.fetch_olivetti_faces` function is the data "
"fetching / caching function that downloads the data archive from AT&T."
msgstr ""

#: ../../datasets/olivetti_faces.rst:18
msgid "As described on the original website:"
msgstr ""

#: ../../datasets/olivetti_faces.rst:20
msgid ""
"There are ten different images of each of 40 distinct subjects. For some "
"subjects, the images were taken at different times, varying the lighting,"
" facial expressions (open / closed eyes, smiling / not smiling) and "
"facial details (glasses / no glasses). All the images were taken against "
"a dark homogeneous background with the subjects in an upright, frontal "
"position (with tolerance for some side movement)."
msgstr ""

#: ../../datasets/olivetti_faces.rst:27
msgid ""
"The image is quantized to 256 grey levels and stored as unsigned 8-bit "
"integers; the loader will convert these to floating point values on the "
"interval [0, 1], which are easier to work with for many algorithms."
msgstr ""

#: ../../datasets/olivetti_faces.rst:31
msgid ""
"The \"target\" for this database is an integer from 0 to 39 indicating "
"the identity of the person pictured; however, with only 10 examples per "
"class, this relatively small dataset is more interesting from an "
"unsupervised or semi-supervised perspective."
msgstr ""

#: ../../datasets/olivetti_faces.rst:36
msgid ""
"The original dataset consisted of 92 x 112, while the version available "
"here consists of 64x64 images."
msgstr ""

#: ../../datasets/olivetti_faces.rst:39
msgid ""
"When using these images, please give credit to AT&T Laboratories "
"Cambridge."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:4
msgid "The 20 newsgroups text dataset"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:6
msgid ""
"The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 "
"topics split in two subsets: one for training (or development) and the "
"other one for testing (or for performance evaluation). The split between "
"the train and test set is based upon a messages posted before and after a"
" specific date."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:12
msgid ""
"This module contains two loaders. The first one, "
":func:`sklearn.datasets.fetch_20newsgroups`, returns a list of the raw "
"texts that can be fed to text feature extractors such as "
":class:`sklearn.feature_extraction.text.CountVectorizer` with custom "
"parameters so as to extract feature vectors. The second one, "
":func:`sklearn.datasets.fetch_20newsgroups_vectorized`, returns ready-to-"
"use features, i.e., it is not necessary to use a feature extractor."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:22 ../../datasets/labeled_faces.rst:28
msgid "Usage"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:24
msgid ""
"The :func:`sklearn.datasets.fetch_20newsgroups` function is a data "
"fetching / caching functions that downloads the data archive from the "
"original `20 newsgroups website`_, extracts the archive contents in the "
"``~/scikit_learn_data/20news_home`` folder and calls the "
":func:`sklearn.datasets.load_files` on either the training or testing set"
" folder, or both of them::"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:57
msgid ""
"The real data lies in the ``filenames`` and ``target`` attributes. The "
"target attribute is the integer index of the category::"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:67
msgid ""
"It is possible to load only a sub-selection of the categories by passing "
"the list of the categories to load to the "
":func:`sklearn.datasets.fetch_20newsgroups` function::"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:84
msgid "Converting text to vectors"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:86
msgid ""
"In order to feed predictive or clustering models with the text data, one "
"first need to turn the text into vectors of numerical values suitable for"
" statistical analysis. This can be achieved with the utilities of the "
"``sklearn.feature_extraction.text`` as demonstrated in the following "
"example that extract `TF-IDF`_ vectors of unigram tokens from a subset of"
" 20news::"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:103
msgid ""
"The extracted TF-IDF vectors are very sparse, with an average of 159 non-"
"zero components by sample in a more than 30000-dimensional space (less "
"than .5% non-zero features)::"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:110
msgid ""
":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function "
"which returns ready-to-use tfidf features instead of file names."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:118
msgid "Filtering text for more realistic training"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:119
msgid ""
"It is easy for a classifier to overfit on particular things that appear "
"in the 20 Newsgroups data, such as newsgroup headers. Many classifiers "
"achieve very high F-scores, but their results would not generalize to "
"other documents that aren't from this window of time."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:124
msgid ""
"For example, let's look at the results of a multinomial Naive Bayes "
"classifier, which is fast to train and achieves a decent F-score::"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:138
msgid ""
"(The example :ref:`example_text_document_classification_20newsgroups.py` "
"shuffles the training and test data, instead of segmenting by time, and "
"in that case multinomial Naive Bayes gets a much higher F-score of 0.88. "
"Are you suspicious yet of what's going on inside this classifier?)"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:143
msgid "Let's take a look at what the most informative features are:"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:158
msgid "You can now see many things that these features have overfit to:"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:160
msgid ""
"Almost every group is distinguished by whether headers such as ``NNTP-"
"Posting-Host:`` and ``Distribution:`` appear more or less often."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:162
msgid ""
"Another significant feature involves whether the sender is affiliated "
"with a university, as indicated either by their headers or their "
"signature."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:164
msgid ""
"The word \"article\" is a significant feature, based on how often people "
"quote previous posts like this: \"In article [article ID], [name] "
"<[e-mail address]> wrote:\""
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:167
msgid ""
"Other features match the names and e-mail addresses of particular people "
"who were posting at the time."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:170
msgid ""
"With such an abundance of clues that distinguish newsgroups, the "
"classifiers barely have to identify topics from text at all, and they all"
" perform at the same high level."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:174
msgid ""
"For this reason, the functions that load 20 Newsgroups data provide a "
"parameter called **remove**, telling it what kinds of information to "
"strip out of each file. **remove** should be a tuple containing any "
"subset of ``('headers', 'footers', 'quotes')``, telling it to remove "
"headers, signature blocks, and quotation blocks respectively."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:188
msgid ""
"This classifier lost over a lot of its F-score, just because we removed "
"metadata that has little to do with topic classification. It loses even "
"more if we also strip this metadata from the training data:"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:203
msgid ""
"Some other classifiers cope better with this harder version of the task. "
"Try running "
":ref:`example_model_selection_grid_search_text_feature_extraction.py` "
"with and without the ``--filter`` option to compare the results."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst
msgid "Recommendation"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:209
msgid ""
"When evaluating text classifiers on the 20 Newsgroups data, you should "
"strip newsgroup-related metadata. In scikit-learn, you can do this by "
"setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be "
"lower because it is more realistic."
msgstr ""

#: ../../datasets/twenty_newsgroups.rst ../../datasets/labeled_faces.rst:116
msgid "Examples"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:216
msgid ":ref:`example_model_selection_grid_search_text_feature_extraction.py`"
msgstr ""

#: ../../datasets/twenty_newsgroups.rst:218
msgid ":ref:`example_text_document_classification_20newsgroups.py`"
msgstr ""

#: ../../datasets/mldata.rst:10
msgid "Downloading datasets from the mldata.org repository"
msgstr ""

#: ../../datasets/mldata.rst:12
msgid ""
"`mldata.org <http://mldata.org>`_ is a public repository for machine "
"learning data, supported by the `PASCAL network <http://www.pascal-"
"network.org>`_ ."
msgstr ""

#: ../../datasets/mldata.rst:15
msgid ""
"The ``sklearn.datasets`` package is able to directly download data sets "
"from the repository using the function "
":func:`sklearn.datasets.fetch_mldata`."
msgstr ""

#: ../../datasets/mldata.rst:19
msgid "For example, to download the MNIST digit recognition database::"
msgstr ""

#: ../../datasets/mldata.rst:24
msgid ""
"The MNIST database contains a total of 70000 examples of handwritten "
"digits of size 28x28 pixels, labeled from 0 to 9::"
msgstr ""

#: ../../datasets/mldata.rst:34
msgid ""
"After the first download, the dataset is cached locally in the path "
"specified by the ``data_home`` keyword argument, which defaults to "
"``~/scikit_learn_data/``::"
msgstr ""

#: ../../datasets/mldata.rst:41
msgid ""
"Data sets in `mldata.org <http://mldata.org>`_ do not adhere to a strict "
"naming or formatting convention. :func:`sklearn.datasets.fetch_mldata` is"
" able to make sense of the most common cases, but allows to tailor the "
"defaults to individual datasets:"
msgstr ""

#: ../../datasets/mldata.rst:46
msgid ""
"The data arrays in `mldata.org <http://mldata.org>`_ are most often "
"shaped as ``(n_features, n_samples)``. This is the opposite of the "
"``scikit-learn`` convention, so :func:`sklearn.datasets.fetch_mldata` "
"transposes the matrix by default. The ``transpose_data`` keyword controls"
" this behavior::"
msgstr ""

#: ../../datasets/mldata.rst:60
msgid ""
"For datasets with multiple columns, :func:`sklearn.datasets.fetch_mldata`"
" tries to identify the target and data columns and rename them to "
"``target`` and ``data``. This is done by looking for arrays named "
"``label`` and ``data`` in the dataset, and failing that by choosing the "
"first array to be ``target`` and the second to be ``data``. This behavior"
" can be changed with the ``target_name`` and ``data_name`` keywords, "
"setting them to a specific name or index number (the name and order of "
"the columns in the datasets can be found at its `mldata.org "
"<http://mldata.org>`_ under the tab \"Data\"::"
msgstr ""

#: ../../datasets/labeled_faces.rst:4
msgid "The Labeled Faces in the Wild face recognition dataset"
msgstr ""

#: ../../datasets/labeled_faces.rst:6
msgid ""
"This dataset is a collection of JPEG pictures of famous people collected "
"over the internet, all details are available on the official website:"
msgstr ""

#: ../../datasets/labeled_faces.rst:9
msgid "http://vis-www.cs.umass.edu/lfw/"
msgstr ""

#: ../../datasets/labeled_faces.rst:11
msgid ""
"Each picture is centered on a single face. The typical task is called "
"Face Verification: given a pair of two pictures, a binary classifier must"
" predict whether the two images are from the same person."
msgstr ""

#: ../../datasets/labeled_faces.rst:15
msgid ""
"An alternative task, Face Recognition or Face Identification is: given "
"the picture of the face of an unknown person, identify the name of the "
"person by referring to a gallery of previously seen pictures of "
"identified persons."
msgstr ""

#: ../../datasets/labeled_faces.rst:20
msgid ""
"Both Face Verification and Face Recognition are tasks that are typically "
"performed on the output of a model trained to perform Face Detection. The"
" most popular model for Face Detection is called Viola-Jones and is "
"implemented in the OpenCV library. The LFW faces were extracted by this "
"face detector from various online websites."
msgstr ""

#: ../../datasets/labeled_faces.rst:30
msgid ""
"``scikit-learn`` provides two loaders that will automatically download, "
"cache, parse the metadata files, decode the jpeg and convert the "
"interesting slices into memmaped numpy arrays. This dataset size is more "
"than 200 MB. The first load typically takes more than a couple of minutes"
" to fully decode the relevant part of the JPEG files into numpy arrays. "
"If the dataset has  been loaded once, the following times the loading "
"times less than 200ms by using a memmaped version memoized on the disk in"
" the ``~/scikit_learn_data/lfw_home/`` folder using ``joblib``."
msgstr ""

#: ../../datasets/labeled_faces.rst:39
msgid ""
"The first loader is used for the Face Identification task: a multi-class "
"classification task (hence supervised learning)::"
msgstr ""

#: ../../datasets/labeled_faces.rst:56
msgid ""
"The default slice is a rectangular shape around the face, removing most "
"of the background::"
msgstr ""

#: ../../datasets/labeled_faces.rst:68
msgid ""
"Each of the ``1140`` faces is assigned to a single person id in the "
"``target`` array::"
msgstr ""

#: ../../datasets/labeled_faces.rst:77
msgid ""
"The second loader is typically used for the face verification task: each "
"sample is a pair of two picture belonging or not to the same person::"
msgstr ""

#: ../../datasets/labeled_faces.rst:95
msgid ""
"Both for the :func:`sklearn.datasets.fetch_lfw_people` and "
":func:`sklearn.datasets.fetch_lfw_pairs` function it is possible to get "
"an additional dimension with the RGB color channels by passing "
"``color=True``, in that case the shape will be ``(2200, 2, 62, 47, 3)``."
msgstr ""

#: ../../datasets/labeled_faces.rst:101
msgid ""
"The :func:`sklearn.datasets.fetch_lfw_pairs` datasets is subdivided into "
"3 subsets: the development ``train`` set, the development ``test`` set "
"and an evaluation ``10_folds`` set meant to compute performance metrics "
"using a 10-folds cross validation scheme."
msgstr ""

#: ../../datasets/labeled_faces.rst
msgid "References:"
msgstr ""

#: ../../datasets/labeled_faces.rst:108
msgid ""
"`Labeled Faces in the Wild: A Database for Studying Face Recognition in "
"Unconstrained Environments. <http://vis-www.cs.umass.edu/lfw/lfw.pdf>`_ "
"Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. "
"University of Massachusetts, Amherst, Technical Report 07-49, October, "
"2007."
msgstr ""

#: ../../datasets/labeled_faces.rst:118
msgid ":ref:`example_applications_face_recognition.py`"
msgstr ""

#: ../../datasets/covtype.rst:5
msgid "Forest covertypes"
msgstr ""

#: ../../datasets/covtype.rst:7
msgid ""
"The samples in this dataset correspond to 30×30m patches of forest in the"
" US, collected for the task of predicting each patch's cover type, i.e. "
"the dominant species of tree. There are seven covertypes, making this a "
"multiclass classification problem. Each sample has 54 features, described"
" on the `dataset's homepage "
"<http://archive.ics.uci.edu/ml/datasets/Covertype>`_. Some of the "
"features are boolean indicators, while others are discrete or continuous "
"measurements."
msgstr ""

#: ../../datasets/covtype.rst:16
msgid ""
":func:`sklearn.datasets.fetch_covtype` will load the covertype dataset; "
"it returns a dictionary-like object with the feature matrix in the "
"``data`` member and the target values in ``target``. The dataset will be "
"downloaded from the web if necessary."
msgstr ""

#: ../../datasets/rcv1.rst:5
msgid "RCV1 dataset"
msgstr ""

#: ../../datasets/rcv1.rst:7
msgid ""
"Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually "
"categorized newswire stories made available by Reuters, Ltd. for research"
" purposes. The dataset is extensively described in [1]_."
msgstr ""

#: ../../datasets/rcv1.rst:9
msgid ""
":func:`sklearn.datasets.fetch_rcv1` will load the following version: "
"RCV1-v2, vectors, full sets, topics multilabels::"
msgstr ""

#: ../../datasets/rcv1.rst:14
msgid "It returns a dictionary-like object, with the following attributes:"
msgstr ""

#: ../../datasets/rcv1.rst:16
#, python-format
msgid ""
"``data``: The feature matrix is a scipy CSR sparse matrix, with 804414 "
"samples and 47236 features. Non-zero values contains cosine-normalized, "
"log TF-IDF vectors. A nearly chronological split is proposed in [1]_: The"
" first 23149 samples are the training set. The last 781265 samples are "
"the testing set. This follows the official LYRL2004 chronological split. "
"The array has 0.16% of non zero values::"
msgstr ""

#: ../../datasets/rcv1.rst:25
#, python-format
msgid ""
"``target``: The target values are stored in a scipy CSR sparse matrix, "
"with 804414 samples and 103 categories. Each sample has a value of 1 in "
"its categories, and 0 in others. The array has 3.15% of non zero values::"
msgstr ""

#: ../../datasets/rcv1.rst:31
msgid ""
"``sample_id``: Each sample can be identified by its ID, ranging (with "
"gaps) from 2286 to 810596::"
msgstr ""

#: ../../datasets/rcv1.rst:37
msgid ""
"``target_names``: The target values are the topics of each sample. Each "
"sample belongs to at least one topic, and to up to 17 topics. There are "
"103 topics, each represented by a string. Their corpus frequencies span "
"five orders of magnitude, from 5 occurrences for 'GMIL', to 381327 for "
"'CCAT'::"
msgstr ""

#: ../../datasets/rcv1.rst:44
msgid ""
"The dataset will be downloaded from the `rcv1 homepage`_ if necessary. "
"The compressed size is about 656 MB."
msgstr ""

#: ../../datasets/rcv1.rst
msgid "References"
msgstr ""

#: ../../datasets/rcv1.rst:52
msgid ""
"Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new "
"benchmark collection for text categorization research. The Journal of "
"Machine Learning Research, 5, 361-397."
msgstr ""

