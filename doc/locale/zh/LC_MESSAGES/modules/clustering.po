# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/clustering.rst:5
msgid "Clustering"
msgstr ""

#: ../../modules/clustering.rst:7
msgid ""
"`Clustering <https://en.wikipedia.org/wiki/Cluster_analysis>`__ of "
"unlabeled data can be performed with the module :mod:`sklearn.cluster`."
msgstr ""

#: ../../modules/clustering.rst:10
msgid ""
"Each clustering algorithm comes in two variants: a class, that implements"
" the ``fit`` method to learn the clusters on train data, and a function, "
"that, given train data, returns an array of integer labels corresponding "
"to the different clusters. For the class, the labels over the training "
"data can be found in the ``labels_`` attribute."
msgstr ""

#: ../../modules/clustering.rst
msgid "Input data"
msgstr ""

#: ../../modules/clustering.rst:20
msgid ""
"One important thing to note is that the algorithms implemented in this "
"module take different kinds of matrix as input.  On one hand, "
":class:`MeanShift` and :class:`KMeans` take data matrices of shape "
"[n_samples, n_features]. These can be obtained from the classes in the "
":mod:`sklearn.feature_extraction` module. On the other hand, "
":class:`AffinityPropagation` and :class:`SpectralClustering` take "
"similarity matrices of shape [n_samples, n_samples].  These can be "
"obtained from the functions in the :mod:`sklearn.metrics.pairwise` "
"module. In other words, :class:`MeanShift` and :class:`KMeans` work with "
"points in a vector space, whereas :class:`AffinityPropagation` and "
":class:`SpectralClustering` can work with arbitrary objects, as long as a"
" similarity measure exists for such objects."
msgstr ""

#: ../../modules/clustering.rst:34
msgid "Overview of clustering methods"
msgstr ""

#: ../../modules/clustering.rst:41
msgid "A comparison of the clustering algorithms in scikit-learn"
msgstr ""

#: ../../modules/clustering.rst:48
msgid "Method name"
msgstr ""

#: ../../modules/clustering.rst:49
msgid "Parameters"
msgstr ""

#: ../../modules/clustering.rst:50
msgid "Scalability"
msgstr ""

#: ../../modules/clustering.rst:51
msgid "Usecase"
msgstr ""

#: ../../modules/clustering.rst:52
msgid "Geometry (metric used)"
msgstr ""

#: ../../modules/clustering.rst:54
msgid ":ref:`K-Means <k_means>`"
msgstr ""

#: ../../modules/clustering.rst:55 ../../modules/clustering.rst:74
#: ../../modules/clustering.rst:80
msgid "number of clusters"
msgstr ""

#: ../../modules/clustering.rst:56
msgid ""
"Very large ``n_samples``, medium ``n_clusters`` with :ref:`MiniBatch code"
" <mini_batch_kmeans>`"
msgstr ""

#: ../../modules/clustering.rst:58
msgid "General-purpose, even cluster size, flat geometry, not too many clusters"
msgstr ""

#: ../../modules/clustering.rst:59 ../../modules/clustering.rst:71
#: ../../modules/clustering.rst:83
msgid "Distances between points"
msgstr ""

#: ../../modules/clustering.rst:61
msgid ":ref:`Affinity propagation <affinity_propagation>`"
msgstr ""

#: ../../modules/clustering.rst:62
msgid "damping, sample preference"
msgstr ""

#: ../../modules/clustering.rst:63
msgid "Not scalable with n_samples"
msgstr ""

#: ../../modules/clustering.rst:64 ../../modules/clustering.rst:70
msgid "Many clusters, uneven cluster size, non-flat geometry"
msgstr ""

#: ../../modules/clustering.rst:65 ../../modules/clustering.rst:77
msgid "Graph distance (e.g. nearest-neighbor graph)"
msgstr ""

#: ../../modules/clustering.rst:67
msgid ":ref:`Mean-shift <mean_shift>`"
msgstr ""

#: ../../modules/clustering.rst:68
msgid "bandwidth"
msgstr ""

#: ../../modules/clustering.rst:69
msgid "Not scalable with ``n_samples``"
msgstr ""

#: ../../modules/clustering.rst:73
msgid ":ref:`Spectral clustering <spectral_clustering>`"
msgstr ""

#: ../../modules/clustering.rst:75
msgid "Medium ``n_samples``, small ``n_clusters``"
msgstr ""

#: ../../modules/clustering.rst:76
msgid "Few clusters, even cluster size, non-flat geometry"
msgstr ""

#: ../../modules/clustering.rst:79
msgid ":ref:`Ward hierarchical clustering <hierarchical_clustering>`"
msgstr ""

#: ../../modules/clustering.rst:81 ../../modules/clustering.rst:87
msgid "Large ``n_samples`` and ``n_clusters``"
msgstr ""

#: ../../modules/clustering.rst:82
msgid "Many clusters, possibly connectivity constraints"
msgstr ""

#: ../../modules/clustering.rst:85
msgid ":ref:`Agglomerative clustering <hierarchical_clustering>`"
msgstr ""

#: ../../modules/clustering.rst:86
msgid "number of clusters, linkage type, distance"
msgstr ""

#: ../../modules/clustering.rst:88
msgid "Many clusters, possibly connectivity constraints, non Euclidean distances"
msgstr ""

#: ../../modules/clustering.rst:90
msgid "Any pairwise distance"
msgstr ""

#: ../../modules/clustering.rst:92
msgid ":ref:`DBSCAN <dbscan>`"
msgstr ""

#: ../../modules/clustering.rst:93
msgid "neighborhood size"
msgstr ""

#: ../../modules/clustering.rst:94
msgid "Very large ``n_samples``, medium ``n_clusters``"
msgstr ""

#: ../../modules/clustering.rst:95
msgid "Non-flat geometry, uneven cluster sizes"
msgstr ""

#: ../../modules/clustering.rst:96
msgid "Distances between nearest points"
msgstr ""

#: ../../modules/clustering.rst:98
msgid ":ref:`Gaussian mixtures <mixture>`"
msgstr ""

#: ../../modules/clustering.rst:99
msgid "many"
msgstr ""

#: ../../modules/clustering.rst:100
msgid "Not scalable"
msgstr ""

#: ../../modules/clustering.rst:101
msgid "Flat geometry, good for density estimation"
msgstr ""

#: ../../modules/clustering.rst:102
msgid "Mahalanobis distances to  centers"
msgstr ""

#: ../../modules/clustering.rst:104
msgid ":ref:`Birch`"
msgstr ""

#: ../../modules/clustering.rst:105
msgid "branching factor, threshold, optional global clusterer."
msgstr ""

#: ../../modules/clustering.rst:106
msgid "Large ``n_clusters`` and ``n_samples``"
msgstr ""

#: ../../modules/clustering.rst:107
msgid "Large dataset, outlier removal, data reduction."
msgstr ""

#: ../../modules/clustering.rst:108
msgid "Euclidean distance between points"
msgstr ""

#: ../../modules/clustering.rst:110
msgid ""
"Non-flat geometry clustering is useful when the clusters have a specific "
"shape, i.e. a non-flat manifold, and the standard euclidean distance is "
"not the right metric. This case arises in the two top rows of the figure "
"above."
msgstr ""

#: ../../modules/clustering.rst:115
msgid ""
"Gaussian mixture models, useful for clustering, are described in "
":ref:`another chapter of the documentation <mixture>` dedicated to "
"mixture models. KMeans can be seen as a special case of Gaussian mixture "
"model with equal covariance per component."
msgstr ""

#: ../../modules/clustering.rst:123
msgid "K-means"
msgstr ""

#: ../../modules/clustering.rst:125
msgid ""
"The :class:`KMeans` algorithm clusters data by trying to separate samples"
" in n groups of equal variance, minimizing a criterion known as the "
"`inertia <inertia>`_ or within-cluster sum-of-squares. This algorithm "
"requires the number of clusters to be specified. It scales well to large "
"number of samples and has been used across a large range of application "
"areas in many different fields."
msgstr ""

#: ../../modules/clustering.rst:132
msgid ""
"The k-means algorithm divides a set of :math:`N` samples :math:`X` into "
":math:`K` disjoint clusters :math:`C`, each described by the mean "
":math:`\\mu_j` of the samples in the cluster. The means are commonly "
"called the cluster \"centroids\"; note that they are not, in general, "
"points from :math:`X`, although they live in the same space. The K-means "
"algorithm aims to choose centroids that minimise the *inertia*, or "
"within-cluster sum of squared criterion:"
msgstr ""

#: ../../modules/clustering.rst:143
msgid ""
"Inertia, or the within-cluster sum of squares criterion, can be "
"recognized as a measure of how internally coherent clusters are. It "
"suffers from various drawbacks:"
msgstr ""

#: ../../modules/clustering.rst:147
msgid ""
"Inertia makes the assumption that clusters are convex and isotropic, "
"which is not always the case. It responds poorly to elongated clusters, "
"or manifolds with irregular shapes."
msgstr ""

#: ../../modules/clustering.rst:151
msgid ""
"Inertia is not a normalized metric: we just know that lower values are "
"better and zero is optimal. But in very high-dimensional spaces, "
"Euclidean distances tend to become inflated (this is an instance of the "
"so-called \"curse of dimensionality\"). Running a dimensionality "
"reduction algorithm such as `PCA <PCA>`_ prior to k-means clustering can "
"alleviate this problem and speed up the computations."
msgstr ""

#: ../../modules/clustering.rst:164
msgid ""
"K-means is often referred to as Lloyd's algorithm. In basic terms, the "
"algorithm has three steps. The first step chooses the initial centroids, "
"with the most basic method being to choose :math:`k` samples from the "
"dataset :math:`X`. After initialization, K-means consists of looping "
"between the two other steps. The first step assigns each sample to its "
"nearest centroid. The second step creates new centroids by taking the "
"mean value of all of the samples assigned to each previous centroid. The "
"difference between the old and the new centroids are computed and the "
"algorithm repeats these last two steps until this value is less than a "
"threshold. In other words, it repeats until the centroids do not move "
"significantly."
msgstr ""

#: ../../modules/clustering.rst:180
msgid ""
"K-means is equivalent to the expectation-maximization algorithm with a "
"small, all-equal, diagonal covariance matrix."
msgstr ""

#: ../../modules/clustering.rst:183
msgid ""
"The algorithm can also be understood through the concept of `Voronoi "
"diagrams <https://en.wikipedia.org/wiki/Voronoi_diagram>`_. First the "
"Voronoi diagram of the points is calculated using the current centroids. "
"Each segment in the Voronoi diagram becomes a separate cluster. Secondly,"
" the centroids are updated to the mean of each segment. The algorithm "
"then repeats this until a stopping criterion is fulfilled. Usually, the "
"algorithm stops when the relative decrease in the objective function "
"between iterations is less than the given tolerance value. This is not "
"the case in this implementation: iteration stops when centroids move less"
" than the tolerance."
msgstr ""

#: ../../modules/clustering.rst:193
msgid ""
"Given enough time, K-means will always converge, however this may be to a"
" local minimum. This is highly dependent on the initialization of the "
"centroids. As a result, the computation is often done several times, with"
" different initializations of the centroids. One method to help address "
"this issue is the k-means++ initialization scheme, which has been "
"implemented in scikit-learn (use the ``init='kmeans++'`` parameter). This"
" initializes the centroids to be (generally) distant from each other, "
"leading to provably better results than random initialization, as shown "
"in the reference."
msgstr ""

#: ../../modules/clustering.rst:202
msgid ""
"A parameter can be given to allow K-means to be run in parallel, called "
"``n_jobs``. Giving this parameter a positive value uses that many "
"processors (default: 1). A value of -1 uses all available processors, "
"with -2 using one less, and so on. Parallelization generally speeds up "
"computation at the cost of memory (in this case, multiple copies of "
"centroids need to be stored, one for each job)."
msgstr ""

#: ../../modules/clustering.rst:211
msgid ""
"The parallel version of K-Means is broken on OS X when `numpy` uses the "
"`Accelerate` Framework. This is expected behavior: `Accelerate` can be "
"called after a fork but you need to execv the subprocess with the Python "
"binary (which multiprocessing does not do under posix)."
msgstr ""

#: ../../modules/clustering.rst:216
msgid ""
"K-means can be used for vector quantization. This is achieved using the "
"transform method of a trained model of :class:`KMeans`."
msgstr ""

#: ../../modules/clustering.rst
msgid "Examples:"
msgstr ""

#: ../../modules/clustering.rst:221
msgid ""
":ref:`example_cluster_plot_kmeans_assumptions.py`: Demonstrating when "
"k-means performs intuitively and when it does not"
msgstr ""

#: ../../modules/clustering.rst:223
msgid ""
":ref:`example_cluster_plot_kmeans_digits.py`: Clustering handwritten "
"digits"
msgstr ""

#: ../../modules/clustering.rst
msgid "References:"
msgstr ""

#: ../../modules/clustering.rst:227
msgid ""
"`\"k-means++: The advantages of careful seeding\" "
"<http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>`_ Arthur, David, and "
"Sergei Vassilvitskii, *Proceedings of the eighteenth annual ACM-SIAM "
"symposium on Discrete algorithms*, Society for Industrial and Applied "
"Mathematics (2007)"
msgstr ""

#: ../../modules/clustering.rst:236
msgid "Mini Batch K-Means"
msgstr ""

#: ../../modules/clustering.rst:238
msgid ""
"The :class:`MiniBatchKMeans` is a variant of the :class:`KMeans` "
"algorithm which uses mini-batches to reduce the computation time, while "
"still attempting to optimise the same objective function. Mini-batches "
"are subsets of the input data, randomly sampled in each training "
"iteration. These mini-batches drastically reduce the amount of "
"computation required to converge to a local solution. In contrast to "
"other algorithms that reduce the convergence time of k-means, mini-batch "
"k-means produces results that are generally only slightly worse than the "
"standard algorithm."
msgstr ""

#: ../../modules/clustering.rst:247
msgid ""
"The algorithm iterates between two major steps, similar to vanilla "
"k-means. In the first step, :math:`b` samples are drawn randomly from the"
" dataset, to form a mini-batch. These are then assigned to the nearest "
"centroid. In the second step, the centroids are updated. In contrast to "
"k-means, this is done on a per-sample basis. For each sample in the mini-"
"batch, the assigned centroid is updated by taking the streaming average "
"of the sample and all previous samples assigned to that centroid. This "
"has the effect of decreasing the rate of change for a centroid over time."
" These steps are performed until convergence or a predetermined number of"
" iterations is reached."
msgstr ""

#: ../../modules/clustering.rst:257
msgid ""
":class:`MiniBatchKMeans` converges faster than :class:`KMeans`, but the "
"quality of the results is reduced. In practice this difference in quality"
" can be quite small, as shown in the example and cited reference."
msgstr ""

#: ../../modules/clustering.rst:269
msgid ""
":ref:`example_cluster_plot_mini_batch_kmeans.py`: Comparison of KMeans "
"and MiniBatchKMeans"
msgstr ""

#: ../../modules/clustering.rst:272
msgid ""
":ref:`example_text_document_clustering.py`: Document clustering using "
"sparse MiniBatchKMeans"
msgstr ""

#: ../../modules/clustering.rst:275
msgid ":ref:`example_cluster_plot_dict_face_patches.py`"
msgstr ""

#: ../../modules/clustering.rst:280
msgid ""
"`\"Web Scale K-Means clustering\" "
"<http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>`_ D. Sculley,"
" *Proceedings of the 19th international conference on World wide web* "
"(2010)"
msgstr ""

#: ../../modules/clustering.rst:288
msgid "Affinity Propagation"
msgstr ""

#: ../../modules/clustering.rst:290
msgid ""
":class:`AffinityPropagation` creates clusters by sending messages between"
" pairs of samples until convergence. A dataset is then described using a "
"small number of exemplars, which are identified as those most "
"representative of other samples. The messages sent between pairs "
"represent the suitability for one sample to be the exemplar of the other,"
" which is updated in response to the values from other pairs. This "
"updating happens iteratively until convergence, at which point the final "
"exemplars are chosen, and hence the final clustering is given."
msgstr ""

#: ../../modules/clustering.rst:305
msgid ""
"Affinity Propagation can be interesting as it chooses the number of "
"clusters based on the data provided. For this purpose, the two important "
"parameters are the *preference*, which controls how many exemplars are "
"used, and the *damping factor*."
msgstr ""

#: ../../modules/clustering.rst:310
msgid ""
"The main drawback of Affinity Propagation is its complexity. The "
"algorithm has a time complexity of the order :math:`O(N^2 T)`, where "
":math:`N` is the number of samples and :math:`T` is the number of "
"iterations until convergence. Further, the memory complexity is of the "
"order :math:`O(N^2)` if a dense similarity matrix is used, but reducible "
"if a sparse similarity matrix is used. This makes Affinity Propagation "
"most appropriate for small to medium sized datasets."
msgstr ""

#: ../../modules/clustering.rst:320
msgid ""
":ref:`example_cluster_plot_affinity_propagation.py`: Affinity Propagation"
" on a synthetic 2D datasets with 3 classes."
msgstr ""

#: ../../modules/clustering.rst:323
msgid ""
":ref:`example_applications_plot_stock_market.py` Affinity Propagation on "
"Financial time series to find groups of companies"
msgstr ""

#: ../../modules/clustering.rst:327
msgid ""
"**Algorithm description:** The messages sent between points belong to one"
" of two categories. The first is the responsibility :math:`r(i, k)`, "
"which is the accumulated evidence that sample :math:`k` should be the "
"exemplar for sample :math:`i`. The second is the availability :math:`a(i,"
" k)` which is the accumulated evidence that sample :math:`i` should "
"choose sample :math:`k` to be its exemplar, and considers the values for "
"all other samples that :math:`k` should be an exemplar. In this way, "
"exemplars are chosen by samples if they are (1) similar enough to many "
"samples and (2) chosen by many samples to be representative of "
"themselves."
msgstr ""

#: ../../modules/clustering.rst:340
msgid ""
"More formally, the responsibility of a sample :math:`k` to be the "
"exemplar of sample :math:`i` is given by:"
msgstr ""

#: ../../modules/clustering.rst:347
msgid ""
"Where :math:`s(i, k)` is the similarity between samples :math:`i` and "
":math:`k`. The availability of sample :math:`k` to be the exemplar of "
"sample :math:`i` is given by:"
msgstr ""

#: ../../modules/clustering.rst:355
msgid ""
"To begin with, all values for :math:`r` and :math:`a` are set to zero, "
"and the calculation of each iterates until convergence."
msgstr ""

#: ../../modules/clustering.rst:361
msgid "Mean Shift"
msgstr ""

#: ../../modules/clustering.rst:362
msgid ""
":class:`MeanShift` clustering aims to discover *blobs* in a smooth "
"density of samples. It is a centroid based algorithm, which works by "
"updating candidates for centroids to be the mean of the points within a "
"given region. These candidates are then filtered in a post-processing "
"stage to eliminate near-duplicates to form the final set of centroids."
msgstr ""

#: ../../modules/clustering.rst:368
msgid ""
"Given a candidate centroid :math:`x_i` for iteration :math:`t`, the "
"candidate is updated according to the following equation:"
msgstr ""

#: ../../modules/clustering.rst:375
msgid ""
"Where :math:`N(x_i)` is the neighborhood of samples within a given "
"distance around :math:`x_i` and :math:`m` is the *mean shift* vector that"
" is computed for each centroid that points towards a region of the "
"maximum increase in the density of points. This is computed using the "
"following equation, effectively updating a centroid to be the mean of the"
" samples within its neighborhood:"
msgstr ""

#: ../../modules/clustering.rst:385
msgid ""
"The algorithm automatically sets the number of clusters, instead of "
"relying on a parameter ``bandwidth``, which dictates the size of the "
"region to search through. This parameter can be set manually, but can be "
"estimated using the provided ``estimate_bandwidth`` function, which is "
"called if the bandwidth is not set."
msgstr ""

#: ../../modules/clustering.rst:390
msgid ""
"The algorithm is not highly scalable, as it requires multiple nearest "
"neighbor searches during the execution of the algorithm. The algorithm is"
" guaranteed to converge, however the algorithm will stop iterating when "
"the change in centroids is small."
msgstr ""

#: ../../modules/clustering.rst:395
msgid ""
"Labelling a new sample is performed by finding the nearest centroid for a"
" given sample."
msgstr ""

#: ../../modules/clustering.rst:407
msgid ""
":ref:`example_cluster_plot_mean_shift.py`: Mean Shift clustering on a "
"synthetic 2D datasets with 3 classes."
msgstr ""

#: ../../modules/clustering.rst:412
msgid ""
"`\"Mean shift: A robust approach toward feature space analysis.\" "
"<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&rep=rep1&type=pdf>`_"
" D. Comaniciu and P. Meer, *IEEE Transactions on Pattern Analysis and "
"Machine Intelligence* (2002)"
msgstr ""

#: ../../modules/clustering.rst:420
msgid "Spectral clustering"
msgstr ""

#: ../../modules/clustering.rst:422
msgid ""
":class:`SpectralClustering` does a low-dimension embedding of the "
"affinity matrix between samples, followed by a KMeans in the low "
"dimensional space. It is especially efficient if the affinity matrix is "
"sparse and the `pyamg <http://pyamg.org/>`_ module is installed. "
"SpectralClustering requires the number of clusters to be specified. It "
"works well for a small number of clusters but is not advised when using "
"many clusters."
msgstr ""

#: ../../modules/clustering.rst:430
msgid ""
"For two clusters, it solves a convex relaxation of the `normalised cuts "
"<http://www.cs.berkeley.edu/~malik/papers/SM-ncut.pdf>`_ problem on the "
"similarity graph: cutting the graph in two so that the weight of the "
"edges cut is small compared to the weights of the edges inside each "
"cluster. This criteria is especially interesting when working on images: "
"graph vertices are pixels, and edges of the similarity graph are a "
"function of the gradient of the image."
msgstr ""

#: ../../modules/clustering.rst:448
msgid "noisy_img segmented_img"
msgstr ""

#: ../../modules/clustering.rst:449
msgid "Transforming distance to well-behaved similarities"
msgstr ""

#: ../../modules/clustering.rst:451
msgid ""
"Note that if the values of your similarity matrix are not well "
"distributed, e.g. with negative values or with a distance matrix rather "
"than a similarity, the spectral problem will be singular and the problem "
"not solvable. In which case it is advised to apply a transformation to "
"the entries of the matrix. For instance, in the case of a signed distance"
" matrix, is common to apply a heat kernel::"
msgstr ""

#: ../../modules/clustering.rst:460
msgid "See the examples for such an application."
msgstr ""

#: ../../modules/clustering.rst:464
msgid ""
":ref:`example_cluster_plot_segmentation_toy.py`: Segmenting objects from "
"a noisy background using spectral clustering."
msgstr ""

#: ../../modules/clustering.rst:467
msgid ""
":ref:`example_cluster_plot_face_segmentation.py`: Spectral clustering to "
"split the image of the raccoon face in regions."
msgstr ""

#: ../../modules/clustering.rst:479
msgid "Different label assignment strategies"
msgstr ""

#: ../../modules/clustering.rst:481
#, python-format
msgid ""
"Different label assignment strategies can be used, corresponding to the "
"``assign_labels`` parameter of :class:`SpectralClustering`. The "
"``\"kmeans\"`` strategy can match finer details of the data, but it can "
"be more unstable. In particular, unless you control the ``random_state``,"
" it may not be reproducible from run-to-run, as it depends on a random "
"initialization. On the other hand, the ``\"discretize\"`` strategy is "
"100% reproducible, but it tends to create parcels of fairly even and "
"geometrical shape."
msgstr ""

#: ../../modules/clustering.rst:491
msgid "``assign_labels=\"kmeans\"``"
msgstr ""

#: ../../modules/clustering.rst:491
msgid "``assign_labels=\"discretize\"``"
msgstr ""

#: ../../modules/clustering.rst:493
msgid "|face_kmeans|"
msgstr ""

#: ../../modules/clustering.rst:493
msgid "|face_discretize|"
msgstr ""

#: ../../modules/clustering.rst:499
msgid ""
"`\"A Tutorial on Spectral Clustering\" "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323>`_ "
"Ulrike von Luxburg, 2007"
msgstr ""

#: ../../modules/clustering.rst:503
msgid ""
"`\"Normalized cuts and image segmentation\" "
"<http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324>`_ "
"Jianbo Shi, Jitendra Malik, 2000"
msgstr ""

#: ../../modules/clustering.rst:507
msgid ""
"`\"A Random Walks View of Spectral Segmentation\" "
"<http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501>`_ Marina"
" Meila, Jianbo Shi, 2001"
msgstr ""

#: ../../modules/clustering.rst:511
msgid ""
"`\"On Spectral Clustering: Analysis and an algorithm\" "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100>`_ "
"Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001"
msgstr ""

#: ../../modules/clustering.rst:519
msgid "Hierarchical clustering"
msgstr ""

#: ../../modules/clustering.rst:521
msgid ""
"Hierarchical clustering is a general family of clustering algorithms that"
" build nested clusters by merging or splitting them successively. This "
"hierarchy of clusters is represented as a tree (or dendrogram). The root "
"of the tree is the unique cluster that gathers all the samples, the "
"leaves being the clusters with only one sample. See the `Wikipedia page "
"<https://en.wikipedia.org/wiki/Hierarchical_clustering>`_ for more "
"details."
msgstr ""

#: ../../modules/clustering.rst:528
msgid ""
"The :class:`AgglomerativeClustering` object performs a hierarchical "
"clustering using a bottom up approach: each observation starts in its own"
" cluster, and clusters are successively merged together. The linkage "
"criteria determines the metric used for the merge strategy:"
msgstr ""

#: ../../modules/clustering.rst:533
msgid ""
"**Ward** minimizes the sum of squared differences within all clusters. It"
" is a variance-minimizing approach and in this sense is similar to the "
"k-means objective function but tackled with an agglomerative hierarchical"
" approach."
msgstr ""

#: ../../modules/clustering.rst:537
msgid ""
"**Maximum** or **complete linkage** minimizes the maximum distance "
"between observations of pairs of clusters."
msgstr ""

#: ../../modules/clustering.rst:539
msgid ""
"**Average linkage** minimizes the average of the distances between all "
"observations of pairs of clusters."
msgstr ""

#: ../../modules/clustering.rst:542
msgid ""
":class:`AgglomerativeClustering` can also scale to large number of "
"samples when it is used jointly with a connectivity matrix, but is "
"computationally expensive when no connectivity constraints are added "
"between samples: it considers at each step all the possible merges."
msgstr ""

#: ../../modules/clustering.rst
msgid ":class:`FeatureAgglomeration`"
msgstr ""

#: ../../modules/clustering.rst:549
msgid ""
"The :class:`FeatureAgglomeration` uses agglomerative clustering to group "
"together features that look very similar, thus decreasing the number of "
"features. It is a dimensionality reduction tool, see "
":ref:`data_reduction`."
msgstr ""

#: ../../modules/clustering.rst:555
msgid "Different linkage type: Ward, complete and average linkage"
msgstr ""

#: ../../modules/clustering.rst:557
msgid ""
":class:`AgglomerativeClustering` supports Ward, average, and complete "
"linkage strategies."
msgstr ""

#: ../../modules/clustering.rst:573
msgid ""
"Agglomerative cluster has a \"rich get richer\" behavior that leads to "
"uneven cluster sizes. In this regard, complete linkage is the worst "
"strategy, and Ward gives the most regular sizes. However, the affinity "
"(or distance used in clustering) cannot be varied with Ward, thus for non"
" Euclidean metrics, average linkage is a good alternative."
msgstr ""

#: ../../modules/clustering.rst:581
msgid ""
":ref:`example_cluster_plot_digits_linkage.py`: exploration of the "
"different linkage strategies in a real dataset."
msgstr ""

#: ../../modules/clustering.rst:586
msgid "Adding connectivity constraints"
msgstr ""

#: ../../modules/clustering.rst:588
msgid ""
"An interesting aspect of :class:`AgglomerativeClustering` is that "
"connectivity constraints can be added to this algorithm (only adjacent "
"clusters can be merged together), through a connectivity matrix that "
"defines for each sample the neighboring samples following a given "
"structure of the data. For instance, in the swiss-roll example below, the"
" connectivity constraints forbid the merging of points that are not "
"adjacent on the swiss roll, and thus avoid forming clusters that extend "
"across overlapping folds of the roll."
msgstr ""

#: ../../modules/clustering.rst:606
msgid "unstructured structured"
msgstr ""

#: ../../modules/clustering.rst:607
msgid ""
"These constraint are useful to impose a certain local structure, but they"
" also make the algorithm faster, especially when the number of the "
"samples is high."
msgstr ""

#: ../../modules/clustering.rst:611
msgid ""
"The connectivity constraints are imposed via an connectivity matrix: a "
"scipy sparse matrix that has elements only at the intersection of a row "
"and a column with indices of the dataset that should be connected. This "
"matrix can be constructed from a-priori information: for instance, you "
"may wish to cluster web pages by only merging pages with a link pointing "
"from one to another. It can also be learned from the data, for instance "
"using :func:`sklearn.neighbors.kneighbors_graph` to restrict merging to "
"nearest neighbors as in :ref:`this example "
"<example_cluster_plot_agglomerative_clustering.py>`, or using "
":func:`sklearn.feature_extraction.image.grid_to_graph` to enable only "
"merging of neighboring pixels on an image, as in the :ref:`raccoon face "
"<example_cluster_plot_face_ward_segmentation.py>` example."
msgstr ""

#: ../../modules/clustering.rst:626
msgid ""
":ref:`example_cluster_plot_face_ward_segmentation.py`: Ward clustering to"
" split the image of a raccoon face in regions."
msgstr ""

#: ../../modules/clustering.rst:629
msgid ""
":ref:`example_cluster_plot_ward_structured_vs_unstructured.py`: Example "
"of Ward algorithm on a swiss-roll, comparison of structured approaches "
"versus unstructured approaches."
msgstr ""

#: ../../modules/clustering.rst:633
msgid ""
":ref:`example_cluster_plot_feature_agglomeration_vs_univariate_selection.py`:"
" Example of dimensionality reduction with feature agglomeration based on "
"Ward hierarchical clustering."
msgstr ""

#: ../../modules/clustering.rst:637
msgid ":ref:`example_cluster_plot_agglomerative_clustering.py`"
msgstr ""

#: ../../modules/clustering.rst:639
msgid "**Connectivity constraints with average and complete linkage**"
msgstr ""

#: ../../modules/clustering.rst:641
msgid ""
"Connectivity constraints and complete or average linkage can enhance the "
"'rich getting richer' aspect of agglomerative clustering, particularly so"
" if they are built with :func:`sklearn.neighbors.kneighbors_graph`. In "
"the limit of a small number of clusters, they tend to give a few "
"macroscopically occupied clusters and almost empty ones. (see the "
"discussion in :ref:`example_cluster_plot_agglomerative_clustering.py`)."
msgstr ""

#: ../../modules/clustering.rst:667
msgid "Varying the metric"
msgstr ""

#: ../../modules/clustering.rst:669
msgid ""
"Average and complete linkage can be used with a variety of distances (or "
"affinities), in particular Euclidean distance (*l2*), Manhattan distance "
"(or Cityblock, or *l1*), cosine distance, or any precomputed affinity "
"matrix."
msgstr ""

#: ../../modules/clustering.rst:674
msgid ""
"*l1* distance is often good for sparse features, or sparse noise: ie many"
" of the features are zero, as in text mining using occurences of rare "
"words."
msgstr ""

#: ../../modules/clustering.rst:678
msgid ""
"*cosine* distance is interesting because it is invariant to global "
"scalings of the signal."
msgstr ""

#: ../../modules/clustering.rst:681
msgid ""
"The guidelines for choosing a metric is to use one that maximizes the "
"distance between samples in different classes, and minimizes that within "
"each class."
msgstr ""

#: ../../modules/clustering.rst:699
msgid ":ref:`example_cluster_plot_agglomerative_clustering_metrics.py`"
msgstr ""

#: ../../modules/clustering.rst:705
msgid "DBSCAN"
msgstr ""

#: ../../modules/clustering.rst:707
msgid ""
"The :class:`DBSCAN` algorithm views clusters as areas of high density "
"separated by areas of low density. Due to this rather generic view, "
"clusters found by DBSCAN can be any shape, as opposed to k-means which "
"assumes that clusters are convex shaped. The central component to the "
"DBSCAN is the concept of *core samples*, which are samples that are in "
"areas of high density. A cluster is therefore a set of core samples, each"
" close to each other (measured by some distance measure) and a set of "
"non-core samples that are close to a core sample (but are not themselves "
"core samples). There are two parameters to the algorithm, ``min_samples``"
" and ``eps``, which define formally what we mean when we say *dense*. "
"Higher ``min_samples`` or lower ``eps`` indicate higher density necessary"
" to form a cluster."
msgstr ""

#: ../../modules/clustering.rst:721
msgid ""
"More formally, we define a core sample as being a sample in the dataset "
"such that there exist ``min_samples`` other samples within a distance of "
"``eps``, which are defined as *neighbors* of the core sample. This tells "
"us that the core sample is in a dense area of the vector space. A cluster"
" is a set of core samples, that can be built by recursively by taking a "
"core sample, finding all of its neighbors that are core samples, finding "
"all of *their* neighbors that are core samples, and so on. A cluster also"
" has a set of non-core samples, which are samples that are neighbors of a"
" core sample in the cluster but are not themselves core samples. "
"Intuitively, these samples are on the fringes of a cluster."
msgstr ""

#: ../../modules/clustering.rst:732
msgid ""
"Any core sample is part of a cluster, by definition. Further, any cluster"
" has at least ``min_samples`` points in it, following the definition of a"
" core sample. For any sample that is not a core sample, and does have a "
"distance higher than ``eps`` to any core sample, it is considered an "
"outlier by the algorithm."
msgstr ""

#: ../../modules/clustering.rst:738
msgid ""
"In the figure below, the color indicates cluster membership, with large "
"circles indicating core samples found by the algorithm. Smaller circles "
"are non-core samples that are still part of a cluster. Moreover, the "
"outliers are indicated by black points below."
msgstr ""

#: ../../modules/clustering.rst:748
msgid "dbscan_results"
msgstr ""

#: ../../modules/clustering.rst:751
msgid ":ref:`example_cluster_plot_dbscan.py`"
msgstr ""

#: ../../modules/clustering.rst
msgid "Implementation"
msgstr ""

#: ../../modules/clustering.rst:755
msgid ""
"The algorithm is non-deterministic, but the core samples will always "
"belong to the same clusters (although the labels may be different). The "
"non-determinism comes from deciding to which cluster a non-core sample "
"belongs. A non-core sample can have a distance lower than ``eps`` to two "
"core samples in different clusters. By the triangular inequality, those "
"two core samples must be more distant than ``eps`` from each other, or "
"they would be in the same cluster. The non-core sample is assigned to "
"whichever cluster is generated first, where the order is determined "
"randomly. Other than the ordering of the dataset, the algorithm is "
"deterministic, making the results relatively stable between runs on the "
"same data."
msgstr ""

#: ../../modules/clustering.rst:767
msgid ""
"The current implementation uses ball trees and kd-trees to determine the "
"neighborhood of points, which avoids calculating the full distance matrix"
" (as was done in scikit-learn versions before 0.14). The possibility to "
"use custom metrics is retained; for details, see "
":class:`NearestNeighbors`."
msgstr ""

#: ../../modules/clustering.rst:776
msgid ""
"\"A Density-Based Algorithm for Discovering Clusters in Large Spatial "
"Databases with Noise\" Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In"
" Proceedings of the 2nd International Conference on Knowledge Discovery "
"and Data Mining, Portland, OR, AAAI Press, pp. 226â€“231. 1996"
msgstr ""

#: ../../modules/clustering.rst:785
msgid "Birch"
msgstr ""

#: ../../modules/clustering.rst:787
msgid ""
"The :class:`Birch` builds a tree called the Characteristic Feature Tree "
"(CFT) for the given data. The data is essentially lossy compressed to a "
"set of Characteristic Feature nodes (CF Nodes). The CF Nodes have a "
"number of subclusters called Characteristic Feature subclusters (CF "
"Subclusters) and these CF Subclusters located in the non-terminal CF "
"Nodes can have CF Nodes as children."
msgstr ""

#: ../../modules/clustering.rst:794
msgid ""
"The CF Subclusters hold the necessary information for clustering which "
"prevents the need to hold the entire input data in memory. This "
"information includes:"
msgstr ""

#: ../../modules/clustering.rst:797
msgid "Number of samples in a subcluster."
msgstr ""

#: ../../modules/clustering.rst:798
msgid "Linear Sum - A n-dimensional vector holding the sum of all samples"
msgstr ""

#: ../../modules/clustering.rst:799
msgid "Squared Sum - Sum of the squared L2 norm of all samples."
msgstr ""

#: ../../modules/clustering.rst:800
msgid "Centroids - To avoid recalculation linear sum / n_samples."
msgstr ""

#: ../../modules/clustering.rst:801
msgid "Squared norm of the centroids."
msgstr ""

#: ../../modules/clustering.rst:803
msgid ""
"The Birch algorithm has two parameters, the threshold and the branching "
"factor. The branching factor limits the number of subclusters in a node "
"and the threshold limits the distance between the entering sample and the"
" existing subclusters."
msgstr ""

#: ../../modules/clustering.rst:808
msgid ""
"This algorithm can be viewed as an instance or data reduction method, "
"since it reduces the input data to a set of subclusters which are "
"obtained directly from the leaves of the CFT. This reduced data can be "
"further processed by feeding it into a global clusterer. This global "
"clusterer can be set by ``n_clusters``. If ``n_clusters`` is set to None,"
" the subclusters from the leaves are directly read off, otherwise a "
"global clustering step labels these subclusters into global clusters "
"(labels) and the samples are mapped to the global label of the nearest "
"subcluster."
msgstr ""

#: ../../modules/clustering.rst:816
msgid "**Algorithm description:**"
msgstr ""

#: ../../modules/clustering.rst:818
msgid ""
"A new sample is inserted into the root of the CF Tree which is a CF Node."
" It is then merged with the subcluster of the root, that has the smallest"
" radius after merging, constrained by the threshold and branching factor "
"conditions. If the subcluster has any child node, then this is done "
"repeatedly till it reaches a leaf. After finding the nearest subcluster "
"in the leaf, the properties of this subcluster and the parent subclusters"
" are recursively updated."
msgstr ""

#: ../../modules/clustering.rst:825
msgid ""
"If the radius of the subcluster obtained by merging the new sample and "
"the nearest subcluster is greater than the square of the threshold and if"
" the number of subclusters is greater than the branching factor, then a "
"space is temporarily allocated to this new sample. The two farthest "
"subclusters are taken and the subclusters are divided into two groups on "
"the basis of the distance between these subclusters."
msgstr ""

#: ../../modules/clustering.rst:832
msgid ""
"If this split node has a parent subcluster and there is room for a new "
"subcluster, then the parent is split into two. If there is no room, then "
"this node is again split into two and the process is continued "
"recursively, till it reaches the root."
msgstr ""

#: ../../modules/clustering.rst:837
msgid "**Birch or MiniBatchKMeans?**"
msgstr ""

#: ../../modules/clustering.rst:839
msgid ""
"Birch does not scale very well to high dimensional data. As a rule of "
"thumb if ``n_features`` is greater than twenty, it is generally better to"
" use MiniBatchKMeans."
msgstr ""

#: ../../modules/clustering.rst:841
msgid ""
"If the number of instances of data needs to be reduced, or if one wants a"
" large number of subclusters either as a preprocessing step or otherwise,"
" Birch is more useful than MiniBatchKMeans."
msgstr ""

#: ../../modules/clustering.rst:846
msgid "**How to use partial_fit?**"
msgstr ""

#: ../../modules/clustering.rst:848
msgid ""
"To avoid the computation of global clustering, for every call of "
"``partial_fit`` the user is advised"
msgstr ""

#: ../../modules/clustering.rst:851
msgid "To set ``n_clusters=None`` initially"
msgstr ""

#: ../../modules/clustering.rst:852
msgid "Train all data by multiple calls to partial_fit."
msgstr ""

#: ../../modules/clustering.rst:853
msgid ""
"Set ``n_clusters`` to a required value using "
"``brc.set_params(n_clusters=n_clusters)``."
msgstr ""

#: ../../modules/clustering.rst:855
msgid ""
"Call ``partial_fit`` finally with no arguments, i.e ``brc.partial_fit()``"
" which performs the global clustering."
msgstr ""

#: ../../modules/clustering.rst:863
msgid ""
"Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data "
"clustering method for large databases. "
"http://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf"
msgstr ""

#: ../../modules/clustering.rst:867
msgid ""
"Roberto Perdisci JBirch - Java implementation of BIRCH clustering "
"algorithm https://code.google.com/p/jbirch/"
msgstr ""

#: ../../modules/clustering.rst:875
msgid "Clustering performance evaluation"
msgstr ""

#: ../../modules/clustering.rst:877
msgid ""
"Evaluating the performance of a clustering algorithm is not as trivial as"
" counting the number of errors or the precision and recall of a "
"supervised classification algorithm. In particular any evaluation metric "
"should not take the absolute values of the cluster labels into account "
"but rather if this clustering define separations of the data similar to "
"some ground truth set of classes or satisfying some assumption such that "
"members belong to the same class are more similar that members of "
"different classes according to some similarity metric."
msgstr ""

#: ../../modules/clustering.rst:891
msgid "Adjusted Rand index"
msgstr ""

#: ../../modules/clustering.rst:893
msgid ""
"Given the knowledge of the ground truth class assignments ``labels_true``"
" and our clustering algorithm assignments of the same samples "
"``labels_pred``, the **adjusted Rand index** is a function that measures "
"the **similarity** of the two assignments, ignoring permutations and "
"**with chance normalization**::"
msgstr ""

#: ../../modules/clustering.rst:906
msgid ""
"One can permute 0 and 1 in the predicted labels, rename 2 to 3, and get "
"the same score::"
msgstr ""

#: ../../modules/clustering.rst:913
msgid ""
"Furthermore, :func:`adjusted_rand_score` is **symmetric**: swapping the "
"argument does not change the score. It can thus be used as a **consensus "
"measure**::"
msgstr ""

#: ../../modules/clustering.rst:920 ../../modules/clustering.rst:1041
msgid "Perfect labeling is scored 1.0::"
msgstr ""

#: ../../modules/clustering.rst:926
msgid "Bad (e.g. independent labelings) have negative or close to 0.0 scores::"
msgstr ""

#: ../../modules/clustering.rst:935 ../../modules/clustering.rst:1064
#: ../../modules/clustering.rst:1242 ../../modules/clustering.rst:1384
msgid "Advantages"
msgstr ""

#: ../../modules/clustering.rst:937
msgid ""
"**Random (uniform) label assignments have a ARI score close to 0.0** for "
"any value of ``n_clusters`` and ``n_samples`` (which is not the case for "
"raw Rand index or the V-measure for instance)."
msgstr ""

#: ../../modules/clustering.rst:941
msgid ""
"**Bounded range [-1, 1]**: negative values are bad (independent "
"labelings), similar clusterings have a positive ARI, 1.0 is the perfect "
"match score."
msgstr ""

#: ../../modules/clustering.rst:945 ../../modules/clustering.rst:1076
#: ../../modules/clustering.rst:1250
msgid ""
"**No assumption is made on the cluster structure**: can be used to "
"compare clustering algorithms such as k-means which assumes isotropic "
"blob shapes with results of spectral clustering algorithms which can find"
" cluster with \"folded\" shapes."
msgstr ""

#: ../../modules/clustering.rst:952 ../../modules/clustering.rst:1083
#: ../../modules/clustering.rst:1257 ../../modules/clustering.rst:1394
msgid "Drawbacks"
msgstr ""

#: ../../modules/clustering.rst:954
msgid ""
"Contrary to inertia, **ARI requires knowledge of the ground truth "
"classes** while is almost never available in practice or requires manual "
"assignment by human annotators (as in the supervised learning setting)."
msgstr ""

#: ../../modules/clustering.rst:958
msgid ""
"However ARI can also be useful in a purely unsupervised setting as a "
"building block for a Consensus Index that can be used for clustering "
"model selection (TODO)."
msgstr ""

#: ../../modules/clustering.rst:965 ../../modules/clustering.rst:1283
msgid ""
":ref:`example_cluster_plot_adjusted_for_chance_measures.py`: Analysis of "
"the impact of the dataset size on the value of clustering measures for "
"random assignments."
msgstr ""

#: ../../modules/clustering.rst:971 ../../modules/clustering.rst:1106
#: ../../modules/clustering.rst:1289
msgid "Mathematical formulation"
msgstr ""

#: ../../modules/clustering.rst:973
msgid ""
"If C is a ground truth class assignment and K the clustering, let us "
"define :math:`a` and :math:`b` as:"
msgstr ""

#: ../../modules/clustering.rst:976
msgid ""
":math:`a`, the number of pairs of elements that are in the same set in C "
"and in the same set in K"
msgstr ""

#: ../../modules/clustering.rst:979
msgid ""
":math:`b`, the number of pairs of elements that are in different sets in "
"C and in different sets in K"
msgstr ""

#: ../../modules/clustering.rst:982
msgid "The raw (unadjusted) Rand index is then given by:"
msgstr ""

#: ../../modules/clustering.rst:986
msgid ""
"Where :math:`C_2^{n_{samples}}` is the total number of possible pairs in "
"the dataset (without ordering)."
msgstr ""

#: ../../modules/clustering.rst:989
msgid ""
"However the RI score does not guarantee that random label assignments "
"will get a value close to zero (esp. if the number of clusters is in the "
"same order of magnitude as the number of samples)."
msgstr ""

#: ../../modules/clustering.rst:993
msgid ""
"To counter this effect we can discount the expected RI "
":math:`E[\\text{RI}]` of random labelings by defining the adjusted Rand "
"index as follows:"
msgstr ""

#: ../../modules/clustering.rst
msgid "References"
msgstr ""

#: ../../modules/clustering.rst:1000
msgid ""
"`Comparing Partitions "
"<http://www.springerlink.com/content/x64124718341j1j0/>`_ L. Hubert and "
"P. Arabie, Journal of Classification 1985"
msgstr ""

#: ../../modules/clustering.rst:1004
msgid ""
"`Wikipedia entry for the adjusted Rand index "
"<https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_"
msgstr ""

#: ../../modules/clustering.rst:1010
msgid "Mutual Information based scores"
msgstr ""

#: ../../modules/clustering.rst:1012
msgid ""
"Given the knowledge of the ground truth class assignments ``labels_true``"
" and our clustering algorithm assignments of the same samples "
"``labels_pred``, the **Mutual Information** is a function that measures "
"the **agreement** of the two assignments, ignoring permutations.  Two "
"different normalized versions of this measure are available, **Normalized"
" Mutual Information(NMI)** and **Adjusted Mutual Information(AMI)**. NMI "
"is often used in the literature while AMI was proposed more recently and "
"is **normalized against chance**::"
msgstr ""

#: ../../modules/clustering.rst:1027
msgid ""
"One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get "
"the same score::"
msgstr ""

#: ../../modules/clustering.rst:1034
msgid ""
"All, :func:`mutual_info_score`, :func:`adjusted_mutual_info_score` and "
":func:`normalized_mutual_info_score` are symmetric: swapping the argument"
" does not change the score. Thus they can be used as a **consensus "
"measure**::"
msgstr ""

#: ../../modules/clustering.rst:1050
msgid ""
"This is not true for ``mutual_info_score``, which is therefore harder to "
"judge::"
msgstr ""

#: ../../modules/clustering.rst:1055
msgid "Bad (e.g. independent labelings) have non-positive scores::"
msgstr ""

#: ../../modules/clustering.rst:1066
msgid ""
"**Random (uniform) label assignments have a AMI score close to 0.0** for "
"any value of ``n_clusters`` and ``n_samples`` (which is not the case for "
"raw Mutual Information or the V-measure for instance)."
msgstr ""

#: ../../modules/clustering.rst:1070
msgid ""
"**Bounded range [0, 1]**:  Values close to zero indicate two label "
"assignments that are largely independent, while values close to one "
"indicate significant agreement. Further, values of exactly 0 indicate "
"**purely** independent label assignments and a AMI of exactly 1 indicates"
" that the two label assignments are equal (with or without permutation)."
msgstr ""

#: ../../modules/clustering.rst:1085
msgid ""
"Contrary to inertia, **MI-based measures require the knowledge of the "
"ground truth classes** while almost never available in practice or "
"requires manual assignment by human annotators (as in the supervised "
"learning setting)."
msgstr ""

#: ../../modules/clustering.rst:1090
msgid ""
"However MI-based measures can also be useful in purely unsupervised "
"setting as a building block for a Consensus Index that can be used for "
"clustering model selection."
msgstr ""

#: ../../modules/clustering.rst:1094
msgid "NMI and MI are not adjusted against chance."
msgstr ""

#: ../../modules/clustering.rst:1099
msgid ""
":ref:`example_cluster_plot_adjusted_for_chance_measures.py`: Analysis of "
"the impact of the dataset size on the value of clustering measures for "
"random assignments. This example also includes the Adjusted Rand Index."
msgstr ""

#: ../../modules/clustering.rst:1108
msgid ""
"Assume two label assignments (of the same N objects), :math:`U` and "
":math:`V`. Their entropy is the amount of uncertainty for a partition "
"set, defined by:"
msgstr ""

#: ../../modules/clustering.rst:1113
msgid ""
"where :math:`P(i) = |U_i| / N` is the probability that an object picked "
"at random from :math:`U` falls into class :math:`U_i`. Likewise for "
":math:`V`:"
msgstr ""

#: ../../modules/clustering.rst:1118
msgid ""
"With :math:`P'(j) = |V_j| / N`. The mutual information (MI) between "
":math:`U` and :math:`V` is calculated by:"
msgstr ""

#: ../../modules/clustering.rst:1123
msgid ""
"where :math:`P(i, j) = |U_i \\cap V_j| / N` is the probability that an "
"object picked at random falls into both classes :math:`U_i` and "
":math:`V_j`."
msgstr ""

#: ../../modules/clustering.rst:1126
msgid "The normalized mutual information is defined as"
msgstr ""

#: ../../modules/clustering.rst:1130
msgid ""
"This value of the mutual information and also the normalized variant is "
"not adjusted for chance and will tend to increase as the number of "
"different labels (clusters) increases, regardless of the actual amount of"
" \"mutual information\" between the label assignments."
msgstr ""

#: ../../modules/clustering.rst:1135
msgid ""
"The expected value for the mutual information can be calculated using the"
" following equation, from Vinh, Epps, and Bailey, (2009). In this "
"equation, :math:`a_i = |U_i|` (the number of elements in :math:`U_i`) and"
" :math:`b_j = |V_j|` (the number of elements in :math:`V_j`)."
msgstr ""

#: ../../modules/clustering.rst:1146
msgid ""
"Using the expected value, the adjusted mutual information can then be "
"calculated using a similar form to that of the adjusted Rand index:"
msgstr ""

#: ../../modules/clustering.rst:1153
msgid ""
"Strehl, Alexander, and Joydeep Ghosh (2002). \"Cluster ensembles â€“ a "
"knowledge reuse framework for combining multiple partitions\". Journal of"
" Machine Learning Research 3: 583â€“617. `doi:10.1162/153244303321897735 "
"<http://strehl.com/download/strehl-jmlr02.pdf>`_."
msgstr ""

#: ../../modules/clustering.rst:1158
msgid ""
"Vinh, Epps, and Bailey, (2009). \"Information theoretic measures for "
"clusterings comparison\". Proceedings of the 26th Annual International "
"Conference on Machine Learning - ICML '09. `doi:10.1145/1553374.1553511 "
"<http://dx.doi.org/10.1145/1553374.1553511>`_. ISBN 9781605585161."
msgstr ""

#: ../../modules/clustering.rst:1164
msgid ""
"Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for "
"Clusterings Comparison: Variants, Properties, Normalization and "
"Correction for Chance, JMLR "
"http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf"
msgstr ""

#: ../../modules/clustering.rst:1169
msgid ""
"`Wikipedia entry for the (normalized) Mutual Information "
"<https://en.wikipedia.org/wiki/Mutual_Information>`_"
msgstr ""

#: ../../modules/clustering.rst:1172
msgid ""
"`Wikipedia entry for the Adjusted Mutual Information "
"<https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_"
msgstr ""

#: ../../modules/clustering.rst:1178
msgid "Homogeneity, completeness and V-measure"
msgstr ""

#: ../../modules/clustering.rst:1180
msgid ""
"Given the knowledge of the ground truth class assignments of the samples,"
" it is possible to define some intuitive metric using conditional entropy"
" analysis."
msgstr ""

#: ../../modules/clustering.rst:1184
msgid ""
"In particular Rosenberg and Hirschberg (2007) define the following two "
"desirable objectives for any cluster assignment:"
msgstr ""

#: ../../modules/clustering.rst:1187
msgid "**homogeneity**: each cluster contains only members of a single class."
msgstr ""

#: ../../modules/clustering.rst:1189
msgid ""
"**completeness**: all members of a given class are assigned to the same "
"cluster."
msgstr ""

#: ../../modules/clustering.rst:1192
msgid ""
"We can turn those concept as scores :func:`homogeneity_score` and "
":func:`completeness_score`. Both are bounded below by 0.0 and above by "
"1.0 (higher is better)::"
msgstr ""

#: ../../modules/clustering.rst:1206
msgid ""
"Their harmonic mean called **V-measure** is computed by "
":func:`v_measure_score`::"
msgstr ""

#: ../../modules/clustering.rst:1212
msgid ""
"The V-measure is actually equivalent to the mutual information (NMI) "
"discussed above normalized by the sum of the label entropies [B2011]_."
msgstr ""

#: ../../modules/clustering.rst:1215
msgid ""
"Homogeneity, completeness and V-measure can be computed at once using "
":func:`homogeneity_completeness_v_measure` as follows::"
msgstr ""

#: ../../modules/clustering.rst:1222
msgid ""
"The following clustering assignment is slightly better, since it is "
"homogeneous but not complete::"
msgstr ""

#: ../../modules/clustering.rst:1232
msgid ""
":func:`v_measure_score` is **symmetric**: it can be used to evaluate the "
"**agreement** of two independent assignments on the same dataset."
msgstr ""

#: ../../modules/clustering.rst:1235
msgid ""
"This is not the case for :func:`completeness_score` and "
":func:`homogeneity_score`: both are bound by the relationship::"
msgstr ""

#: ../../modules/clustering.rst:1244
msgid "**Bounded scores**: 0.0 is as bad as it can be, 1.0 is a perfect score."
msgstr ""

#: ../../modules/clustering.rst:1246
msgid ""
"Intuitive interpretation: clustering with bad V-measure can be "
"**qualitatively analyzed in terms of homogeneity and completeness** to "
"better feel what 'kind' of mistakes is done by the assignment."
msgstr ""

#: ../../modules/clustering.rst:1259
msgid ""
"The previously introduced metrics are **not normalized with regards to "
"random labeling**: this means that depending on the number of samples, "
"clusters and ground truth classes, a completely random labeling will not "
"always yield the same values for homogeneity, completeness and hence "
"v-measure. In particular **random labeling won't yield zero scores "
"especially when the number of clusters is large**."
msgstr ""

#: ../../modules/clustering.rst:1266
msgid ""
"This problem can safely be ignored when the number of samples is more "
"than a thousand and the number of clusters is less than 10. **For smaller"
" sample sizes or larger number of clusters it is safer to use an adjusted"
" index such as the Adjusted Rand Index (ARI)**."
msgstr ""

#: ../../modules/clustering.rst:1276
msgid ""
"These metrics **require the knowledge of the ground truth classes** while"
" almost never available in practice or requires manual assignment by "
"human annotators (as in the supervised learning setting)."
msgstr ""

#: ../../modules/clustering.rst:1291
msgid "Homogeneity and completeness scores are formally given by:"
msgstr ""

#: ../../modules/clustering.rst:1297
msgid ""
"where :math:`H(C|K)` is the **conditional entropy of the classes given "
"the cluster assignments** and is given by:"
msgstr ""

#: ../../modules/clustering.rst:1303
msgid "and :math:`H(C)` is the **entropy of the classes** and is given by:"
msgstr ""

#: ../../modules/clustering.rst:1307
msgid ""
"with :math:`n` the total number of samples, :math:`n_c` and :math:`n_k` "
"the number of samples respectively belonging to class :math:`c` and "
"cluster :math:`k`, and finally :math:`n_{c,k}` the number of samples from"
" class :math:`c` assigned to cluster :math:`k`."
msgstr ""

#: ../../modules/clustering.rst:1312
msgid ""
"The **conditional entropy of clusters given class** :math:`H(K|C)` and "
"the **entropy of clusters** :math:`H(K)` are defined in a symmetric "
"manner."
msgstr ""

#: ../../modules/clustering.rst:1315
msgid ""
"Rosenberg and Hirschberg further define **V-measure** as the **harmonic "
"mean of homogeneity and completeness**:"
msgstr ""

#: ../../modules/clustering.rst:1322
msgid ""
"`V-Measure: A conditional entropy-based external cluster evaluation "
"measure <http://aclweb.org/anthology/D/D07/D07-1043.pdf>`_ Andrew "
"Rosenberg and Julia Hirschberg, 2007"
msgstr ""

#: ../../modules/clustering.rst:1326
msgid ""
"`Identication and Characterization of Events in Social Media "
"<http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf>`_, Hila "
"Becker, PhD Thesis."
msgstr ""

#: ../../modules/clustering.rst:1333
msgid "Silhouette Coefficient"
msgstr ""

#: ../../modules/clustering.rst:1335
msgid ""
"If the ground truth labels are not known, evaluation must be performed "
"using the model itself. The Silhouette Coefficient "
"(:func:`sklearn.metrics.silhouette_score`) is an example of such an "
"evaluation, where a higher Silhouette Coefficient score relates to a "
"model with better defined clusters. The Silhouette Coefficient is defined"
" for each sample and is composed of two scores:"
msgstr ""

#: ../../modules/clustering.rst:1343
msgid ""
"**a**: The mean distance between a sample and all other points in the "
"same class."
msgstr ""

#: ../../modules/clustering.rst:1346
msgid ""
"**b**: The mean distance between a sample and all other points in the "
"*next nearest cluster*."
msgstr ""

#: ../../modules/clustering.rst:1349
msgid "The Silhouette Coefficient *s* for a single sample is then given as:"
msgstr ""

#: ../../modules/clustering.rst:1353
msgid ""
"The Silhouette Coefficient for a set of samples is given as the mean of "
"the Silhouette Coefficient for each sample."
msgstr ""

#: ../../modules/clustering.rst:1364
msgid ""
"In normal usage, the Silhouette Coefficient is applied to the results of "
"a cluster analysis."
msgstr ""

#: ../../modules/clustering.rst:1377
msgid ""
"Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the "
"Interpretation and Validation of Cluster Analysis\". Computational and "
"Applied Mathematics 20: 53â€“65. `doi:10.1016/0377-0427(87)90125-7 "
"<http://dx.doi.org/10.1016/0377-0427(87)90125-7>`_."
msgstr ""

#: ../../modules/clustering.rst:1386
msgid ""
"The score is bounded between -1 for incorrect clustering and +1 for "
"highly dense clustering. Scores around zero indicate overlapping "
"clusters."
msgstr ""

#: ../../modules/clustering.rst:1389
msgid ""
"The score is higher when clusters are dense and well separated, which "
"relates to a standard concept of a cluster."
msgstr ""

#: ../../modules/clustering.rst:1396
msgid ""
"The Silhouette Coefficient is generally higher for convex clusters than "
"other concepts of clusters, such as density based clusters like those "
"obtained through DBSCAN."
msgstr ""

#: ../../modules/clustering.rst:1402
msgid ""
":ref:`example_cluster_plot_kmeans_silhouette_analysis.py` : In this "
"example the silhouette analysis is used to choose an optimal value for "
"n_clusters."
msgstr ""

