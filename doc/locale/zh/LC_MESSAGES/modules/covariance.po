# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/covariance.rst:5
msgid "Covariance estimation"
msgstr ""

#: ../../modules/covariance.rst:10
msgid ""
"Many statistical problems require at some point the estimation of a "
"population's covariance matrix, which can be seen as an estimation of "
"data set scatter plot shape. Most of the time, such an estimation has to "
"be done on a sample whose properties (size, structure, homogeneity) has a"
" large influence on the estimation's quality. The `sklearn.covariance` "
"package aims at providing tools affording an accurate estimation of a "
"population's covariance matrix under various settings."
msgstr ""

#: ../../modules/covariance.rst:19
msgid ""
"We assume that the observations are independent and identically "
"distributed (i.i.d.)."
msgstr ""

#: ../../modules/covariance.rst:24
msgid "Empirical covariance"
msgstr ""

#: ../../modules/covariance.rst:26
msgid ""
"The covariance matrix of a data set is known to be well approximated with"
" the classical *maximum likelihood estimator* (or \"empirical "
"covariance\"), provided the number of observations is large enough "
"compared to the number of features (the variables describing the "
"observations). More precisely, the Maximum Likelihood Estimator of a "
"sample is an unbiased estimator of the corresponding population "
"covariance matrix."
msgstr ""

#: ../../modules/covariance.rst:34
msgid ""
"The empirical covariance matrix of a sample can be computed using the "
":func:`empirical_covariance` function of the package, or by fitting an "
":class:`EmpiricalCovariance` object to the data sample with the "
":meth:`EmpiricalCovariance.fit` method.  Be careful that depending "
"whether the data are centered or not, the result will be different, so "
"one may want to use the ``assume_centered`` parameter accurately. More "
"precisely if one uses ``assume_centered=False``, then the test set is "
"supposed to have the same mean vector as the training set. If not so, "
"both should be centered by the user, and ``assume_centered=True`` should "
"be used."
msgstr ""

#: ../../modules/covariance.rst
msgid "Examples:"
msgstr ""

#: ../../modules/covariance.rst:46
msgid ""
"See :ref:`example_covariance_plot_covariance_estimation.py` for an "
"example on how to fit an :class:`EmpiricalCovariance` object to data."
msgstr ""

#: ../../modules/covariance.rst:54
msgid "Shrunk Covariance"
msgstr ""

#: ../../modules/covariance.rst:57
msgid "Basic shrinkage"
msgstr ""

#: ../../modules/covariance.rst:59
msgid ""
"Despite being an unbiased estimator of the covariance matrix, the Maximum"
" Likelihood Estimator is not a good estimator of the eigenvalues of the "
"covariance matrix, so the precision matrix obtained from its inversion is"
" not accurate. Sometimes, it even occurs that the empirical covariance "
"matrix cannot be inverted for numerical reasons. To avoid such an "
"inversion problem, a transformation of the empirical covariance matrix "
"has been introduced: the ``shrinkage``."
msgstr ""

#: ../../modules/covariance.rst:67
msgid ""
"In the scikit-learn, this transformation (with a user-defined shrinkage "
"coefficient) can be directly applied to a pre-computed covariance with "
"the :func:`shrunk_covariance` method. Also, a shrunk estimator of the "
"covariance can be fitted to data with a :class:`ShrunkCovariance` object "
"and its :meth:`ShrunkCovariance.fit` method.  Again, depending whether "
"the data are centered or not, the result will be different, so one may "
"want to use the ``assume_centered`` parameter accurately."
msgstr ""

#: ../../modules/covariance.rst:76
msgid ""
"Mathematically, this shrinkage consists in reducing the ratio between the"
" smallest and the largest eigenvalue of the empirical covariance matrix. "
"It can be done by simply shifting every eigenvalue according to a given "
"offset, which is equivalent of finding the l2-penalized Maximum "
"Likelihood Estimator of the covariance matrix. In practice, shrinkage "
"boils down to a simple a convex transformation : :math:`\\Sigma_{\\rm "
"shrunk} = (1-\\alpha)\\hat{\\Sigma} + \\alpha\\frac{{\\rm "
"Tr}\\hat{\\Sigma}}{p}\\rm Id`."
msgstr ""

#: ../../modules/covariance.rst:85
msgid ""
"Choosing the amount of shrinkage, :math:`\\alpha` amounts to setting a "
"bias/variance trade-off, and is discussed below."
msgstr ""

#: ../../modules/covariance.rst:90
msgid ""
"See :ref:`example_covariance_plot_covariance_estimation.py` for an "
"example on how to fit a :class:`ShrunkCovariance` object to data."
msgstr ""

#: ../../modules/covariance.rst:96
msgid "Ledoit-Wolf shrinkage"
msgstr ""

#: ../../modules/covariance.rst:98
msgid ""
"In their 2004 paper [1], O. Ledoit and M. Wolf propose a formula so as to"
" compute the optimal shrinkage coefficient :math:`\\alpha` that minimizes"
" the Mean Squared Error between the estimated and the real covariance "
"matrix."
msgstr ""

#: ../../modules/covariance.rst:103
msgid ""
"The Ledoit-Wolf estimator of the covariance matrix can be computed on a "
"sample with the :meth:`ledoit_wolf` function of the `sklearn.covariance` "
"package, or it can be otherwise obtained by fitting a :class:`LedoitWolf`"
" object to the same sample."
msgstr ""

#: ../../modules/covariance.rst:110
msgid ""
"See :ref:`example_covariance_plot_covariance_estimation.py` for an "
"example on how to fit a :class:`LedoitWolf` object to data and for "
"visualizing the performances of the Ledoit-Wolf estimator in terms of "
"likelihood."
msgstr ""

#: ../../modules/covariance.rst:118
msgid ""
"[1] O. Ledoit and M. Wolf, \"A Well-Conditioned Estimator for Large-"
"Dimensional"
msgstr ""

#: ../../modules/covariance.rst:117
msgid ""
"Covariance Matrices\", Journal of Multivariate Analysis, Volume 88, Issue"
" 2, February 2004, pages 365-411."
msgstr ""

#: ../../modules/covariance.rst:123
msgid "Oracle Approximating Shrinkage"
msgstr ""

#: ../../modules/covariance.rst:125
msgid ""
"Under the assumption that the data are Gaussian distributed, Chen et al. "
"[2] derived a formula aimed at choosing a shrinkage coefficient that "
"yields a smaller Mean Squared Error than the one given by Ledoit and "
"Wolf's formula. The resulting estimator is known as the Oracle Shrinkage "
"Approximating estimator of the covariance."
msgstr ""

#: ../../modules/covariance.rst:131
msgid ""
"The OAS estimator of the covariance matrix can be computed on a sample "
"with the :meth:`oas` function of the `sklearn.covariance` package, or it "
"can be otherwise obtained by fitting an :class:`OAS` object to the same "
"sample."
msgstr ""

#: ../../modules/covariance.rst:141
msgid ""
"Bias-variance trade-off when setting the shrinkage: comparing the choices"
" of Ledoit-Wolf and OAS estimators"
msgstr ""

#: ../../modules/covariance.rst:145
msgid "[2] Chen et al., \"Shrinkage Algorithms for MMSE Covariance Estimation\","
msgstr ""

#: ../../modules/covariance.rst:145
msgid "IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010."
msgstr ""

#: ../../modules/covariance.rst:149
msgid ""
"See :ref:`example_covariance_plot_covariance_estimation.py` for an "
"example on how to fit an :class:`OAS` object to data."
msgstr ""

#: ../../modules/covariance.rst:153
msgid ""
"See :ref:`example_covariance_plot_lw_vs_oas.py` to visualize the Mean "
"Squared Error difference between a :class:`LedoitWolf` and an "
":class:`OAS` estimator of the covariance."
msgstr ""

#: ../../modules/covariance.rst:167
msgid "Sparse inverse covariance"
msgstr ""

#: ../../modules/covariance.rst:169
msgid ""
"The matrix inverse of the covariance matrix, often called the precision "
"matrix, is proportional to the partial correlation matrix. It gives the "
"partial independence relationship. In other words, if two features are "
"independent conditionally on the others, the corresponding coefficient in"
" the precision matrix will be zero. This is why it makes sense to "
"estimate a sparse precision matrix: by learning independence relations "
"from the data, the estimation of the covariance matrix is better "
"conditioned. This is known as *covariance selection*."
msgstr ""

#: ../../modules/covariance.rst:178
msgid ""
"In the small-samples situation, in which ``n_samples`` is on the order of"
" ``n_features`` or smaller, sparse inverse covariance estimators tend to "
"work better than shrunk covariance estimators. However, in the opposite "
"situation, or for very correlated data, they can be numerically unstable."
" In addition, unlike shrinkage estimators, sparse estimators are able to "
"recover off-diagonal structure."
msgstr ""

#: ../../modules/covariance.rst:185
msgid ""
"The :class:`GraphLasso` estimator uses an l1 penalty to enforce sparsity "
"on the precision matrix: the higher its ``alpha`` parameter, the more "
"sparse the precision matrix. The corresponding :class:`GraphLassoCV` "
"object uses cross-validation to automatically set the ``alpha`` "
"parameter."
msgstr ""

#: ../../modules/covariance.rst:195
msgid ""
"*A comparison of maximum likelihood, shrinkage and sparse estimates of "
"the covariance and precision matrix in the very small samples settings.*"
msgstr ""

#: ../../modules/covariance.rst:199
msgid "**Structure recovery**"
msgstr ""

#: ../../modules/covariance.rst:201
msgid ""
"Recovering a graphical structure from correlations in the data is a "
"challenging thing. If you are interested in such recovery keep in mind "
"that:"
msgstr ""

#: ../../modules/covariance.rst:205
msgid ""
"Recovery is easier from a correlation matrix than a covariance matrix: "
"standardize your observations before running :class:`GraphLasso`"
msgstr ""

#: ../../modules/covariance.rst:208
msgid ""
"If the underlying graph has nodes with much more connections than the "
"average node, the algorithm will miss some of these connections."
msgstr ""

#: ../../modules/covariance.rst:211
msgid ""
"If your number of observations is not large compared to the number of "
"edges in your underlying graph, you will not recover it."
msgstr ""

#: ../../modules/covariance.rst:214
msgid ""
"Even if you are in favorable recovery conditions, the alpha parameter "
"chosen by cross-validation (e.g. using the :class:`GraphLassoCV` object) "
"will lead to selecting too many edges. However, the relevant edges will "
"have heavier weights than the irrelevant ones."
msgstr ""

#: ../../modules/covariance.rst:220
msgid "The mathematical formulation is the following:"
msgstr ""

#: ../../modules/covariance.rst:229
msgid ""
"Where :math:`K` is the precision matrix to be estimated, and :math:`S` is"
" the sample covariance matrix. :math:`\\|K\\|_1` is the sum of the "
"absolute values of off-diagonal coefficients of :math:`K`. The algorithm "
"employed to solve this problem is the GLasso algorithm, from the Friedman"
" 2008 Biostatistics paper. It is the same algorithm as in the R "
"``glasso`` package."
msgstr ""

#: ../../modules/covariance.rst:238
msgid ""
":ref:`example_covariance_plot_sparse_cov.py`: example on synthetic data "
"showing some recovery of a structure, and comparing to other covariance "
"estimators."
msgstr ""

#: ../../modules/covariance.rst:242
msgid ""
":ref:`example_applications_plot_stock_market.py`: example on real stock "
"market data, finding which symbols are most linked."
msgstr ""

#: ../../modules/covariance.rst
msgid "References:"
msgstr ""

#: ../../modules/covariance.rst:247
msgid ""
"Friedman et al, `\"Sparse inverse covariance estimation with the "
"graphical lasso\" "
"<http://biostatistics.oxfordjournals.org/content/9/3/432.short>`_, "
"Biostatistics 9, pp 432, 2008"
msgstr ""

#: ../../modules/covariance.rst:254
msgid "Robust Covariance Estimation"
msgstr ""

#: ../../modules/covariance.rst:256
msgid ""
"Real data set are often subjects to measurement or recording errors. "
"Regular but uncommon observations may also appear for a variety of "
"reason. Every observation which is very uncommon is called an outlier. "
"The empirical covariance estimator and the shrunk covariance estimators "
"presented above are very sensitive to the presence of outlying "
"observations in the data. Therefore, one should use robust covariance "
"estimators to estimate the covariance of its real data sets. "
"Alternatively, robust covariance estimators can be used to perform "
"outlier detection and discard/downweight some observations according to "
"further processing of the data."
msgstr ""

#: ../../modules/covariance.rst:268
msgid ""
"The ``sklearn.covariance`` package implements a robust estimator of "
"covariance, the Minimum Covariance Determinant [3]."
msgstr ""

#: ../../modules/covariance.rst:273
msgid "Minimum Covariance Determinant"
msgstr ""

#: ../../modules/covariance.rst:275
msgid ""
"The Minimum Covariance Determinant estimator is a robust estimator of a "
"data set's covariance introduced by P.J. Rousseeuw in [3].  The idea is "
"to find a given proportion (h) of \"good\" observations which are not "
"outliers and compute their empirical covariance matrix.  This empirical "
"covariance matrix is then rescaled to compensate the performed selection "
"of observations (\"consistency step\").  Having computed the Minimum "
"Covariance Determinant estimator, one can give weights to observations "
"according to their Mahalanobis distance, leading the a reweighted "
"estimate of the covariance matrix of the data set (\"reweighting step\")."
msgstr ""

#: ../../modules/covariance.rst:286
msgid ""
"Rousseeuw and Van Driessen [4] developed the FastMCD algorithm in order "
"to compute the Minimum Covariance Determinant. This algorithm is used in "
"scikit-learn when fitting an MCD object to data. The FastMCD algorithm "
"also computes a robust estimate of the data set location at the same "
"time."
msgstr ""

#: ../../modules/covariance.rst:292
msgid ""
"Raw estimates can be accessed as ``raw_location_`` and "
"``raw_covariance_`` attributes of a :class:`MinCovDet` robust covariance "
"estimator object."
msgstr ""

#: ../../modules/covariance.rst:295
msgid "[3] P. J. Rousseeuw. Least median of squares regression."
msgstr ""

#: ../../modules/covariance.rst:296
msgid "Am Stat Ass, 79:871, 1984."
msgstr ""

#: ../../modules/covariance.rst:299
msgid "[4] A Fast Algorithm for the Minimum Covariance Determinant Estimator,"
msgstr ""

#: ../../modules/covariance.rst:298
msgid ""
"1999, American Statistical Association and the American Society for "
"Quality, TECHNOMETRICS."
msgstr ""

#: ../../modules/covariance.rst:303
msgid ""
"See :ref:`example_covariance_plot_robust_vs_empirical_covariance.py` for "
"an example on how to fit a :class:`MinCovDet` object to data and see how "
"the estimate remains accurate despite the presence of outliers."
msgstr ""

#: ../../modules/covariance.rst:307
msgid ""
"See :ref:`example_covariance_plot_mahalanobis_distances.py` to visualize "
"the difference between :class:`EmpiricalCovariance` and "
":class:`MinCovDet` covariance estimators in terms of Mahalanobis distance"
" (so we get a better estimate of the precision matrix too)."
msgstr ""

#: ../../modules/covariance.rst:327
msgid "Influence of outliers on location and covariance estimates"
msgstr ""

#: ../../modules/covariance.rst:328
msgid "Separating inliers from outliers using a Mahalanobis distance"
msgstr ""

#: ../../modules/covariance.rst:330
msgid "|robust_vs_emp|"
msgstr ""

#: ../../modules/covariance.rst:331
msgid "|mahalanobis|"
msgstr ""

