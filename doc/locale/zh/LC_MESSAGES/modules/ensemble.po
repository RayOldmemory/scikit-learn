# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/ensemble.rst:5
msgid "Ensemble methods"
msgstr ""

#: ../../modules/ensemble.rst:9
msgid ""
"The goal of **ensemble methods** is to combine the predictions of several"
" base estimators built with a given learning algorithm in order to "
"improve generalizability / robustness over a single estimator."
msgstr ""

#: ../../modules/ensemble.rst:13
msgid "Two families of ensemble methods are usually distinguished:"
msgstr ""

#: ../../modules/ensemble.rst:15
msgid ""
"In **averaging methods**, the driving principle is to build several "
"estimators independently and then to average their predictions. On "
"average, the combined estimator is usually better than any of the single "
"base estimator because its variance is reduced."
msgstr ""

#: ../../modules/ensemble.rst:20
msgid ""
"**Examples:** :ref:`Bagging methods <bagging>`, :ref:`Forests of "
"randomized trees <forest>`, ..."
msgstr ""

#: ../../modules/ensemble.rst:22
msgid ""
"By contrast, in **boosting methods**, base estimators are built "
"sequentially and one tries to reduce the bias of the combined estimator. "
"The motivation is to combine several weak models to produce a powerful "
"ensemble."
msgstr ""

#: ../../modules/ensemble.rst:26
msgid ""
"**Examples:** :ref:`AdaBoost <adaboost>`, :ref:`Gradient Tree Boosting "
"<gradient_boosting>`, ..."
msgstr ""

#: ../../modules/ensemble.rst:32
msgid "Bagging meta-estimator"
msgstr ""

#: ../../modules/ensemble.rst:34
msgid ""
"In ensemble algorithms, bagging methods form a class of algorithms which "
"build several instances of a black-box estimator on random subsets of the"
" original training set and then aggregate their individual predictions to"
" form a final prediction. These methods are used as a way to reduce the "
"variance of a base estimator (e.g., a decision tree), by introducing "
"randomization into its construction procedure and then making an ensemble"
" out of it. In many cases, bagging methods constitute a very simple way "
"to improve with respect to a single model, without making it necessary to"
" adapt the underlying base algorithm. As they provide a way to reduce "
"overfitting, bagging methods work best with strong and complex models "
"(e.g., fully developed decision trees), in contrast with boosting methods"
" which usually work best with weak models (e.g., shallow decision trees)."
msgstr ""

#: ../../modules/ensemble.rst:47
msgid ""
"Bagging methods come in many flavours but mostly differ from each other "
"by the way they draw random subsets of the training set:"
msgstr ""

#: ../../modules/ensemble.rst:50
msgid ""
"When random subsets of the dataset are drawn as random subsets of the "
"samples, then this algorithm is known as Pasting [B1999]_."
msgstr ""

#: ../../modules/ensemble.rst:53
msgid ""
"When samples are drawn with replacement, then the method is known as "
"Bagging [B1996]_."
msgstr ""

#: ../../modules/ensemble.rst:56
msgid ""
"When random subsets of the dataset are drawn as random subsets of the "
"features, then the method is known as Random Subspaces [H1998]_."
msgstr ""

#: ../../modules/ensemble.rst:59
msgid ""
"Finally, when base estimators are built on subsets of both samples and "
"features, then the method is known as Random Patches [LG2012]_."
msgstr ""

#: ../../modules/ensemble.rst:62
#, python-format
msgid ""
"In scikit-learn, bagging methods are offered as a unified "
":class:`BaggingClassifier` meta-estimator  (resp. "
":class:`BaggingRegressor`), taking as input a user-specified base "
"estimator along with parameters specifying the strategy to draw random "
"subsets. In particular, ``max_samples`` and ``max_features`` control the "
"size of the subsets (in terms of samples and features), while "
"``bootstrap`` and ``bootstrap_features`` control whether samples and "
"features are drawn with or without replacement. When using a subset of "
"the available samples the generalization error can be estimated with the "
"out-of-bag samples by setting ``oob_score=True``. As an example, the "
"snippet below illustrates how to instantiate a bagging ensemble of "
":class:`KNeighborsClassifier` base estimators, each built on random "
"subsets of 50% of the samples and 50% of the features."
msgstr ""

#: ../../modules/ensemble.rst
msgid "Examples:"
msgstr ""

#: ../../modules/ensemble.rst:82
msgid ":ref:`example_ensemble_plot_bias_variance.py`"
msgstr ""

#: ../../modules/ensemble.rst
msgid "References"
msgstr ""

#: ../../modules/ensemble.rst:86
msgid ""
"L. Breiman, \"Pasting small votes for classification in large databases "
"and on-line\", Machine Learning, 36(1), 85-103, 1999."
msgstr ""

#: ../../modules/ensemble.rst:89
msgid ""
"L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140, "
"1996."
msgstr ""

#: ../../modules/ensemble.rst:92
msgid ""
"T. Ho, \"The random subspace method for constructing decision forests\", "
"Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998."
msgstr ""

#: ../../modules/ensemble.rst:96
msgid ""
"G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine "
"Learning and Knowledge Discovery in Databases, 346-361, 2012."
msgstr ""

#: ../../modules/ensemble.rst:102
msgid "Forests of randomized trees"
msgstr ""

#: ../../modules/ensemble.rst:104
msgid ""
"The :mod:`sklearn.ensemble` module includes two averaging algorithms "
"based on randomized :ref:`decision trees <tree>`: the RandomForest "
"algorithm and the Extra-Trees method. Both algorithms are perturb-and-"
"combine techniques [B1998]_ specifically designed for trees. This means a"
" diverse set of classifiers is created by introducing randomness in the "
"classifier construction.  The prediction of the ensemble is given as the "
"averaged prediction of the individual classifiers."
msgstr ""

#: ../../modules/ensemble.rst:112
msgid ""
"As other classifiers, forest classifiers have to be fitted with two "
"arrays: a sparse or dense array X of size ``[n_samples, n_features]`` "
"holding the training samples, and an array Y of size ``[n_samples]`` "
"holding the target values (class labels) for the training samples::"
msgstr ""

#: ../../modules/ensemble.rst:123
msgid ""
"Like :ref:`decision trees <tree>`, forests of trees also extend to :ref"
":`multi-output problems <tree_multioutput>`  (if Y is an array of size "
"``[n_samples, n_outputs]``)."
msgstr ""

#: ../../modules/ensemble.rst:128
msgid "Random Forests"
msgstr ""

#: ../../modules/ensemble.rst:130
msgid ""
"In random forests (see :class:`RandomForestClassifier` and "
":class:`RandomForestRegressor` classes), each tree in the ensemble is "
"built from a sample drawn with replacement (i.e., a bootstrap sample) "
"from the training set. In addition, when splitting a node during the "
"construction of the tree, the split that is chosen is no longer the best "
"split among all features. Instead, the split that is picked is the best "
"split among a random subset of the features. As a result of this "
"randomness, the bias of the forest usually slightly increases (with "
"respect to the bias of a single non-random tree) but, due to averaging, "
"its variance also decreases, usually more than compensating for the "
"increase in bias, hence yielding an overall better model."
msgstr ""

#: ../../modules/ensemble.rst:142
msgid ""
"In contrast to the original publication [B2001]_, the scikit-learn "
"implementation combines classifiers by averaging their probabilistic "
"prediction, instead of letting each classifier vote for a single class."
msgstr ""

#: ../../modules/ensemble.rst:147
msgid "Extremely Randomized Trees"
msgstr ""

#: ../../modules/ensemble.rst:149
msgid ""
"In extremely randomized trees (see :class:`ExtraTreesClassifier` and "
":class:`ExtraTreesRegressor` classes), randomness goes one step further "
"in the way splits are computed. As in random forests, a random subset of "
"candidate features is used, but instead of looking for the most "
"discriminative thresholds, thresholds are drawn at random for each "
"candidate feature and the best of these randomly-generated thresholds is "
"picked as the splitting rule. This usually allows to reduce the variance "
"of the model a bit more, at the expense of a slightly greater increase in"
" bias::"
msgstr ""

#: ../../modules/ensemble.rst:192
msgid "Parameters"
msgstr ""

#: ../../modules/ensemble.rst:194
msgid ""
"The main parameters to adjust when using these methods is "
"``n_estimators`` and ``max_features``. The former is the number of trees "
"in the forest. The larger the better, but also the longer it will take to"
" compute. In addition, note that results will stop getting significantly "
"better beyond a critical number of trees. The latter is the size of the "
"random subsets of features to consider when splitting a node. The lower "
"the greater the reduction of variance, but also the greater the increase "
"in bias. Empirical good default values are ``max_features=n_features`` "
"for regression problems, and ``max_features=sqrt(n_features)`` for "
"classification tasks (where ``n_features`` is the number of features in "
"the data). Good results are often achieved when setting "
"``max_depth=None`` in combination with ``min_samples_split=1`` (i.e., "
"when fully developing the trees). Bear in mind though that these values "
"are usually not optimal, and might result in models that consume a lot of"
" ram. The best parameter values should always be cross-validated. In "
"addition, note that in random forests, bootstrap samples are used by "
"default (``bootstrap=True``) while the default strategy for extra-trees "
"is to use the whole dataset (``bootstrap=False``). When using bootstrap "
"sampling the generalization error can be estimated on the left out or "
"out-of-bag samples. This can be enabled by setting ``oob_score=True``."
msgstr ""

#: ../../modules/ensemble.rst:217
msgid "Parallelization"
msgstr ""

#: ../../modules/ensemble.rst:219
msgid ""
"Finally, this module also features the parallel construction of the trees"
" and the parallel computation of the predictions through the ``n_jobs`` "
"parameter. If ``n_jobs=k`` then computations are partitioned into ``k`` "
"jobs, and run on ``k`` cores of the machine. If ``n_jobs=-1`` then all "
"cores available on the machine are used. Note that because of inter-"
"process communication overhead, the speedup might not be linear (i.e., "
"using ``k`` jobs will unfortunately not be ``k`` times as fast). "
"Significant speedup can still be achieved though when building a large "
"number of trees, or when building a single tree requires a fair amount of"
" time (e.g., on large datasets)."
msgstr ""

#: ../../modules/ensemble.rst:232
msgid ":ref:`example_ensemble_plot_forest_iris.py`"
msgstr ""

#: ../../modules/ensemble.rst:233 ../../modules/ensemble.rst:279
msgid ":ref:`example_ensemble_plot_forest_importances_faces.py`"
msgstr ""

#: ../../modules/ensemble.rst:234
msgid ":ref:`example_plot_multioutput_face_completion.py`"
msgstr ""

#: ../../modules/ensemble.rst:238
msgid "Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001."
msgstr ""

#: ../../modules/ensemble.rst:240
msgid "Breiman, \"Arcing Classifiers\", Annals of Statistics 1998."
msgstr ""

#: ../../modules/ensemble.rst:242
msgid ""
"P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\", "
"Machine Learning, 63(1), 3-42, 2006."
msgstr ""

#: ../../modules/ensemble.rst:248
msgid "Feature importance evaluation"
msgstr ""

#: ../../modules/ensemble.rst:250
msgid ""
"The relative rank (i.e. depth) of a feature used as a decision node in a "
"tree can be used to assess the relative importance of that feature with "
"respect to the predictability of the target variable. Features used at "
"the top of the tree are used contribute to the final prediction decision "
"of a larger fraction of the input samples. The **expected fraction of the"
" samples** they contribute to can thus be used as an estimate of the "
"**relative importance of the features**."
msgstr ""

#: ../../modules/ensemble.rst:258
msgid ""
"By **averaging** those expected activity rates over several randomized "
"trees one can **reduce the variance** of such an estimate and use it for "
"feature selection."
msgstr ""

#: ../../modules/ensemble.rst:262
msgid ""
"The following example shows a color-coded representation of the relative "
"importances of each individual pixel for a face recognition task using a "
":class:`ExtraTreesClassifier` model."
msgstr ""

#: ../../modules/ensemble.rst:271
msgid ""
"In practice those estimates are stored as an attribute named "
"``feature_importances_`` on the fitted model. This is an array with shape"
" ``(n_features,)`` whose values are positive and sum to 1.0. The higher "
"the value, the more important is the contribution of the matching feature"
" to the prediction function."
msgstr ""

#: ../../modules/ensemble.rst:280
msgid ":ref:`example_ensemble_plot_forest_importances.py`"
msgstr ""

#: ../../modules/ensemble.rst:285
msgid "Totally Random Trees Embedding"
msgstr ""

#: ../../modules/ensemble.rst:287
msgid ""
":class:`RandomTreesEmbedding` implements an unsupervised transformation "
"of the data.  Using a forest of completely random trees, "
":class:`RandomTreesEmbedding` encodes the data by the indices of the "
"leaves a data point ends up in.  This index is then encoded in a one-of-K"
" manner, leading to a high dimensional, sparse binary coding. This coding"
" can be computed very efficiently and can then be used as a basis for "
"other learning tasks. The size and sparsity of the code can be influenced"
" by choosing the number of trees and the maximum depth per tree. For each"
" tree in the ensemble, the coding contains one entry of one. The size of "
"the coding is at most ``n_estimators * 2 ** max_depth``, the maximum "
"number of leaves in the forest."
msgstr ""

#: ../../modules/ensemble.rst:299
msgid ""
"As neighboring data points are more likely to lie within the same leaf of"
" a tree, the transformation performs an implicit, non-parametric density "
"estimation."
msgstr ""

#: ../../modules/ensemble.rst:304
msgid ":ref:`example_ensemble_plot_random_forest_embedding.py`"
msgstr ""

#: ../../modules/ensemble.rst:306
msgid ""
":ref:`example_manifold_plot_lle_digits.py` compares non-linear "
"dimensionality reduction techniques on handwritten digits."
msgstr ""

#: ../../modules/ensemble.rst:309
msgid ""
":ref:`example_ensemble_plot_feature_transformation.py` compares "
"supervised and unsupervised tree based feature transformations."
msgstr ""

#: ../../modules/ensemble.rst:314
msgid ""
":ref:`manifold` techniques can also be useful to derive non-linear "
"representations of feature space, also these approaches focus also on "
"dimensionality reduction."
msgstr ""

#: ../../modules/ensemble.rst:322
msgid "AdaBoost"
msgstr ""

#: ../../modules/ensemble.rst:324
msgid ""
"The module :mod:`sklearn.ensemble` includes the popular boosting "
"algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995]_."
msgstr ""

#: ../../modules/ensemble.rst:327
msgid ""
"The core principle of AdaBoost is to fit a sequence of weak learners "
"(i.e., models that are only slightly better than random guessing, such as"
" small decision trees) on repeatedly modified versions of the data. The "
"predictions from all of them are then combined through a weighted "
"majority vote (or sum) to produce the final prediction. The data "
"modifications at each so-called boosting iteration consist of applying "
"weights :math:`w_1`, :math:`w_2`, ..., :math:`w_N` to each of the "
"training samples. Initially, those weights are all set to :math:`w_i = "
"1/N`, so that the first step simply trains a weak learner on the original"
" data. For each successive iteration, the sample weights are individually"
" modified and the learning algorithm is reapplied to the reweighted data."
" At a given step, those training examples that were incorrectly predicted"
" by the boosted model induced at the previous step have their weights "
"increased, whereas the weights are decreased for those that were "
"predicted correctly. As iterations proceed, examples that are difficult "
"to predict receive ever-increasing influence. Each subsequent weak "
"learner is thereby forced to concentrate on the examples that are missed "
"by the previous ones in the sequence [HTF]_."
msgstr ""

#: ../../modules/ensemble.rst:350
msgid "AdaBoost can be used both for classification and regression problems:"
msgstr ""

#: ../../modules/ensemble.rst:352
msgid ""
"For multi-class classification, :class:`AdaBoostClassifier` implements "
"AdaBoost-SAMME and AdaBoost-SAMME.R [ZZRH2009]_."
msgstr ""

#: ../../modules/ensemble.rst:355
msgid ""
"For regression, :class:`AdaBoostRegressor` implements AdaBoost.R2 "
"[D1997]_."
msgstr ""

#: ../../modules/ensemble.rst:358 ../../modules/ensemble.rst:946
#: ../../modules/ensemble.rst:1056
msgid "Usage"
msgstr ""

#: ../../modules/ensemble.rst:360
msgid ""
"The following example shows how to fit an AdaBoost classifier with 100 "
"weak learners::"
msgstr ""

#: ../../modules/ensemble.rst:373
msgid ""
"The number of weak learners is controlled by the parameter "
"``n_estimators``. The ``learning_rate`` parameter controls the "
"contribution of the weak learners in the final combination. By default, "
"weak learners are decision stumps. Different weak learners can be "
"specified through the ``base_estimator`` parameter. The main parameters "
"to tune to obtain good results are ``n_estimators`` and the complexity of"
" the base estimators (e.g., its depth ``max_depth`` or minimum required "
"number of samples at a leaf ``min_samples_leaf`` in case of decision "
"trees)."
msgstr ""

#: ../../modules/ensemble.rst:384
msgid ""
":ref:`example_ensemble_plot_adaboost_hastie_10_2.py` compares the "
"classification error of a decision stump, decision tree, and a boosted "
"decision stump using AdaBoost-SAMME and AdaBoost-SAMME.R."
msgstr ""

#: ../../modules/ensemble.rst:388
msgid ""
":ref:`example_ensemble_plot_adaboost_multiclass.py` shows the performance"
" of AdaBoost-SAMME and AdaBoost-SAMME.R on a multi-class problem."
msgstr ""

#: ../../modules/ensemble.rst:391
msgid ""
":ref:`example_ensemble_plot_adaboost_twoclass.py` shows the decision "
"boundary and decision function values for a non-linearly separable two-"
"class problem using AdaBoost-SAMME."
msgstr ""

#: ../../modules/ensemble.rst:395
msgid ""
":ref:`example_ensemble_plot_adaboost_regression.py` demonstrates "
"regression with the AdaBoost.R2 algorithm."
msgstr ""

#: ../../modules/ensemble.rst:400
msgid ""
"Y. Freund, and R. Schapire, \"A Decision-Theoretic Generalization of On-"
"Line Learning and an Application to Boosting\", 1997."
msgstr ""

#: ../../modules/ensemble.rst:403
msgid "J. Zhu, H. Zou, S. Rosset, T. Hastie. \"Multi-class AdaBoost\", 2009."
msgstr ""

#: ../../modules/ensemble.rst:406
msgid "Drucker. \"Improving Regressors using Boosting Techniques\", 1997."
msgstr ""

#: ../../modules/ensemble.rst:408
msgid ""
"T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical "
"Learning Ed. 2\", Springer, 2009."
msgstr ""

#: ../../modules/ensemble.rst:415
msgid "Gradient Tree Boosting"
msgstr ""

#: ../../modules/ensemble.rst:417
msgid ""
"`Gradient Tree Boosting "
"<http://en.wikipedia.org/wiki/Gradient_boosting>`_ or Gradient Boosted "
"Regression Trees (GBRT) is a generalization of boosting to arbitrary "
"differentiable loss functions. GBRT is an accurate and effective off-the-"
"shelf procedure that can be used for both regression and classification "
"problems.  Gradient Tree Boosting models are used in a variety of areas "
"including Web search ranking and ecology."
msgstr ""

#: ../../modules/ensemble.rst:425
msgid "The advantages of GBRT are:"
msgstr ""

#: ../../modules/ensemble.rst:427
msgid "Natural handling of data of mixed type (= heterogeneous features)"
msgstr ""

#: ../../modules/ensemble.rst:429
msgid "Predictive power"
msgstr ""

#: ../../modules/ensemble.rst:431
msgid "Robustness to outliers in output space (via robust loss functions)"
msgstr ""

#: ../../modules/ensemble.rst:433
msgid "The disadvantages of GBRT are:"
msgstr ""

#: ../../modules/ensemble.rst:435
msgid ""
"Scalability, due to the sequential nature of boosting it can hardly be "
"parallelized."
msgstr ""

#: ../../modules/ensemble.rst:438
msgid ""
"The module :mod:`sklearn.ensemble` provides methods for both "
"classification and regression via gradient boosted regression trees."
msgstr ""

#: ../../modules/ensemble.rst:443 ../../modules/ensemble.rst:648
msgid "Classification"
msgstr ""

#: ../../modules/ensemble.rst:445
msgid ""
":class:`GradientBoostingClassifier` supports both binary and multi-class "
"classification. The following example shows how to fit a gradient "
"boosting classifier with 100 decision stumps as weak learners::"
msgstr ""

#: ../../modules/ensemble.rst:462
msgid ""
"The number of weak learners (i.e. regression trees) is controlled by the "
"parameter ``n_estimators``; :ref:`The size of each tree "
"<gradient_boosting_tree_size>` can be controlled either by setting the "
"tree depth via ``max_depth`` or by setting the number of leaf nodes via "
"``max_leaf_nodes``. The ``learning_rate`` is a hyper-parameter in the "
"range (0.0, 1.0] that controls overfitting via :ref:`shrinkage "
"<gradient_boosting_shrinkage>` ."
msgstr ""

#: ../../modules/ensemble.rst:466
msgid ""
"Classification with more than 2 classes requires the induction of "
"``n_classes`` regression trees at each at each iteration, thus, the total"
" number of induced trees equals ``n_classes * n_estimators``. For "
"datasets with a large number of classes we strongly recommend to use "
":class:`RandomForestClassifier` as an alternative to "
":class:`GradientBoostingClassifier` ."
msgstr ""

#: ../../modules/ensemble.rst:474 ../../modules/ensemble.rst:631
msgid "Regression"
msgstr ""

#: ../../modules/ensemble.rst:476
msgid ""
":class:`GradientBoostingRegressor` supports a number of :ref:`different "
"loss functions <gradient_boosting_loss>` for regression which can be "
"specified via the argument ``loss``; the default loss function for "
"regression is least squares (``'ls'``)."
msgstr ""

#: ../../modules/ensemble.rst:496
msgid ""
"The figure below shows the results of applying "
":class:`GradientBoostingRegressor` with least squares loss and 500 base "
"learners to the Boston house price dataset "
"(:func:`sklearn.datasets.load_boston`). The plot on the left shows the "
"train and test error at each iteration. The train error at each iteration"
" is stored in the :attr:`~GradientBoostingRegressor.train_score_` "
"attribute of the gradient boosting model. The test error at each "
"iterations can be obtained via the "
":meth:`~GradientBoostingRegressor.staged_predict` method which returns a "
"generator that yields the predictions at each stage. Plots like these can"
" be used to determine the optimal number of trees (i.e. ``n_estimators``)"
" by early stopping. The plot on the right shows the feature importances "
"which can be obtained via the ``feature_importances_`` property."
msgstr ""

#: ../../modules/ensemble.rst:516 ../../modules/ensemble.rst:782
msgid ":ref:`example_ensemble_plot_gradient_boosting_regression.py`"
msgstr ""

#: ../../modules/ensemble.rst:517 ../../modules/ensemble.rst:737
msgid ":ref:`example_ensemble_plot_gradient_boosting_oob.py`"
msgstr ""

#: ../../modules/ensemble.rst:522
msgid "Fitting additional weak-learners"
msgstr ""

#: ../../modules/ensemble.rst:524
msgid ""
"Both :class:`GradientBoostingRegressor` and "
":class:`GradientBoostingClassifier` support ``warm_start=True`` which "
"allows you to add more estimators to an already fitted model."
msgstr ""

#: ../../modules/ensemble.rst:538
msgid "Controlling the tree size"
msgstr ""

#: ../../modules/ensemble.rst:540
msgid ""
"The size of the regression tree base learners defines the level of "
"variable interactions that can be captured by the gradient boosting "
"model. In general, a tree of depth ``h`` can capture interactions of "
"order ``h`` . There are two ways in which the size of the individual "
"regression trees can be controlled."
msgstr ""

#: ../../modules/ensemble.rst:546
msgid ""
"If you specify ``max_depth=h`` then complete binary trees of depth ``h`` "
"will be grown. Such trees will have (at most) ``2**h`` leaf nodes and "
"``2**h - 1`` split nodes."
msgstr ""

#: ../../modules/ensemble.rst:550
msgid ""
"Alternatively, you can control the tree size by specifying the number of "
"leaf nodes via the parameter ``max_leaf_nodes``. In this case, trees will"
" be grown using best-first search where nodes with the highest "
"improvement in impurity will be expanded first. A tree with "
"``max_leaf_nodes=k`` has ``k - 1`` split nodes and thus can model "
"interactions of up to order ``max_leaf_nodes - 1`` ."
msgstr ""

#: ../../modules/ensemble.rst:557
msgid ""
"We found that ``max_leaf_nodes=k`` gives comparable results to "
"``max_depth=k-1`` but is significantly faster to train at the expense of "
"a slightly higher training error. The parameter ``max_leaf_nodes`` "
"corresponds to the variable ``J`` in the chapter on gradient boosting in "
"[F2001]_ and is related to the parameter ``interaction.depth`` in R's gbm"
" package where ``max_leaf_nodes == interaction.depth + 1`` ."
msgstr ""

#: ../../modules/ensemble.rst:565
msgid "Mathematical formulation"
msgstr ""

#: ../../modules/ensemble.rst:567
msgid "GBRT considers additive models of the following form:"
msgstr ""

#: ../../modules/ensemble.rst:573
msgid ""
"where :math:`h_m(x)` are the basis functions which are usually called "
"*weak learners* in the context of boosting. Gradient Tree Boosting uses "
":ref:`decision trees <tree>` of fixed size as weak learners. Decision "
"trees have a number of abilities that make them valuable for boosting, "
"namely the ability to handle data of mixed type and the ability to model "
"complex functions."
msgstr ""

#: ../../modules/ensemble.rst:580
msgid ""
"Similar to other boosting algorithms GBRT builds the additive model in a "
"forward stagewise fashion:"
msgstr ""

#: ../../modules/ensemble.rst:587
msgid ""
"At each stage the decision tree :math:`h_m(x)` is chosen to minimize the "
"loss function :math:`L` given the current model :math:`F_{m-1}` and its "
"fit :math:`F_{m-1}(x_i)`"
msgstr ""

#: ../../modules/ensemble.rst:596
msgid ""
"The initial model :math:`F_{0}` is problem specific, for least-squares "
"regression one usually chooses the mean of the target values."
msgstr ""

#: ../../modules/ensemble.rst:599
msgid ""
"The initial model can also be specified via the ``init`` argument. The "
"passed object has to implement ``fit`` and ``predict``."
msgstr ""

#: ../../modules/ensemble.rst:602
msgid ""
"Gradient Boosting attempts to solve this minimization problem numerically"
" via steepest descent: The steepest descent direction is the negative "
"gradient of the loss function evaluated at the current model "
":math:`F_{m-1}` which can be calculated for any differentiable loss "
"function:"
msgstr ""

#: ../../modules/ensemble.rst:613
msgid "Where the step length :math:`\\gamma_m` is chosen using line search:"
msgstr ""

#: ../../modules/ensemble.rst:620
msgid ""
"The algorithms for regression and classification only differ in the "
"concrete loss function used."
msgstr ""

#: ../../modules/ensemble.rst:626
msgid "Loss Functions"
msgstr ""

#: ../../modules/ensemble.rst:628
msgid ""
"The following loss functions are supported and can be specified using the"
" parameter ``loss``:"
msgstr ""

#: ../../modules/ensemble.rst:633
msgid ""
"Least squares (``'ls'``): The natural choice for regression due to its "
"superior computational properties. The initial model is given by the mean"
" of the target values."
msgstr ""

#: ../../modules/ensemble.rst:636
msgid ""
"Least absolute deviation (``'lad'``): A robust loss function for "
"regression. The initial model is given by the median of the target "
"values."
msgstr ""

#: ../../modules/ensemble.rst:639
msgid ""
"Huber (``'huber'``): Another robust loss function that combines least "
"squares and least absolute deviation; use ``alpha`` to control the "
"sensitivity with regards to outliers (see [F2001]_ for more details)."
msgstr ""

#: ../../modules/ensemble.rst:643
msgid ""
"Quantile (``'quantile'``): A loss function for quantile regression. Use "
"``0 < alpha < 1`` to specify the quantile. This loss function can be used"
" to create prediction intervals (see "
":ref:`example_ensemble_plot_gradient_boosting_quantile.py`)."
msgstr ""

#: ../../modules/ensemble.rst:650
msgid ""
"Binomial deviance (``'deviance'``): The negative binomial log-likelihood "
"loss function for binary classification (provides probability estimates)."
"  The initial model is given by the log odds-ratio."
msgstr ""

#: ../../modules/ensemble.rst:654
msgid ""
"Multinomial deviance (``'deviance'``): The negative multinomial log-"
"likelihood loss function for multi-class classification with "
"``n_classes`` mutually exclusive classes. It provides probability "
"estimates.  The initial model is given by the prior probability of each "
"class. At each iteration ``n_classes`` regression trees have to be "
"constructed which makes GBRT rather inefficient for data sets with a "
"large number of classes."
msgstr ""

#: ../../modules/ensemble.rst:661
msgid ""
"Exponential loss (``'exponential'``): The same loss function as "
":class:`AdaBoostClassifier`. Less robust to mislabeled examples than "
"``'deviance'``; can only be used for binary classification."
msgstr ""

#: ../../modules/ensemble.rst:667
msgid "Regularization"
msgstr ""

#: ../../modules/ensemble.rst:672
msgid "Shrinkage"
msgstr ""

#: ../../modules/ensemble.rst:674
msgid ""
"[F2001]_ proposed a simple regularization strategy that scales the "
"contribution of each weak learner by a factor :math:`\\nu`:"
msgstr ""

#: ../../modules/ensemble.rst:681
msgid ""
"The parameter :math:`\\nu` is also called the **learning rate** because "
"it scales the step length the the gradient descent procedure; it can be "
"set via the ``learning_rate`` parameter."
msgstr ""

#: ../../modules/ensemble.rst:685
msgid ""
"The parameter ``learning_rate`` strongly interacts with the parameter "
"``n_estimators``, the number of weak learners to fit. Smaller values of "
"``learning_rate`` require larger numbers of weak learners to maintain a "
"constant training error. Empirical evidence suggests that small values of"
" ``learning_rate`` favor better test error. [HTF2009]_ recommend to set "
"the learning rate to a small constant (e.g. ``learning_rate <= 0.1``) and"
" choose ``n_estimators`` by early stopping. For a more detailed "
"discussion of the interaction between ``learning_rate`` and "
"``n_estimators`` see [R2007]_."
msgstr ""

#: ../../modules/ensemble.rst:696
msgid "Subsampling"
msgstr ""

#: ../../modules/ensemble.rst:698
msgid ""
"[F1999]_ proposed stochastic gradient boosting, which combines gradient "
"boosting with bootstrap averaging (bagging). At each iteration the base "
"classifier is trained on a fraction ``subsample`` of the available "
"training data. The subsample is drawn without replacement. A typical "
"value of ``subsample`` is 0.5."
msgstr ""

#: ../../modules/ensemble.rst:704
msgid ""
"The figure below illustrates the effect of shrinkage and subsampling on "
"the goodness-of-fit of the model. We can clearly see that shrinkage "
"outperforms no-shrinkage. Subsampling with shrinkage can further increase"
" the accuracy of the model. Subsampling without shrinkage, on the other "
"hand, does poorly."
msgstr ""

#: ../../modules/ensemble.rst:715
msgid ""
"Another strategy to reduce the variance is by subsampling the features "
"analogous to the random splits in :class:`RandomForestClassifier` . The "
"number of subsampled features can be controlled via the ``max_features`` "
"parameter."
msgstr ""

#: ../../modules/ensemble.rst:720
msgid ""
"Using a small ``max_features`` value can significantly decrease the "
"runtime."
msgstr ""

#: ../../modules/ensemble.rst:722
msgid ""
"Stochastic gradient boosting allows to compute out-of-bag estimates of "
"the test deviance by computing the improvement in deviance on the "
"examples that are not included in the bootstrap sample (i.e. the out-of-"
"bag examples). The improvements are stored in the attribute "
":attr:`~GradientBoostingRegressor.oob_improvement_`. "
"``oob_improvement_[i]`` holds the improvement in terms of the loss on the"
" OOB samples if you add the i-th stage to the current predictions. Out-"
"of-bag estimates can be used for model selection, for example to "
"determine the optimal number of iterations. OOB estimates are usually "
"very pessimistic thus we recommend to use cross-validation instead and "
"only use OOB if cross-validation is too time consuming."
msgstr ""

#: ../../modules/ensemble.rst:736
msgid ":ref:`example_ensemble_plot_gradient_boosting_regularization.py`"
msgstr ""

#: ../../modules/ensemble.rst:738
msgid ":ref:`example_ensemble_plot_ensemble_oob.py`"
msgstr ""

#: ../../modules/ensemble.rst:741
msgid "Interpretation"
msgstr ""

#: ../../modules/ensemble.rst:743
msgid ""
"Individual decision trees can be interpreted easily by simply visualizing"
" the tree structure. Gradient boosting models, however, comprise hundreds"
" of regression trees thus they cannot be easily interpreted by visual "
"inspection of the individual trees. Fortunately, a number of techniques "
"have been proposed to summarize and interpret gradient boosting models."
msgstr ""

#: ../../modules/ensemble.rst:751
msgid "Feature importance"
msgstr ""

#: ../../modules/ensemble.rst:753
msgid ""
"Often features do not contribute equally to predict the target response; "
"in many situations the majority of the features are in fact irrelevant. "
"When interpreting a model, the first question usually is: what are those "
"important features and how do they contributing in predicting the target "
"response?"
msgstr ""

#: ../../modules/ensemble.rst:760
msgid ""
"Individual decision trees intrinsically perform feature selection by "
"selecting appropriate split points. This information can be used to "
"measure the importance of each feature; the basic idea is: the more often"
" a feature is used in the split points of a tree the more important that "
"feature is. This notion of importance can be extended to decision tree "
"ensembles by simply averaging the feature importance of each tree (see "
":ref:`random_forest_feature_importance` for more details)."
msgstr ""

#: ../../modules/ensemble.rst:768
msgid ""
"The feature importance scores of a fit gradient boosting model can be "
"accessed via the ``feature_importances_`` property::"
msgstr ""

#: ../../modules/ensemble.rst:789
msgid "Partial dependence"
msgstr ""

#: ../../modules/ensemble.rst:791
msgid ""
"Partial dependence plots (PDP) show the dependence between the target "
"response and a set of 'target' features, marginalizing over the values of"
" all other features (the 'complement' features). Intuitively, we can "
"interpret the partial dependence as the expected target response [1]_ as "
"a function of the 'target' features [2]_."
msgstr ""

#: ../../modules/ensemble.rst:797
msgid ""
"Due to the limits of human perception the size of the target feature set "
"must be small (usually, one or two) thus the target features are usually "
"chosen among the most important features."
msgstr ""

#: ../../modules/ensemble.rst:801
msgid ""
"The Figure below shows four one-way and one two-way partial dependence "
"plots for the California housing dataset:"
msgstr ""

#: ../../modules/ensemble.rst:809
msgid ""
"One-way PDPs tell us about the interaction between the target response "
"and the target feature (e.g. linear, non-linear). The upper left plot in "
"the above Figure shows the effect of the median income in a district on "
"the median house price; we can clearly see a linear relationship among "
"them."
msgstr ""

#: ../../modules/ensemble.rst:815
msgid ""
"PDPs with two target features show the interactions among the two "
"features. For example, the two-variable PDP in the above Figure shows the"
" dependence of median house price on joint values of house age and avg. "
"occupants per household. We can clearly see an interaction between the "
"two features: For an avg. occupancy greater than two, the house price is "
"nearly independent of the house age, whereas for values less than two "
"there is a strong dependence on age."
msgstr ""

#: ../../modules/ensemble.rst:824
msgid ""
"The module :mod:`partial_dependence` provides a convenience function "
":func:`~sklearn.ensemble.partial_dependence.plot_partial_dependence` to "
"create one-way and two-way partial dependence plots. In the below example"
" we show how to create a grid of partial dependence plots: two one-way "
"PDPs for the features ``0`` and ``1`` and a two-way PDP between the two "
"features::"
msgstr ""

#: ../../modules/ensemble.rst:841
msgid ""
"For multi-class models, you need to set the class label for which the "
"PDPs should be created via the ``label`` argument::"
msgstr ""

#: ../../modules/ensemble.rst:851
msgid ""
"If you need the raw values of the partial dependence function rather than"
" the plots you can use the "
":func:`~sklearn.ensemble.partial_dependence.partial_dependence` "
"function::"
msgstr ""

#: ../../modules/ensemble.rst:863
msgid ""
"The function requires either the argument ``grid`` which specifies the "
"values of the target features on which the partial dependence function "
"should be evaluated or the argument ``X`` which is a convenience mode for"
" automatically creating ``grid`` from the training data. If ``X`` is "
"given, the ``axes`` value returned by the function gives the axis for "
"each target feature."
msgstr ""

#: ../../modules/ensemble.rst:870
msgid ""
"For each value of the 'target' features in the ``grid`` the partial "
"dependence function need to marginalize the predictions of a tree over "
"all possible values of the 'complement' features. In decision trees this "
"function can be evaluated efficiently without reference to the training "
"data. For each grid point a weighted tree traversal is performed: if a "
"split node involves a 'target' feature, the corresponding left or right "
"branch is followed, otherwise both branches are followed, each branch is "
"weighted by the fraction of training samples that entered that branch. "
"Finally, the partial dependence is given by a weighted average of all "
"visited leaves. For tree ensembles the results of each individual tree "
"are again averaged."
msgstr ""

#: ../../modules/ensemble.rst:884
msgid "Footnotes"
msgstr ""

#: ../../modules/ensemble.rst:885
msgid ""
"For classification with ``loss='deviance'``  the target response is "
"logit(p)."
msgstr ""

#: ../../modules/ensemble.rst:888
msgid ""
"More precisely its the expectation of the target response after "
"accounting for the initial model; partial dependence plots do not include"
" the ``init`` model."
msgstr ""

#: ../../modules/ensemble.rst:894
msgid ":ref:`example_ensemble_plot_partial_dependence.py`"
msgstr ""

#: ../../modules/ensemble.rst:899
msgid ""
"J. Friedman, \"Greedy Function Approximation: A Gradient Boosting "
"Machine\", The Annals of Statistics, Vol. 29, No. 5, 2001."
msgstr ""

#: ../../modules/ensemble.rst:902
msgid "Friedman, \"Stochastic Gradient Boosting\", 1999"
msgstr ""

#: ../../modules/ensemble.rst:904
msgid ""
"Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical Learning"
" Ed. 2\", Springer, 2009."
msgstr ""

#: ../../modules/ensemble.rst:906
msgid "Ridgeway, \"Generalized Boosted Models: A guide to the gbm package\", 2007"
msgstr ""

#: ../../modules/ensemble.rst:912
msgid "VotingClassifier"
msgstr ""

#: ../../modules/ensemble.rst:914
msgid ""
"The idea behind the voting classifier implementation is to combine "
"conceptually different machine learning classifiers and use a majority "
"vote or the average predicted probabilities (soft vote) to predict the "
"class labels. Such a classifier can be useful for a set of equally well "
"performing model in order to balance out their individual weaknesses."
msgstr ""

#: ../../modules/ensemble.rst:922
msgid "Majority Class Labels (Majority/Hard Voting)"
msgstr ""

#: ../../modules/ensemble.rst:924
msgid ""
"In majority voting, the predicted class label for a particular sample is "
"the class label that represents the majority (mode) of the class labels "
"predicted by each individual classifier."
msgstr ""

#: ../../modules/ensemble.rst:928
msgid "E.g., if the prediction for a given sample is"
msgstr ""

#: ../../modules/ensemble.rst:930
msgid "classifier 1 -> class 1"
msgstr ""

#: ../../modules/ensemble.rst:931 ../../modules/ensemble.rst:941
msgid "classifier 2 -> class 1"
msgstr ""

#: ../../modules/ensemble.rst:932
msgid "classifier 3 -> class 2"
msgstr ""

#: ../../modules/ensemble.rst:934
msgid ""
"the VotingClassifier (with ``voting='hard'``) would classify the sample "
"as \"class 1\" based on the majority class label."
msgstr ""

#: ../../modules/ensemble.rst:937
msgid ""
"In the cases of a tie, the `VotingClassifier` will select the class based"
" on the ascending sort order. E.g., in the following scenario"
msgstr ""

#: ../../modules/ensemble.rst:940
msgid "classifier 1 -> class 2"
msgstr ""

#: ../../modules/ensemble.rst:943
msgid "the class label 1 will be assigned to the sample."
msgstr ""

#: ../../modules/ensemble.rst:948
msgid "The following example shows how to fit the majority rule classifier::"
msgstr ""

#: ../../modules/ensemble.rst:976
msgid "Weighted Average Probabilities (Soft Voting)"
msgstr ""

#: ../../modules/ensemble.rst:978
msgid ""
"In contrast to majority voting (hard voting), soft voting returns the "
"class label as argmax of the sum of predicted probabilities."
msgstr ""

#: ../../modules/ensemble.rst:981
msgid ""
"Specific weights can be assigned to each classifier via the ``weights`` "
"parameter. When weights are provided, the predicted class probabilities "
"for each classifier are collected, multiplied by the classifier weight, "
"and averaged. The final class label is then derived from the class label "
"with the highest average probability."
msgstr ""

#: ../../modules/ensemble.rst:987
msgid ""
"To illustrate this with a simple example, let's assume we have 3 "
"classifiers and a 3-class classification problems where we assign equal "
"weights to all classifiers: w1=1, w2=1, w3=1."
msgstr ""

#: ../../modules/ensemble.rst:991
msgid ""
"The weighted average probabilities for a sample would then be calculated "
"as follows:"
msgstr ""

#: ../../modules/ensemble.rst:995
msgid "classifier"
msgstr ""

#: ../../modules/ensemble.rst:995
msgid "class 1"
msgstr ""

#: ../../modules/ensemble.rst:995
msgid "class 2"
msgstr ""

#: ../../modules/ensemble.rst:995
msgid "class 3"
msgstr ""

#: ../../modules/ensemble.rst:997
msgid "classifier 1"
msgstr ""

#: ../../modules/ensemble.rst:997
msgid "w1 * 0.2"
msgstr ""

#: ../../modules/ensemble.rst:997
msgid "w1 * 0.5"
msgstr ""

#: ../../modules/ensemble.rst:997
msgid "w1 * 0.3"
msgstr ""

#: ../../modules/ensemble.rst:998
msgid "classifier 2"
msgstr ""

#: ../../modules/ensemble.rst:998
msgid "w2 * 0.6"
msgstr ""

#: ../../modules/ensemble.rst:998
msgid "w2 * 0.3"
msgstr ""

#: ../../modules/ensemble.rst:998
msgid "w2 * 0.1"
msgstr ""

#: ../../modules/ensemble.rst:999
msgid "classifier 3"
msgstr ""

#: ../../modules/ensemble.rst:999
msgid "w3 * 0.3"
msgstr ""

#: ../../modules/ensemble.rst:999
msgid "w3 * 0.4"
msgstr ""

#: ../../modules/ensemble.rst:1000
msgid "weighted average"
msgstr ""

#: ../../modules/ensemble.rst:1000
msgid "0.37"
msgstr ""

#: ../../modules/ensemble.rst:1000
msgid "0.4"
msgstr ""

#: ../../modules/ensemble.rst:1000
msgid "0.3"
msgstr ""

#: ../../modules/ensemble.rst:1003
msgid ""
"Here, the predicted class label is 2, since it has the highest average "
"probability."
msgstr ""

#: ../../modules/ensemble.rst:1006
msgid ""
"The following example illustrates how the decision regions may change "
"when a soft `VotingClassifier` is used based on an linear Support Vector "
"Machine, a Decision Tree, and a K-nearest neighbor classifier::"
msgstr ""

#: ../../modules/ensemble.rst:1039
msgid "Using the `VotingClassifier` with `GridSearch`"
msgstr ""

#: ../../modules/ensemble.rst:1041
msgid ""
"The `VotingClassifier` can also be used together with `GridSearch` in "
"order to tune the hyperparameters of the individual estimators::"
msgstr ""

#: ../../modules/ensemble.rst:1058
msgid ""
"In order to predict the class labels based on the predicted class-"
"probabilities (scikit-learn estimators in the VotingClassifier must "
"support ``predict_proba`` method)::"
msgstr ""

#: ../../modules/ensemble.rst:1064
msgid "Optionally, weights can be provided for the individual classifiers::"
msgstr ""

