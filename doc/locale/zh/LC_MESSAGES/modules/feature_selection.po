# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/feature_selection.rst:7
msgid "Feature selection"
msgstr ""

#: ../../modules/feature_selection.rst:10
msgid ""
"The classes in the :mod:`sklearn.feature_selection` module can be used "
"for feature selection/dimensionality reduction on sample sets, either to "
"improve estimators' accuracy scores or to boost their performance on very"
" high-dimensional datasets."
msgstr ""

#: ../../modules/feature_selection.rst:19
msgid "Removing features with low variance"
msgstr ""

#: ../../modules/feature_selection.rst:21
msgid ""
":class:`VarianceThreshold` is a simple baseline approach to feature "
"selection. It removes all features whose variance doesn't meet some "
"threshold. By default, it removes all zero-variance features, i.e. "
"features that have the same value in all samples."
msgstr ""

#: ../../modules/feature_selection.rst:26
#, python-format
msgid ""
"As an example, suppose that we have a dataset with boolean features, and "
"we want to remove all features that are either one or zero (on or off) in"
" more than 80% of the samples. Boolean features are Bernoulli random "
"variables, and the variance of such variables is given by"
msgstr ""

#: ../../modules/feature_selection.rst:34
msgid "so we can select using the threshold ``.8 * (1 - .8)``::"
msgstr ""

#: ../../modules/feature_selection.rst:47
msgid ""
"As expected, ``VarianceThreshold`` has removed the first column, which "
"has a probability :math:`p = 5/6 > .8` of containing a zero."
msgstr ""

#: ../../modules/feature_selection.rst:53
msgid "Univariate feature selection"
msgstr ""

#: ../../modules/feature_selection.rst:55
msgid ""
"Univariate feature selection works by selecting the best features based "
"on univariate statistical tests. It can be seen as a preprocessing step "
"to an estimator. Scikit-learn exposes feature selection routines as "
"objects that implement the ``transform`` method:"
msgstr ""

#: ../../modules/feature_selection.rst:60
msgid ""
":class:`SelectKBest` removes all but the :math:`k` highest scoring "
"features"
msgstr ""

#: ../../modules/feature_selection.rst:62
msgid ""
":class:`SelectPercentile` removes all but a user-specified highest "
"scoring percentage of features"
msgstr ""

#: ../../modules/feature_selection.rst:65
msgid ""
"using common univariate statistical tests for each feature: false "
"positive rate :class:`SelectFpr`, false discovery rate "
":class:`SelectFdr`, or family wise error :class:`SelectFwe`."
msgstr ""

#: ../../modules/feature_selection.rst:71
msgid ":class:`GenericUnivariateSelect` allows to perform univariate feature"
msgstr ""

#: ../../modules/feature_selection.rst:70
msgid ""
"selection with a configurable strategy. This allows to select the best "
"univariate selection strategy with hyper-parameter search estimator."
msgstr ""

#: ../../modules/feature_selection.rst:73
msgid ""
"For instance, we can perform a :math:`\\chi^2` test to the samples to "
"retrieve only the two best features as follows:"
msgstr ""

#: ../../modules/feature_selection.rst:87
msgid ""
"These objects take as input a scoring function that returns univariate "
"p-values:"
msgstr ""

#: ../../modules/feature_selection.rst:90
msgid "For regression: :func:`f_regression`"
msgstr ""

#: ../../modules/feature_selection.rst:92
msgid "For classification: :func:`chi2` or :func:`f_classif`"
msgstr ""

#: ../../modules/feature_selection.rst
msgid "Feature selection with sparse data"
msgstr ""

#: ../../modules/feature_selection.rst:96
msgid ""
"If you use sparse data (i.e. data represented as sparse matrices), only "
":func:`chi2` will deal with the data without making it dense."
msgstr ""

#: ../../modules/feature_selection.rst:101
msgid ""
"Beware not to use a regression scoring function with a classification "
"problem, you will get useless results."
msgstr ""

#: ../../modules/feature_selection.rst
msgid "Examples:"
msgstr ""

#: ../../modules/feature_selection.rst:106
msgid ":ref:`example_feature_selection_plot_feature_selection.py`"
msgstr ""

#: ../../modules/feature_selection.rst:111
msgid "Recursive feature elimination"
msgstr ""

#: ../../modules/feature_selection.rst:113
msgid ""
"Given an external estimator that assigns weights to features (e.g., the "
"coefficients of a linear model), recursive feature elimination "
"(:class:`RFE`) is to select features by recursively considering smaller "
"and smaller sets of features.  First, the estimator is trained on the "
"initial set of features and weights are assigned to each one of them. "
"Then, features whose absolute weights are the smallest are pruned from "
"the current set features. That procedure is recursively repeated on the "
"pruned set until the desired number of features to select is eventually "
"reached."
msgstr ""

#: ../../modules/feature_selection.rst:122
msgid ""
":class:`RFECV` performs RFE in a cross-validation loop to find the "
"optimal number of features."
msgstr ""

#: ../../modules/feature_selection.rst:127
msgid ""
":ref:`example_feature_selection_plot_rfe_digits.py`: A recursive feature "
"elimination example showing the relevance of pixels in a digit "
"classification task."
msgstr ""

#: ../../modules/feature_selection.rst:130
msgid ""
":ref:`example_feature_selection_plot_rfe_with_cross_validation.py`: A "
"recursive feature elimination example with automatic tuning of the number"
" of features selected with cross-validation."
msgstr ""

#: ../../modules/feature_selection.rst:137
msgid "Feature selection using SelectFromModel"
msgstr ""

#: ../../modules/feature_selection.rst:139
msgid ""
":class:`SelectFromModel` is a meta-transformer that can be used along "
"with any estimator that has a ``coef_`` or ``feature_importances_`` "
"attribute after fitting. The features are considered unimportant and "
"removed, if the corresponding ``coef_`` or ``feature_importances_`` "
"values are below the provided ``threshold`` parameter. Apart from "
"specifying the threshold numerically, there are build-in heuristics for "
"finding a threshold using a string argument. Available heuristics are "
"\"mean\", \"median\" and float multiples of these like \"0.1*mean\"."
msgstr ""

#: ../../modules/feature_selection.rst:148
msgid "For examples on how it is to be used refer to the sections below."
msgstr ""

#: ../../modules/feature_selection.rst
msgid "Examples"
msgstr ""

#: ../../modules/feature_selection.rst:152
msgid ""
":ref:`example_feature_selection_plot_select_from_model_boston.py`: "
"Selecting the two most important features from the Boston dataset without"
" knowing the threshold beforehand."
msgstr ""

#: ../../modules/feature_selection.rst:159
msgid "L1-based feature selection"
msgstr ""

#: ../../modules/feature_selection.rst:163
msgid ""
":ref:`Linear models <linear_model>` penalized with the L1 norm have "
"sparse solutions: many of their estimated coefficients are zero. When the"
" goal is to reduce the dimensionality of the data to use with another "
"classifier, they can be used along with "
":class:`feature_selection.SelectFromModel` to select the non-zero "
"coefficients. In particular, sparse estimators useful for this purpose "
"are the :class:`linear_model.Lasso` for regression, and of "
":class:`linear_model.LogisticRegression` and :class:`svm.LinearSVC` for "
"classification::"
msgstr ""

#: ../../modules/feature_selection.rst:185
msgid ""
"With SVMs and logistic-regression, the parameter C controls the sparsity:"
" the smaller C the fewer features selected. With Lasso, the higher the "
"alpha parameter, the fewer features selected."
msgstr ""

#: ../../modules/feature_selection.rst:191
msgid ""
":ref:`example_text_document_classification_20newsgroups.py`: Comparison "
"of different algorithms for document classification including L1-based "
"feature selection."
msgstr ""

#: ../../modules/feature_selection.rst
msgid "**L1-recovery and compressive sensing**"
msgstr ""

#: ../../modules/feature_selection.rst:199
msgid ""
"For a good choice of alpha, the :ref:`lasso` can fully recover the exact "
"set of non-zero variables using only few observations, provided certain "
"specific conditions are met. In particular, the number of samples should "
"be \"sufficiently large\", or L1 models will perform at random, where "
"\"sufficiently large\" depends on the number of non-zero coefficients, "
"the logarithm of the number of features, the amount of noise, the "
"smallest absolute value of non-zero coefficients, and the structure of "
"the design matrix X. In addition, the design matrix must display certain "
"specific properties, such as not being too correlated."
msgstr ""

#: ../../modules/feature_selection.rst:209
msgid ""
"There is no general rule to select an alpha parameter for recovery of "
"non-zero coefficients. It can by set by cross-validation "
"(:class:`LassoCV` or :class:`LassoLarsCV`), though this may lead to "
"under-penalized models: including a small number of non-relevant "
"variables is not detrimental to prediction score. BIC "
"(:class:`LassoLarsIC`) tends, on the opposite, to set high values of "
"alpha."
msgstr ""

#: ../../modules/feature_selection.rst:217
msgid ""
"**Reference** Richard G. Baraniuk \"Compressive Sensing\", IEEE Signal "
"Processing Magazine [120] July 2007 "
"http://dsp.rice.edu/files/cs/baraniukCSlecture07.pdf"
msgstr ""

#: ../../modules/feature_selection.rst:224
msgid "Randomized sparse models"
msgstr ""

#: ../../modules/feature_selection.rst:228
msgid ""
"The limitation of L1-based sparse models is that faced with a group of "
"very correlated features, they will select only one. To mitigate this "
"problem, it is possible to use randomization techniques, reestimating the"
" sparse model many times perturbing the design matrix or sub-sampling "
"data and counting how many times a given regressor is selected."
msgstr ""

#: ../../modules/feature_selection.rst:234
msgid ""
":class:`RandomizedLasso` implements this strategy for regression "
"settings, using the Lasso, while :class:`RandomizedLogisticRegression` "
"uses the logistic regression and is suitable for classification tasks.  "
"To get a full path of stability scores you can use "
":func:`lasso_stability_path`."
msgstr ""

#: ../../modules/feature_selection.rst:244
msgid ""
"Note that for randomized sparse models to be more powerful than standard "
"F statistics at detecting non-zero features, the ground truth model "
"should be sparse, in other words, there should be only a small fraction "
"of features non zero."
msgstr ""

#: ../../modules/feature_selection.rst:251
msgid ""
":ref:`example_linear_model_plot_sparse_recovery.py`: An example comparing"
" different feature selection approaches and discussing in which situation"
" each approach is to be favored."
msgstr ""

#: ../../modules/feature_selection.rst
msgid "References:"
msgstr ""

#: ../../modules/feature_selection.rst:257
msgid ""
"N. Meinshausen, P. Buhlmann, \"Stability selection\", Journal of the "
"Royal Statistical Society, 72 (2010) http://arxiv.org/pdf/0809.2932"
msgstr ""

#: ../../modules/feature_selection.rst:261
msgid ""
"F. Bach, \"Model-Consistent Sparse Estimation through the Bootstrap\" "
"http://hal.inria.fr/hal-00354771/"
msgstr ""

#: ../../modules/feature_selection.rst:265
msgid "Tree-based feature selection"
msgstr ""

#: ../../modules/feature_selection.rst:267
msgid ""
"Tree-based estimators (see the :mod:`sklearn.tree` module and forest of "
"trees in the :mod:`sklearn.ensemble` module) can be used to compute "
"feature importances, which in turn can be used to discard irrelevant "
"features (when coupled with the "
":class:`sklearn.feature_selection.SelectFromModel` meta-transformer)::"
msgstr ""

#: ../../modules/feature_selection.rst:291
msgid ""
":ref:`example_ensemble_plot_forest_importances.py`: example on synthetic "
"data showing the recovery of the actually meaningful features."
msgstr ""

#: ../../modules/feature_selection.rst:295
msgid ""
":ref:`example_ensemble_plot_forest_importances_faces.py`: example on face"
" recognition data."
msgstr ""

#: ../../modules/feature_selection.rst:299
msgid "Feature selection as part of a pipeline"
msgstr ""

#: ../../modules/feature_selection.rst:301
msgid ""
"Feature selection is usually used as a pre-processing step before doing "
"the actual learning. The recommended way to do this in scikit-learn is to"
" use a :class:`sklearn.pipeline.Pipeline`::"
msgstr ""

#: ../../modules/feature_selection.rst:311
msgid ""
"In this snippet we make use of a :class:`sklearn.svm.LinearSVC` coupled "
"with :class:`sklearn.feature_selection.SelectFromModel` to evaluate "
"feature importances and select the most relevant features. Then, a "
":class:`sklearn.ensemble.RandomForestClassifier` is trained on the "
"transformed output, i.e. using only relevant features. You can perform "
"similar operations with the other feature selection methods and also "
"classifiers that provide a way to evaluate feature importances of course."
" See the :class:`sklearn.pipeline.Pipeline` examples for more details."
msgstr ""

