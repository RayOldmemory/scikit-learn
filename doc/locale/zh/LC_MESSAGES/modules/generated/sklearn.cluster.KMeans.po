# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.cluster.KMeans.rst:2
msgid ":mod:`sklearn.cluster`.KMeans"
msgstr ""

#: :3
msgid "K-Means clustering"
msgstr ""

#: :5
msgid "Read more in the :ref:`User Guide <k_means>`."
msgstr ""

#: :9
msgid "**n_clusters** : int, optional, default: 8"
msgstr ""

#: :11
msgid ""
"The number of clusters to form as well as the number of centroids to "
"generate."
msgstr ""

#: :14
msgid "**max_iter** : int, default: 300"
msgstr ""

#: :16
msgid "Maximum number of iterations of the k-means algorithm for a single run."
msgstr ""

#: :19
msgid "**n_init** : int, default: 10"
msgstr ""

#: :21
msgid ""
"Number of time the k-means algorithm will be run with different centroid "
"seeds. The final results will be the best output of n_init consecutive "
"runs in terms of inertia."
msgstr ""

#: :25
msgid "**init** : {'k-means++', 'random' or an ndarray}"
msgstr ""

#: :27
msgid "Method for initialization, defaults to 'k-means++':"
msgstr ""

#: :29
msgid ""
"'k-means++' : selects initial cluster centers for k-mean clustering in a "
"smart way to speed up convergence. See section Notes in k_init for more "
"details."
msgstr ""

#: :33
msgid ""
"'random': choose k observations (rows) at random from data for the "
"initial centroids."
msgstr ""

#: :36
msgid ""
"If an ndarray is passed, it should be of shape (n_clusters, n_features) "
"and gives the initial centers."
msgstr ""

#: :39
msgid "**precompute_distances** : {'auto', True, False}"
msgstr ""

#: :41
msgid "Precompute distances (faster but takes more memory)."
msgstr ""

#: :43
msgid ""
"'auto' : do not precompute distances if n_samples * n_clusters > 12 "
"million. This corresponds to about 100MB overhead per job using double "
"precision."
msgstr ""

#: :47
msgid "True : always precompute distances"
msgstr ""

#: :49
msgid "False : never precompute distances"
msgstr ""

#: :51
msgid "**tol** : float, default: 1e-4"
msgstr ""

#: :53
msgid "Relative tolerance with regards to inertia to declare convergence"
msgstr ""

#: :55
msgid "**n_jobs** : int"
msgstr ""

#: :57
msgid ""
"The number of jobs to use for the computation. This works by computing "
"each of the n_init runs in parallel."
msgstr ""

#: :60
msgid ""
"If -1 all CPUs are used. If 1 is given, no parallel computing code is "
"used at all, which is useful for debugging. For n_jobs below -1, (n_cpus "
"+ 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used."
msgstr ""

#: :65
msgid "**random_state** : integer or numpy.RandomState, optional"
msgstr ""

#: :67
msgid ""
"The generator used to initialize the centers. If an integer is given, it "
"fixes the seed. Defaults to the global numpy random number generator."
msgstr ""

#: :71
msgid "**verbose** : int, default 0"
msgstr ""

#: :73
msgid "Verbosity mode."
msgstr ""

#: :75
msgid "**copy_x** : boolean, default True"
msgstr ""

#: :77
msgid ""
"When pre-computing distances it is more numerically accurate to center "
"the data first.  If copy_x is True, then the original data is not "
"modified.  If False, the original data is modified, and put back before "
"the function returns, but small numerical differences may be introduced "
"by subtracting and then adding the data mean."
msgstr ""

#: :85
msgid "**cluster_centers_** : array, [n_clusters, n_features]"
msgstr ""

#: :87
msgid "Coordinates of cluster centers"
msgstr ""

#: :89
msgid "**labels_ :** :"
msgstr ""

#: :91
msgid "Labels of each point"
msgstr ""

#: :93
msgid "**inertia_** : float"
msgstr ""

#: :95
msgid "Sum of distances of samples to their closest cluster center."
msgstr ""

#: :99
msgid ":obj:`MiniBatchKMeans`"
msgstr ""

#: :100
msgid ""
"Alternative online implementation that does incremental updates of the "
"centers positions using mini-batches. For large scale learning (say "
"n_samples > 10k) MiniBatchKMeans is probably much faster to than the "
"default batch implementation."
msgstr ""

#: :103
msgid "Notes"
msgstr ""

#: :104
msgid "The k-means problem is solved using Lloyd's algorithm."
msgstr ""

#: :106
msgid ""
"The average complexity is given by O(k n T), were n is the number of "
"samples and T is the number of iteration."
msgstr ""

#: :109
msgid ""
"The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p "
"= n_features. (D. Arthur and S. Vassilvitskii, 'How slow is the k-means "
"method?' SoCG2006)"
msgstr ""

#: :113
msgid ""
"In practice, the k-means algorithm is very fast (one of the fastest "
"clustering algorithms available), but it falls in local minima. That's "
"why it can be useful to restart it several times."
msgstr ""

#: :118
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.cluster.KMeans.fit>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Compute k-means clustering."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit_predict <sklearn.cluster.KMeans.fit_predict>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Compute cluster centers and predict cluster index for each sample."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit_transform <sklearn.cluster.KMeans.fit_transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Compute clustering and transform X to cluster-distance space."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`get_params <sklearn.cluster.KMeans.get_params>`\\ ([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.cluster.KMeans.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict the closest cluster each sample in X belongs to."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`score <sklearn.cluster.KMeans.score>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3 :15
msgid "Opposite of the value of X on the K-means objective."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`set_params <sklearn.cluster.KMeans.set_params>`\\ (\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`transform <sklearn.cluster.KMeans.transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Transform X to a cluster-distance space."
msgstr ""

#: :7
msgid "**X** : array-like or sparse matrix, shape=(n_samples, n_features)"
msgstr ""

#: :5
msgid "Convenience method; equivalent to calling fit(X) followed by predict(X)."
msgstr ""

#: :5
msgid "Equivalent to fit(X).transform(X), but more efficiently implemented."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :5
msgid ""
"In the vector quantization literature, `cluster_centers_` is called the "
"code book and each value returned by `predict` is the index of the "
"closest code in the code book."
msgstr ""

#: :11 :7
msgid "**X** : {array-like, sparse matrix}, shape = [n_samples, n_features]"
msgstr ""

#: :13
msgid "New data to predict."
msgstr ""

#: :17
msgid "**labels** : array, shape [n_samples,]"
msgstr ""

#: :19
msgid "Index of the cluster each sample belongs to."
msgstr ""

#: :9
msgid "New data."
msgstr ""

#: :13
msgid "**score** : float"
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: :5
msgid ""
"In the new space, each dimension is the distance to the cluster centers."
"  Note that even if X is sparse, the array returned by `transform` will "
"typically be dense."
msgstr ""

#: :13
msgid "New data to transform."
msgstr ""

#: :17
msgid "**X_new** : array, shape [n_samples, k]"
msgstr ""

#: :19
msgid "X transformed in the new space."
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:3
msgid "Examples using ``sklearn.cluster.KMeans``"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:25
msgid ":ref:`example_cluster_plot_kmeans_assumptions.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:45
msgid ":ref:`example_cluster_plot_cluster_iris.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:65
msgid ":ref:`example_cluster_plot_face_compress.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:85
msgid ":ref:`example_cluster_plot_color_quantization.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:105
msgid ":ref:`example_cluster_plot_kmeans_stability_low_dim_dense.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:125
msgid ":ref:`example_cluster_plot_kmeans_digits.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:145
msgid ":ref:`example_cluster_plot_mini_batch_kmeans.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:165
msgid ":ref:`example_cluster_plot_kmeans_silhouette_analysis.py`"
msgstr ""

#: ../../modules/generated/sklearn.cluster.KMeans.examples:185
msgid ":ref:`example_text_document_clustering.py`"
msgstr ""

