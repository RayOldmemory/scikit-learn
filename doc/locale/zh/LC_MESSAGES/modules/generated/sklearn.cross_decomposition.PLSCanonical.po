# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.cross_decomposition.PLSCanonical.rst:2
msgid ":mod:`sklearn.cross_decomposition`.PLSCanonical"
msgstr ""

#: :3
msgid ""
"PLSCanonical implements the 2 blocks canonical PLS of the original Wold "
"algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000]."
msgstr ""

#: :6
msgid ""
"This class inherits from PLS with mode=\"A\" and "
"deflation_mode=\"canonical\", norm_y_weights=True and "
"algorithm=\"nipals\", but svd should provide similar results up to "
"numerical errors."
msgstr ""

#: :10
msgid "Read more in the :ref:`User Guide <cross_decomposition>`."
msgstr ""

#: :14
msgid "**scale** : boolean, scale data? (default True)"
msgstr ""

#: :16
msgid "**algorithm** : string, \"nipals\" or \"svd\""
msgstr ""

#: :18
msgid ""
"The algorithm used to estimate the weights. It will be called "
"n_components times, i.e. once for each iteration of the outer loop."
msgstr ""

#: :21
msgid "**max_iter** : an integer, (default 500)"
msgstr ""

#: :23
msgid ""
"the maximum number of iterations of the NIPALS inner loop (used only if "
"algorithm=\"nipals\")"
msgstr ""

#: :26
msgid "**tol** : non-negative real, default 1e-06"
msgstr ""

#: :28
msgid "the tolerance used in the iterative algorithm"
msgstr ""

#: :30 :17 :12
msgid "**copy** : boolean, default True"
msgstr ""

#: :32
msgid ""
"Whether the deflation should be done on a copy. Let the default value to "
"True unless you don't care about side effect"
msgstr ""

#: :35
msgid "**n_components** : int, number of components to keep. (default 2)."
msgstr ""

#: :39
msgid "**x_weights_** : array, shape = [p, n_components]"
msgstr ""

#: :41
msgid "X block weights vectors."
msgstr ""

#: :43
msgid "**y_weights_** : array, shape = [q, n_components]"
msgstr ""

#: :45
msgid "Y block weights vectors."
msgstr ""

#: :47
msgid "**x_loadings_** : array, shape = [p, n_components]"
msgstr ""

#: :49
msgid "X block loadings vectors."
msgstr ""

#: :51
msgid "**y_loadings_** : array, shape = [q, n_components]"
msgstr ""

#: :53
msgid "Y block loadings vectors."
msgstr ""

#: :55
msgid "**x_scores_** : array, shape = [n_samples, n_components]"
msgstr ""

#: :57
msgid "X scores."
msgstr ""

#: :59
msgid "**y_scores_** : array, shape = [n_samples, n_components]"
msgstr ""

#: :61
msgid "Y scores."
msgstr ""

#: :63
msgid "**x_rotations_** : array, shape = [p, n_components]"
msgstr ""

#: :65
msgid "X block to latents rotations."
msgstr ""

#: :67
msgid "**y_rotations_** : array, shape = [q, n_components]"
msgstr ""

#: :69
msgid "Y block to latents rotations."
msgstr ""

#: :71
msgid "**n_iter_** : array-like"
msgstr ""

#: :73
msgid ""
"Number of iterations of the NIPALS inner loop for each component. Not "
"useful if the algorithm provided is \"svd\"."
msgstr ""

#: :78
msgid ":obj:`CCA`, :obj:`PLSSVD`"
msgstr ""

#: :81 :17
msgid "Notes"
msgstr ""

#: :82
msgid "Matrices::"
msgstr ""

#: :91
msgid "Are computed such that::"
msgstr ""

#: :99
msgid "where Xk and Yk are residual matrices at iteration k."
msgstr ""

#: :101
msgid ""
"`Slides explaining PLS "
"<http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`"
msgstr ""

#: :103
msgid "For each component k, find weights u, v that optimize::"
msgstr ""

#: :107
msgid ""
"Note that it maximizes both the correlations between the scores and the "
"intra-block variances."
msgstr ""

#: :110
msgid ""
"The residual matrix of X (Xk+1) block is obtained by the deflation on the"
" current X score: x_score."
msgstr ""

#: :113
msgid ""
"The residual matrix of Y (Yk+1) block is obtained by deflation on the "
"current Y score. This performs a canonical symmetric version of the PLS "
"regression. But slightly different than the CCA. This is mostly used for "
"modeling."
msgstr ""

#: :118
msgid ""
"This implementation provides the same results that the \"plspm\" package "
"provided in the R language (R-project), using the function plsca(X, Y). "
"Results are equal or collinear with the function ``pls(..., mode = "
"\"canonical\")`` of the \"mixOmics\" package. The difference relies in "
"the fact that mixOmics implementation does not exactly implement the Wold"
" algorithm since it does not normalize y_weights to one."
msgstr ""

#: :126
msgid "References"
msgstr ""

#: :127
msgid ""
"Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with "
"emphasis on the two-block case. Technical Report 371, Department of "
"Statistics, University of Washington, Seattle, 2000."
msgstr ""

#: :131
msgid ""
"Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: "
"Editions Technic."
msgstr ""

#: :137
msgid "Examples"
msgstr ""

#: :149
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.cross_decomposition.PLSCanonical.fit>`\\ (X, Y)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit model to data."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit_transform "
"<sklearn.cross_decomposition.PLSCanonical.fit_transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Learn and apply the dimension reduction on the train data."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_params <sklearn.cross_decomposition.PLSCanonical.get_params>`\\"
" ([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`predict <sklearn.cross_decomposition.PLSCanonical.predict>`\\ (X[, "
"copy])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Apply the dimension reduction learned on the train data."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`score <sklearn.cross_decomposition.PLSCanonical.score>`\\ (X, y[, "
"sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Returns the coefficient of determination R^2 of the prediction."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`set_params <sklearn.cross_decomposition.PLSCanonical.set_params>`\\"
" (\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`transform <sklearn.cross_decomposition.PLSCanonical.transform>`\\ "
"(X[, Y, copy])"
msgstr ""

#: :7
msgid "**X** : array-like, shape = [n_samples, n_features]"
msgstr ""

#: :9
msgid ""
"Training vectors, where n_samples in the number of samples and n_features"
" is the number of predictors."
msgstr ""

#: :12
msgid "**Y** : array-like of response, shape = [n_samples, n_targets]"
msgstr ""

#: :14
msgid ""
"Target vectors, where n_samples in the number of samples and n_targets is"
" the number of response variables."
msgstr ""

#: :7
msgid "**X** : array-like of predictors, shape = [n_samples, p]"
msgstr ""

#: :9
msgid ""
"Training vectors, where n_samples in the number of samples and p is the "
"number of predictors."
msgstr ""

#: :12
msgid "**Y** : array-like of response, shape = [n_samples, q], optional"
msgstr ""

#: :14
msgid ""
"Training vectors, where n_samples in the number of samples and q is the "
"number of response variables."
msgstr ""

#: :19 :14
msgid "Whether to copy X and Y, or perform in-place normalization."
msgstr ""

#: :23
msgid "**x_scores if Y is not given, (x_scores, y_scores) otherwise.** :"
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :18
msgid ""
"This call requires the estimation of a p x q matrix, which may be an "
"issue in high dimensional space."
msgstr ""

#: :5
msgid ""
"The coefficient R^2 is defined as (1 - u/v), where u is the regression "
"sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum "
"of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is "
"1.0 and it can be negative (because the model can be arbitrarily worse). "
"A constant model that always predicts the expected value of y, "
"disregarding the input features, would get a R^2 score of 0.0."
msgstr ""

#: :15
msgid "**X** : array-like, shape = (n_samples, n_features)"
msgstr ""

#: :17
msgid "Test samples."
msgstr ""

#: :19
msgid "**y** : array-like, shape = (n_samples) or (n_samples, n_outputs)"
msgstr ""

#: :21
msgid "True values for X."
msgstr ""

#: :23
msgid "**sample_weight** : array-like, shape = [n_samples], optional"
msgstr ""

#: :25
msgid "Sample weights."
msgstr ""

#: :29
msgid "**score** : float"
msgstr ""

#: :31
msgid "R^2 of self.predict(X) wrt. y."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: ../../modules/generated/sklearn.cross_decomposition.PLSCanonical.examples:3
msgid "Examples using ``sklearn.cross_decomposition.PLSCanonical``"
msgstr ""

#: ../../modules/generated/sklearn.cross_decomposition.PLSCanonical.examples:25
msgid ":ref:`example_cross_decomposition_plot_compare_cross_decomposition.py`"
msgstr ""

