# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.decomposition.IncrementalPCA.rst:2
msgid ":mod:`sklearn.decomposition`.IncrementalPCA"
msgstr ""

#: :3
msgid "Incremental principal components analysis (IPCA)."
msgstr ""

#: :5
msgid ""
"Linear dimensionality reduction using Singular Value Decomposition of "
"centered data, keeping only the most significant singular vectors to "
"project the data to a lower dimensional space."
msgstr ""

#: :9
msgid ""
"Depending on the size of the input data, this algorithm can be much more "
"memory efficient than a PCA."
msgstr ""

#: :12
msgid ""
"This algorithm has constant memory complexity, on the order of "
"``batch_size``, enabling use of np.memmap files without loading the "
"entire file into memory."
msgstr ""

#: :16
msgid ""
"The computational overhead of each SVD is ``O(batch_size * n_features ** "
"2)``, but only 2 * batch_size samples remain in memory at a time. There "
"will be ``n_samples / batch_size`` SVD computations to get the principal "
"components, versus 1 large SVD of complexity ``O(n_samples * n_features "
"** 2)`` for PCA."
msgstr ""

#: :22
msgid "Read more in the :ref:`User Guide <IncrementalPCA>`."
msgstr ""

#: :26
msgid "**n_components** : int or None, (default=None)"
msgstr ""

#: :28
msgid ""
"Number of components to keep. If ``n_components `` is ``None``, then "
"``n_components`` is set to ``min(n_samples, n_features)``."
msgstr ""

#: :31
msgid "**batch_size** : int or None, (default=None)"
msgstr ""

#: :33
msgid ""
"The number of samples to use for each batch. Only used when calling "
"``fit``. If ``batch_size`` is ``None``, then ``batch_size`` is inferred "
"from the data and set to ``5 * n_features``, to provide a balance between"
" approximation accuracy and memory consumption."
msgstr ""

#: :38
msgid "**copy** : bool, (default=True)"
msgstr ""

#: :40
msgid ""
"If False, X will be overwritten. ``copy=False`` can be used to save "
"memory but is unsafe for general use."
msgstr ""

#: :43
msgid "**whiten** : bool, optional"
msgstr ""

#: :45
msgid ""
"When True (False by default) the ``components_`` vectors are divided by "
"``n_samples`` times ``components_`` to ensure uncorrelated outputs with "
"unit component-wise variances."
msgstr ""

#: :49
msgid ""
"Whitening will remove some information from the transformed signal (the "
"relative variance scales of the components) but can sometimes improve the"
" predictive accuracy of the downstream estimators by making data respect "
"some hard-wired assumptions."
msgstr ""

#: :56
msgid "**components_** : array, shape (n_components, n_features)"
msgstr ""

#: :58
msgid "Components with maximum variance."
msgstr ""

#: :60
msgid "**explained_variance_** : array, shape (n_components,)"
msgstr ""

#: :62
msgid "Variance explained by each of the selected components."
msgstr ""

#: :64
msgid "**explained_variance_ratio_** : array, shape (n_components,)"
msgstr ""

#: :66
msgid ""
"Percentage of variance explained by each of the selected components. If "
"all components are stored, the sum of explained variances is equal to 1.0"
msgstr ""

#: :70
msgid "**mean_** : array, shape (n_features,)"
msgstr ""

#: :72
msgid "Per-feature empirical mean, aggregate over calls to ``partial_fit``."
msgstr ""

#: :74
msgid "**var_** : array, shape (n_features,)"
msgstr ""

#: :76
msgid "Per-feature empirical variance, aggregate over calls to ``partial_fit``."
msgstr ""

#: :79
msgid "**noise_variance_** : float"
msgstr ""

#: :81
msgid ""
"The estimated noise covariance following the Probabilistic PCA model from"
" Tipping and Bishop 1999. See \"Pattern Recognition and Machine "
"Learning\" by C. Bishop, 12.2.1 p. 574 or "
"http://www.miketipping.com/papers/met-mppca.pdf."
msgstr ""

#: :86
msgid "**n_components_** : int"
msgstr ""

#: :88
msgid "The estimated number of components. Relevant when ``n_components=None``."
msgstr ""

#: :91
msgid "**n_samples_seen_** : int"
msgstr ""

#: :93
msgid ""
"The number of samples processed by the estimator. Will be reset on new "
"calls to fit, but increments across ``partial_fit`` calls."
msgstr ""

#: :98
msgid ""
":obj:`PCA`, :obj:`RandomizedPCA`, :obj:`KernelPCA`, :obj:`SparsePCA`, "
":obj:`TruncatedSVD`"
msgstr ""

#: :101 :19
msgid "Notes"
msgstr ""

#: :102
msgid ""
"Implements the incremental PCA model from: `D. Ross, J. Lim, R. Lin, M. "
"Yang, Incremental Learning for Robust Visual Tracking, International "
"Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008.`"
" See http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf"
msgstr ""

#: :108
msgid ""
"This model is an extension of the Sequential Karhunen-Loeve Transform "
"from: `A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis "
"Extraction and its Application to Images, IEEE Transactions on Image "
"Processing, Volume 9, Number 8, pp. 1371-1374, August 2000.` See "
"http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf"
msgstr ""

#: :114
msgid ""
"We have specifically abstained from an optimization used by authors of "
"both papers, a QR decomposition used in specific situations to reduce the"
" algorithmic complexity of the SVD. The source for this technique is "
"`Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5,"
" section 5.4.4, pp 252-253.`. This technique has been omitted because it "
"is advantageous only when decomposing a matrix with ``n_samples`` (rows) "
">= 5/3 * ``n_features`` (columns), and hurts the readability of the "
"implemented algorithm. This would be a good opportunity for future "
"optimization, if it is deemed necessary."
msgstr ""

#: :125
msgid "References"
msgstr ""

#: :128
msgid "Ross, J. Lim, R. Lin, M. Yang. Incremental Learning for Robust Visual"
msgstr ""

#: :127
msgid ""
"Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3,"
" pp. 125-141, May 2008."
msgstr ""

#: :131
msgid "Golub and C. Van Loan. Matrix Computations, Third Edition, Chapter 5,"
msgstr ""

#: :131
msgid "Section 5.4.4, pp. 252-253."
msgstr ""

#: :136
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.decomposition.IncrementalPCA.fit>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit the model with X, using minibatches of size batch_size."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit_transform "
"<sklearn.decomposition.IncrementalPCA.fit_transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit to data, then transform it."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_covariance "
"<sklearn.decomposition.IncrementalPCA.get_covariance>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Compute data covariance with the generative model."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_params <sklearn.decomposition.IncrementalPCA.get_params>`\\ "
"([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_precision "
"<sklearn.decomposition.IncrementalPCA.get_precision>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Compute data precision matrix with the generative model."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`inverse_transform "
"<sklearn.decomposition.IncrementalPCA.inverse_transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Transform data back to its original space."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`partial_fit <sklearn.decomposition.IncrementalPCA.partial_fit>`\\ "
"(X[, y, check_input])"
msgstr ""

#: ../../<autosummary>:1
msgid "Incremental fit with X."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`set_params <sklearn.decomposition.IncrementalPCA.set_params>`\\ "
"(\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`transform <sklearn.decomposition.IncrementalPCA.transform>`\\ (X[, "
"y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Apply dimensionality reduction to X."
msgstr ""

#: :7
msgid "**X: array-like, shape (n_samples, n_features)** :"
msgstr ""

#: :9
msgid ""
"Training data, where n_samples is the number of samples and n_features is"
" the number of features."
msgstr ""

#: :12
msgid "**y: Passthrough for ``Pipeline`` compatibility.** :"
msgstr ""

#: :16 :14
msgid "**self: object** :"
msgstr ""

#: :18 :16
msgid "Returns the instance itself."
msgstr ""

#: :5
msgid ""
"Fits transformer to X and y with optional parameters fit_params and "
"returns a transformed version of X."
msgstr ""

#: :10
msgid "**X** : numpy array of shape [n_samples, n_features]"
msgstr ""

#: :12
msgid "Training set."
msgstr ""

#: :14
msgid "**y** : numpy array of shape [n_samples]"
msgstr ""

#: :16
msgid "Target values."
msgstr ""

#: :20
msgid "**X_new** : numpy array of shape [n_samples, n_features_new]"
msgstr ""

#: :22
msgid "Transformed array."
msgstr ""

#: :5
msgid ""
"``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)`` "
"where  S**2 contains the explained variances, and sigma2 contains the "
"noise variances."
msgstr ""

#: :11
msgid "**cov** : array, shape=(n_features, n_features)"
msgstr ""

#: :13
msgid "Estimated covariance of data."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :5
msgid ""
"Equals the inverse of the covariance but computed with the matrix "
"inversion lemma for efficiency."
msgstr ""

#: :10
msgid "**precision** : array, shape=(n_features, n_features)"
msgstr ""

#: :12
msgid "Estimated precision of data."
msgstr ""

#: :5
msgid "In other words, return an input X_original whose transform would be X."
msgstr ""

#: :9
msgid "**X** : array-like, shape (n_samples, n_components)"
msgstr ""

#: :11
msgid ""
"New data, where n_samples is the number of samples and n_components is "
"the number of components."
msgstr ""

#: :16
msgid "**X_original array-like, shape (n_samples, n_features)** :"
msgstr ""

#: :20
msgid ""
"If whitening is enabled, inverse_transform will compute the exact inverse"
" operation, which includes reversing whitening."
msgstr ""

#: :3
msgid "Incremental fit with X. All of X is processed as a single batch."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: :5
msgid ""
"X is projected on the first principal components previously extracted "
"from a training set."
msgstr ""

#: :10
msgid "**X** : array-like, shape (n_samples, n_features)"
msgstr ""

#: :12
msgid ""
"New data, where n_samples is the number of samples and n_features is the "
"number of features."
msgstr ""

#: :17
msgid "**X_new** : array-like, shape (n_samples, n_components)"
msgstr ""

#: :20
msgid "Examples"
msgstr ""

#: ../../modules/generated/sklearn.decomposition.IncrementalPCA.examples:3
msgid "Examples using ``sklearn.decomposition.IncrementalPCA``"
msgstr ""

#: ../../modules/generated/sklearn.decomposition.IncrementalPCA.examples:25
msgid ":ref:`example_decomposition_plot_incremental_pca.py`"
msgstr ""

