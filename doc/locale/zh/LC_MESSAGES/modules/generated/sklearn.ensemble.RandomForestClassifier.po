# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.rst:2
msgid ":mod:`sklearn.ensemble`.RandomForestClassifier"
msgstr ""

#: :3
msgid "A random forest classifier."
msgstr ""

#: :5
msgid ""
"A random forest is a meta estimator that fits a number of decision tree "
"classifiers on various sub-samples of the dataset and use averaging to "
"improve the predictive accuracy and control over-fitting. The sub-sample "
"size is always the same as the original input sample size but the samples"
" are drawn with replacement if `bootstrap=True` (default)."
msgstr ""

#: :12
msgid "Read more in the :ref:`User Guide <forest>`."
msgstr ""

#: :16
msgid "**n_estimators** : integer, optional (default=10)"
msgstr ""

#: :18
msgid "The number of trees in the forest."
msgstr ""

#: :20
msgid "**criterion** : string, optional (default=\"gini\")"
msgstr ""

#: :22
msgid ""
"The function to measure the quality of a split. Supported criteria are "
"\"gini\" for the Gini impurity and \"entropy\" for the information gain. "
"Note: this parameter is tree-specific."
msgstr ""

#: :26
msgid "**max_features** : int, float, string or None, optional (default=\"auto\")"
msgstr ""

#: :28
msgid "The number of features to consider when looking for the best split:"
msgstr ""

#: :30
msgid "If int, then consider `max_features` features at each split."
msgstr ""

#: :31
msgid ""
"If float, then `max_features` is a percentage and `int(max_features * "
"n_features)` features are considered at each split."
msgstr ""

#: :34
msgid "If \"auto\", then `max_features=sqrt(n_features)`."
msgstr ""

#: :35
msgid "If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\")."
msgstr ""

#: :36
msgid "If \"log2\", then `max_features=log2(n_features)`."
msgstr ""

#: :37
msgid "If None, then `max_features=n_features`."
msgstr ""

#: :39
msgid ""
"Note: the search for a split does not stop until at least one valid "
"partition of the node samples is found, even if it requires to "
"effectively inspect more than ``max_features`` features. Note: this "
"parameter is tree-specific."
msgstr ""

#: :44
msgid "**max_depth** : integer or None, optional (default=None)"
msgstr ""

#: :46
msgid ""
"The maximum depth of the tree. If None, then nodes are expanded until all"
" leaves are pure or until all leaves contain less than min_samples_split "
"samples. Ignored if ``max_leaf_nodes`` is not None. Note: this parameter "
"is tree-specific."
msgstr ""

#: :52
msgid "**min_samples_split** : integer, optional (default=2)"
msgstr ""

#: :54
msgid ""
"The minimum number of samples required to split an internal node. Note: "
"this parameter is tree-specific."
msgstr ""

#: :57
msgid "**min_samples_leaf** : integer, optional (default=1)"
msgstr ""

#: :59
msgid ""
"The minimum number of samples in newly created leaves.  A split is "
"discarded if after the split, one of the leaves would contain less then "
"``min_samples_leaf`` samples. Note: this parameter is tree-specific."
msgstr ""

#: :64
msgid "**min_weight_fraction_leaf** : float, optional (default=0.)"
msgstr ""

#: :66
msgid ""
"The minimum weighted fraction of the input samples required to be at a "
"leaf node. Note: this parameter is tree-specific."
msgstr ""

#: :70
msgid "**max_leaf_nodes** : int or None, optional (default=None)"
msgstr ""

#: :72
msgid ""
"Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are "
"defined as relative reduction in impurity. If None then unlimited number "
"of leaf nodes. If not None then ``max_depth`` will be ignored. Note: this"
" parameter is tree-specific."
msgstr ""

#: :78
msgid "**bootstrap** : boolean, optional (default=True)"
msgstr ""

#: :80
msgid "Whether bootstrap samples are used when building trees."
msgstr ""

#: :82
msgid "**oob_score** : bool"
msgstr ""

#: :84
msgid "Whether to use out-of-bag samples to estimate the generalization error."
msgstr ""

#: :87
msgid "**n_jobs** : integer, optional (default=1)"
msgstr ""

#: :89
msgid ""
"The number of jobs to run in parallel for both `fit` and `predict`. If "
"-1, then the number of jobs is set to the number of cores."
msgstr ""

#: :92
msgid ""
"**random_state** : int, RandomState instance or None, optional "
"(default=None)"
msgstr ""

#: :94
msgid ""
"If int, random_state is the seed used by the random number generator; If "
"RandomState instance, random_state is the random number generator; If "
"None, the random number generator is the RandomState instance used by "
"`np.random`."
msgstr ""

#: :99
msgid "**verbose** : int, optional (default=0)"
msgstr ""

#: :101
msgid "Controls the verbosity of the tree building process."
msgstr ""

#: :103
msgid "**warm_start** : bool, optional (default=False)"
msgstr ""

#: :105
msgid ""
"When set to ``True``, reuse the solution of the previous call to fit and "
"add more estimators to the ensemble, otherwise, just fit a whole new "
"forest."
msgstr ""

#: :109
msgid ""
"**class_weight** : dict, list of dicts, \"balanced\", "
"\"balanced_subsample\" or None, optional"
msgstr ""

#: :111
msgid ""
"Weights associated with classes in the form ``{class_label: weight}``. If"
" not given, all classes are supposed to have weight one. For multi-output"
" problems, a list of dicts can be provided in the same order as the "
"columns of y."
msgstr ""

#: :116
msgid ""
"The \"balanced\" mode uses the values of y to automatically adjust "
"weights inversely proportional to class frequencies in the input data as "
"``n_samples / (n_classes * np.bincount(y))``"
msgstr ""

#: :120
msgid ""
"The \"balanced_subsample\" mode is the same as \"balanced\" except that "
"weights are computed based on the bootstrap sample for every tree grown."
msgstr ""

#: :123
msgid "For multi-output, the weights of each column of y will be multiplied."
msgstr ""

#: :125
msgid ""
"Note that these weights will be multiplied with sample_weight (passed "
"through the fit method) if sample_weight is specified."
msgstr ""

#: :130
msgid "**estimators_** : list of DecisionTreeClassifier"
msgstr ""

#: :132
msgid "The collection of fitted sub-estimators."
msgstr ""

#: :134
msgid "**classes_** : array of shape = [n_classes] or a list of such arrays"
msgstr ""

#: :136
msgid ""
"The classes labels (single output problem), or a list of arrays of class "
"labels (multi-output problem)."
msgstr ""

#: :139
msgid "**n_classes_** : int or list"
msgstr ""

#: :141
msgid ""
"The number of classes (single output problem), or a list containing the "
"number of classes for each output (multi-output problem)."
msgstr ""

#: :144
msgid "**n_features_** : int"
msgstr ""

#: :146
msgid "The number of features when ``fit`` is performed."
msgstr ""

#: :148
msgid "**n_outputs_** : int"
msgstr ""

#: :150
msgid "The number of outputs when ``fit`` is performed."
msgstr ""

#: :152
msgid "**feature_importances_** : array of shape = [n_features]"
msgstr ""

#: :154
msgid "The feature importances (the higher, the more important the feature)."
msgstr ""

#: :156
msgid "**oob_score_** : float"
msgstr ""

#: :158
msgid "Score of the training dataset obtained using an out-of-bag estimate."
msgstr ""

#: :160
msgid "**oob_decision_function_** : array of shape = [n_samples, n_classes]"
msgstr ""

#: :162
msgid ""
"Decision function computed with out-of-bag estimate on the training set. "
"If n_estimators is small it might be possible that a data point was never"
" left out during the bootstrap. In this case, `oob_decision_function_` "
"might contain NaN."
msgstr ""

#: :169
msgid ":obj:`DecisionTreeClassifier`, :obj:`ExtraTreesClassifier`"
msgstr ""

#: :172
msgid "References"
msgstr ""

#: :173
msgid "Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001."
msgstr ""

#: :180
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`apply <sklearn.ensemble.RandomForestClassifier.apply>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Apply trees in the forest to X, return leaf indices."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit <sklearn.ensemble.RandomForestClassifier.fit>`\\ (X, y[, "
"sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Build a forest of trees from the training set (X, y)."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit_transform "
"<sklearn.ensemble.RandomForestClassifier.fit_transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit to data, then transform it."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_params <sklearn.ensemble.RandomForestClassifier.get_params>`\\ "
"([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.ensemble.RandomForestClassifier.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict class for X."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`predict_log_proba "
"<sklearn.ensemble.RandomForestClassifier.predict_log_proba>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict class log-probabilities for X."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`predict_proba "
"<sklearn.ensemble.RandomForestClassifier.predict_proba>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict class probabilities for X."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`score <sklearn.ensemble.RandomForestClassifier.score>`\\ (X, y[, "
"sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Returns the mean accuracy on the given test data and labels."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`set_params <sklearn.ensemble.RandomForestClassifier.set_params>`\\ "
"(\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`transform <sklearn.ensemble.RandomForestClassifier.transform>`\\ "
"(\\*args, \\*\\*kwargs)"
msgstr ""

#: ../../<autosummary>:1
msgid ""
"DEPRECATED: Support to use estimators as feature selectors will be "
"removed in version 0.19."
msgstr ""

#: :7
msgid "**X** : array-like or sparse matrix, shape = [n_samples, n_features]"
msgstr ""

#: :9 :14 :13
msgid ""
"The input samples. Internally, it will be converted to "
"``dtype=np.float32`` and if a sparse matrix is provided to a sparse "
"``csr_matrix``."
msgstr ""

#: :15
msgid "**X_leaves** : array_like, shape = [n_samples, n_estimators]"
msgstr ""

#: :17
msgid ""
"For each datapoint x in X and for each tree in the forest, return the "
"index of the leaf x ends up in."
msgstr ""

#: :4
msgid "Return the feature importances (the higher, the more important the"
msgstr ""

#: :4
msgid "feature)."
msgstr ""

#: :8
msgid "**feature_importances_** : array, shape = [n_features]"
msgstr ""

#: :7 :12 :11
msgid "**X** : array-like or sparse matrix of shape = [n_samples, n_features]"
msgstr ""

#: :9
msgid ""
"The training input samples. Internally, it will be converted to "
"``dtype=np.float32`` and if a sparse matrix is provided to a sparse "
"``csc_matrix``."
msgstr ""

#: :13
msgid "**y** : array-like, shape = [n_samples] or [n_samples, n_outputs]"
msgstr ""

#: :15
msgid ""
"The target values (class labels in classification, real numbers in "
"regression)."
msgstr ""

#: :18
msgid "**sample_weight** : array-like, shape = [n_samples] or None"
msgstr ""

#: :20
msgid ""
"Sample weights. If None, then samples are equally weighted. Splits that "
"would create child nodes with net zero or negative weight are ignored "
"while searching for a split in each node. In the case of classification, "
"splits are also ignored if they would result in any single class carrying"
" a negative weight in either child node."
msgstr ""

#: :28
msgid "**self** : object"
msgstr ""

#: :30
msgid "Returns self."
msgstr ""

#: :5
msgid ""
"Fits transformer to X and y with optional parameters fit_params and "
"returns a transformed version of X."
msgstr ""

#: :10
msgid "**X** : numpy array of shape [n_samples, n_features]"
msgstr ""

#: :12
msgid "Training set."
msgstr ""

#: :14
msgid "**y** : numpy array of shape [n_samples]"
msgstr ""

#: :16
msgid "Target values."
msgstr ""

#: :20
msgid "**X_new** : numpy array of shape [n_samples, n_features_new]"
msgstr ""

#: :22
msgid "Transformed array."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :5
msgid ""
"The predicted class of an input sample is a vote by the trees in the "
"forest, weighted by their probability estimates. That is, the predicted "
"class is the one with highest mean probability estimate across the trees."
msgstr ""

#: :20
msgid "**y** : array of shape = [n_samples] or [n_samples, n_outputs]"
msgstr ""

#: :22
msgid "The predicted classes."
msgstr ""

#: :5
msgid ""
"The predicted class log-probabilities of an input sample is computed as "
"the log of the mean predicted class probabilities of the trees in the "
"forest."
msgstr ""

#: :19 :20
msgid "**p** : array of shape = [n_samples, n_classes], or a list of n_outputs"
msgstr ""

#: :21 :22
msgid ""
"such arrays if n_outputs > 1. The class probabilities of the input "
"samples. The order of the classes corresponds to that in the attribute "
"`classes_`."
msgstr ""

#: :5
msgid ""
"The predicted class probabilities of an input sample is computed as the "
"mean predicted class probabilities of the trees in the forest. The class "
"probability of a single tree is the fraction of samples of the same class"
" in a leaf."
msgstr ""

#: :5
msgid ""
"In multi-label classification, this is the subset accuracy which is a "
"harsh metric since you require for each sample that each label set be "
"correctly predicted."
msgstr ""

#: :11
msgid "**X** : array-like, shape = (n_samples, n_features)"
msgstr ""

#: :13
msgid "Test samples."
msgstr ""

#: :15
msgid "**y** : array-like, shape = (n_samples) or (n_samples, n_outputs)"
msgstr ""

#: :17
msgid "True labels for X."
msgstr ""

#: :19
msgid "**sample_weight** : array-like, shape = [n_samples], optional"
msgstr ""

#: :21
msgid "Sample weights."
msgstr ""

#: :25
msgid "**score** : float"
msgstr ""

#: :27
msgid "Mean accuracy of self.predict(X) wrt. y."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: :3
msgid ""
"DEPRECATED: Support to use estimators as feature selectors will be "
"removed in version 0.19. Use SelectFromModel instead."
msgstr ""

#: :5
msgid "Reduce X to its most important features."
msgstr ""

#: :7
msgid ""
"Uses ``coef_`` or ``feature_importances_`` to determine the most "
"important features.  For models with a ``coef_`` for each class, the "
"absolute sum over the classes is used."
msgstr ""

#: :13
msgid "**X** : array or scipy sparse matrix of shape [n_samples, n_features]"
msgstr ""

#: :15
msgid "The input samples."
msgstr ""

#: :24
msgid "threshold"
msgstr ""

#: :23
msgid "string, float or None, optional (default=None)"
msgstr ""

#: :18
msgid ""
"The threshold value to use for feature selection. Features whose "
"importance is greater or equal are kept while the others are discarded. "
"If \"median\" (resp. \"mean\"), then the threshold value is the median "
"(resp. the mean) of the feature importances. A scaling factor (e.g., "
"\"1.25*mean\") may also be used. If None and if available, the object "
"attribute ``threshold`` is used. Otherwise, \"mean\" is used by default."
msgstr ""

#: :28
msgid "**X_r** : array of shape [n_samples, n_selected_features]"
msgstr ""

#: :30
msgid "The input samples with only the selected features."
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:3
msgid "Examples using ``sklearn.ensemble.RandomForestClassifier``"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:25
msgid ":ref:`example_calibration_plot_compare_calibration.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:45
msgid ":ref:`example_calibration_plot_calibration_multiclass.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:65
msgid ":ref:`example_classification_plot_classifier_comparison.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:85
msgid ":ref:`example_ensemble_plot_voting_probas.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:105
msgid ":ref:`example_ensemble_plot_ensemble_oob.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:125
msgid ":ref:`example_ensemble_plot_feature_transformation.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:145
msgid ":ref:`example_ensemble_plot_forest_iris.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:165
msgid ":ref:`example_model_selection_randomized_search.py`"
msgstr ""

#: ../../modules/generated/sklearn.ensemble.RandomForestClassifier.examples:185
msgid ":ref:`example_text_document_classification_20newsgroups.py`"
msgstr ""

