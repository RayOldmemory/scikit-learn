# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.rst:2
msgid ":mod:`sklearn.feature_extraction.text`.HashingVectorizer"
msgstr ""

#: :3
msgid "Convert a collection of text documents to a matrix of token occurrences"
msgstr ""

#: :5
msgid ""
"It turns a collection of text documents into a scipy.sparse matrix "
"holding token occurrence counts (or binary occurrence information), "
"possibly normalized as token frequencies if norm='l1' or projected on the"
" euclidean unit sphere if norm='l2'."
msgstr ""

#: :10
msgid ""
"This text vectorizer implementation uses the hashing trick to find the "
"token string name to feature integer index mapping."
msgstr ""

#: :13
msgid "This strategy has several advantages:"
msgstr ""

#: :15
msgid ""
"it is very low memory scalable to large datasets as there is no need to "
"store a vocabulary dictionary in memory"
msgstr ""

#: :18
msgid ""
"it is fast to pickle and un-pickle as it holds no state besides the "
"constructor parameters"
msgstr ""

#: :21
msgid ""
"it can be used in a streaming (partial fit) or parallel pipeline as there"
" is no state computed during fit."
msgstr ""

#: :24
msgid ""
"There are also a couple of cons (vs using a CountVectorizer with an in-"
"memory vocabulary):"
msgstr ""

#: :27
msgid ""
"there is no way to compute the inverse transform (from feature indices to"
" string feature names) which can be a problem when trying to introspect "
"which features are most important to a model."
msgstr ""

#: :31
msgid ""
"there can be collisions: distinct tokens can be mapped to the same "
"feature index. However in practice this is rarely an issue if n_features "
"is large enough (e.g. 2 ** 18 for text classification problems)."
msgstr ""

#: :35
msgid "no IDF weighting as this would render the transformer stateful."
msgstr ""

#: :37
msgid "The hash function employed is the signed 32-bit version of Murmurhash3."
msgstr ""

#: :39
msgid "Read more in the :ref:`User Guide <text_feature_extraction>`."
msgstr ""

#: :43
msgid "**input** : string {'filename', 'file', 'content'}"
msgstr ""

#: :45
msgid ""
"If 'filename', the sequence passed as an argument to fit is expected to "
"be a list of filenames that need reading to fetch the raw content to "
"analyze."
msgstr ""

#: :49
msgid ""
"If 'file', the sequence items must have a 'read' method (file-like "
"object) that is called to fetch the bytes in memory."
msgstr ""

#: :52
msgid ""
"Otherwise the input is expected to be the sequence strings or bytes items"
" are expected to be analyzed directly."
msgstr ""

#: :55
msgid "**encoding** : string, default='utf-8'"
msgstr ""

#: :57
msgid "If bytes or files are given to analyze, this encoding is used to decode."
msgstr ""

#: :60
msgid "**decode_error** : {'strict', 'ignore', 'replace'}"
msgstr ""

#: :62
msgid ""
"Instruction on what to do if a byte sequence is given to analyze that "
"contains characters not of the given `encoding`. By default, it is "
"'strict', meaning that a UnicodeDecodeError will be raised. Other values "
"are 'ignore' and 'replace'."
msgstr ""

#: :67
msgid "**strip_accents** : {'ascii', 'unicode', None}"
msgstr ""

#: :69
msgid ""
"Remove accents during the preprocessing step. 'ascii' is a fast method "
"that only works on characters that have an direct ASCII mapping. "
"'unicode' is a slightly slower method that works on any characters. None "
"(default) does nothing."
msgstr ""

#: :75
msgid "**analyzer** : string, {'word', 'char', 'char_wb'} or callable"
msgstr ""

#: :77
msgid ""
"Whether the feature should be made of word or character n-grams. Option "
"'char_wb' creates character n-grams only from text inside word "
"boundaries."
msgstr ""

#: :81
msgid ""
"If a callable is passed it is used to extract the sequence of features "
"out of the raw, unprocessed input."
msgstr ""

#: :84
msgid "**preprocessor** : callable or None (default)"
msgstr ""

#: :86
msgid ""
"Override the preprocessing (string transformation) stage while preserving"
" the tokenizing and n-grams generation steps."
msgstr ""

#: :89
msgid "**tokenizer** : callable or None (default)"
msgstr ""

#: :91
msgid ""
"Override the string tokenization step while preserving the preprocessing "
"and n-grams generation steps. Only applies if ``analyzer == 'word'``."
msgstr ""

#: :95
msgid "**ngram_range** : tuple (min_n, max_n), default=(1, 1)"
msgstr ""

#: :97
msgid ""
"The lower and upper boundary of the range of n-values for different "
"n-grams to be extracted. All values of n such that min_n <= n <= max_n "
"will be used."
msgstr ""

#: :101
msgid "**stop_words** : string {'english'}, list, or None (default)"
msgstr ""

#: :103
msgid "If 'english', a built-in stop word list for English is used."
msgstr ""

#: :105
msgid ""
"If a list, that list is assumed to contain stop words, all of which will "
"be removed from the resulting tokens. Only applies if ``analyzer == "
"'word'``."
msgstr ""

#: :109
msgid "**lowercase** : boolean, default=True"
msgstr ""

#: :111
msgid "Convert all characters to lowercase before tokenizing."
msgstr ""

#: :113
msgid "**token_pattern** : string"
msgstr ""

#: :115
msgid ""
"Regular expression denoting what constitutes a \"token\", only used if "
"``analyzer == 'word'``. The default regexp selects tokens of 2 or more "
"alphanumeric characters (punctuation is completely ignored and always "
"treated as a token separator)."
msgstr ""

#: :120
msgid "**n_features** : integer, default=(2 ** 20)"
msgstr ""

#: :122
msgid ""
"The number of features (columns) in the output matrices. Small numbers of"
" features are likely to cause hash collisions, but large numbers will "
"cause larger coefficient dimensions in linear learners."
msgstr ""

#: :126
msgid "**norm** : 'l1', 'l2' or None, optional"
msgstr ""

#: :128
msgid "Norm used to normalize term vectors. None for no normalization."
msgstr ""

#: :130
msgid "**binary: boolean, default=False.** :"
msgstr ""

#: :132
msgid ""
"If True, all non zero counts are set to 1. This is useful for discrete "
"probabilistic models that model binary events rather than integer counts."
msgstr ""

#: :136
msgid "**dtype: type, optional** :"
msgstr ""

#: :138
msgid "Type of the matrix returned by fit_transform() or transform()."
msgstr ""

#: :140
msgid "**non_negative** : boolean, default=False"
msgstr ""

#: :142
msgid ""
"Whether output matrices should contain non-negative values only; "
"effectively calls abs on the matrix prior to returning it. When True, "
"output values can be interpreted as frequencies. When False, output "
"values will have expected value zero."
msgstr ""

#: :149
msgid ":obj:`CountVectorizer`, :obj:`TfidfVectorizer`"
msgstr ""

#: :152
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`build_analyzer "
"<sklearn.feature_extraction.text.HashingVectorizer.build_analyzer>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Return a callable that handles preprocessing and tokenization"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`build_preprocessor "
"<sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor>`\\"
" ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Return a function to preprocess the text before tokenization"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`build_tokenizer "
"<sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Return a function that splits a string into a sequence of tokens"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`decode "
"<sklearn.feature_extraction.text.HashingVectorizer.decode>`\\ (doc)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Decode the input into a string of unicode symbols"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit <sklearn.feature_extraction.text.HashingVectorizer.fit>`\\ (X[,"
" y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Does nothing: this transformer is stateless."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit_transform "
"<sklearn.feature_extraction.text.HashingVectorizer.fit_transform>`\\ (X[,"
" y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Transform a sequence of documents to a document-term matrix."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_params "
"<sklearn.feature_extraction.text.HashingVectorizer.get_params>`\\ "
"([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_stop_words "
"<sklearn.feature_extraction.text.HashingVectorizer.get_stop_words>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Build or fetch the effective stop words list"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`partial_fit "
"<sklearn.feature_extraction.text.HashingVectorizer.partial_fit>`\\ (X[, "
"y])"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`set_params "
"<sklearn.feature_extraction.text.HashingVectorizer.set_params>`\\ "
"(\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`transform "
"<sklearn.feature_extraction.text.HashingVectorizer.transform>`\\ (X[, y])"
msgstr ""

#: :5
msgid "The decoding strategy depends on the vectorizer parameters."
msgstr ""

#: :7
msgid "**X** : iterable over raw text documents, length = n_samples"
msgstr ""

#: :9
msgid ""
"Samples. Each sample must be a text document (either bytes or unicode "
"strings, file name or file object depending on the constructor argument) "
"which will be tokenized and hashed."
msgstr ""

#: :13
msgid "**y** : (ignored)"
msgstr ""

#: :17
msgid "**X** : scipy.sparse matrix, shape = (n_samples, self.n_features)"
msgstr ""

#: :19
msgid "Document-term matrix."
msgstr ""

#: :3
msgid ""
"DEPRECATED: The `fixed_vocabulary` attribute is deprecated and will be "
"removed in 0.18.  Please use `fixed_vocabulary_` instead."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :5
msgid ""
"This method is just there to mark the fact that this transformer can work"
" in a streaming setup."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: ../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:3
msgid "Examples using ``sklearn.feature_extraction.text.HashingVectorizer``"
msgstr ""

#: ../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:25
msgid ":ref:`example_applications_plot_out_of_core_classification.py`"
msgstr ""

#: ../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:45
msgid ":ref:`example_text_document_clustering.py`"
msgstr ""

#: ../../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:65
msgid ":ref:`example_text_document_classification_20newsgroups.py`"
msgstr ""

