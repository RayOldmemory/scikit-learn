# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.linear_model.ElasticNetCV.rst:2
msgid ":mod:`sklearn.linear_model`.ElasticNetCV"
msgstr ""

#: :3
msgid "Elastic Net model with iterative fitting along a regularization path"
msgstr ""

#: :5
msgid "The best model is selected by cross-validation."
msgstr ""

#: :7 :25
msgid "Read more in the :ref:`User Guide <elastic_net>`."
msgstr ""

#: :11
msgid "**l1_ratio** : float or array of floats, optional"
msgstr ""

#: :13
msgid ""
"float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 "
"penalties). For ``l1_ratio = 0`` the penalty is an L2 penalty. For "
"``l1_ratio = 1`` it is an L1 penalty. For ``0 < l1_ratio < 1``, the "
"penalty is a combination of L1 and L2 This parameter can be a list, in "
"which case the different values are tested by cross-validation and the "
"one giving the best prediction score is used. Note that a good choice of "
"list of values for l1_ratio is often to put more values close to 1 (i.e. "
"Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, "
".99, 1]``"
msgstr ""

#: :24
msgid "**eps** : float, optional"
msgstr ""

#: :26
msgid ""
"Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = "
"1e-3``."
msgstr ""

#: :29 :49
msgid "**n_alphas** : int, optional"
msgstr ""

#: :31
msgid "Number of alphas along the regularization path, used for each l1_ratio."
msgstr ""

#: :33
msgid "**alphas** : numpy array, optional"
msgstr ""

#: :35 :55
msgid ""
"List of alphas where to compute the models. If None alphas are set "
"automatically"
msgstr ""

#: :38 :58
msgid "**precompute** : True | False | 'auto' | array-like"
msgstr ""

#: :40 :60
msgid ""
"Whether to use a precomputed Gram matrix to speed up calculations. If set"
" to ``'auto'`` let us decide. The Gram matrix can also be passed as "
"argument."
msgstr ""

#: :44
msgid "**max_iter** : int, optional"
msgstr ""

#: :46
msgid "The maximum number of iterations"
msgstr ""

#: :48
msgid "**tol** : float, optional"
msgstr ""

#: :50
msgid ""
"The tolerance for the optimization: if the updates are smaller than "
"``tol``, the optimization code checks the dual gap for optimality and "
"continues until it is smaller than ``tol``."
msgstr ""

#: :55
msgid "**cv** : int, cross-validation generator or an iterable, optional"
msgstr ""

#: :57
msgid ""
"Determines the cross-validation splitting strategy. Possible inputs for "
"cv are:"
msgstr ""

#: :60
msgid "None, to use the default 3-fold cross-validation,"
msgstr ""

#: :61
msgid "integer, to specify the number of folds."
msgstr ""

#: :62
msgid "An object to be used as a cross-validation generator."
msgstr ""

#: :63
msgid "An iterable yielding train/test splits."
msgstr ""

#: :65
msgid "For integer/None inputs, :class:`KFold` is used."
msgstr ""

#: :67
msgid ""
"Refer :ref:`User Guide <cross_validation>` for the various cross-"
"validation strategies that can be used here."
msgstr ""

#: :70 :77
msgid "**verbose** : bool or integer"
msgstr ""

#: :72 :79
msgid "Amount of verbosity."
msgstr ""

#: :74
msgid "**n_jobs** : integer, optional"
msgstr ""

#: :76
msgid ""
"Number of CPUs to use during the cross validation. If ``-1``, use all the"
" CPUs."
msgstr ""

#: :79
msgid "**positive** : bool, optional"
msgstr ""

#: :81
msgid "When set to ``True``, forces the coefficients to be positive."
msgstr ""

#: :83
msgid "**selection** : str, default 'cyclic'"
msgstr ""

#: :85
msgid ""
"If set to 'random', a random coefficient is updated every iteration "
"rather than looping over features sequentially by default. This (setting "
"to 'random') often leads to significantly faster convergence especially "
"when tol is higher than 1e-4."
msgstr ""

#: :90
msgid "**random_state** : int, RandomState instance, or None (default)"
msgstr ""

#: :92
msgid ""
"The seed of the pseudo random number generator that selects a random "
"feature to update. Useful only when selection is set to 'random'."
msgstr ""

#: :96
msgid "**fit_intercept** : boolean"
msgstr ""

#: :98
msgid ""
"whether to calculate the intercept for this model. If set to false, no "
"intercept will be used in calculations (e.g. data is expected to be "
"already centered)."
msgstr ""

#: :102
msgid "**normalize** : boolean, optional, default False"
msgstr ""

#: :104
msgid "If ``True``, the regressors X will be normalized before regression."
msgstr ""

#: :106 :69
msgid "**copy_X** : boolean, optional, default True"
msgstr ""

#: :108 :71
msgid "If ``True``, X will be copied; else, it may be overwritten."
msgstr ""

#: :112
msgid "**alpha_** : float"
msgstr ""

#: :114
msgid "The amount of penalization chosen by cross validation"
msgstr ""

#: :116
msgid "**l1_ratio_** : float"
msgstr ""

#: :118
msgid "The compromise between l1 and l2 penalization chosen by cross validation"
msgstr ""

#: :121
msgid "**coef_** : array, shape (n_features,) | (n_targets, n_features)"
msgstr ""

#: :123
msgid "Parameter vector (w in the cost function formula),"
msgstr ""

#: :125
msgid "**intercept_** : float | array, shape (n_targets, n_features)"
msgstr ""

#: :127
msgid "Independent term in the decision function."
msgstr ""

#: :129
msgid "**mse_path_** : array, shape (n_l1_ratio, n_alpha, n_folds)"
msgstr ""

#: :131
msgid ""
"Mean square error for the test set on each fold, varying l1_ratio and "
"alpha."
msgstr ""

#: :134
msgid "**alphas_** : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas)"
msgstr ""

#: :136
msgid "The grid of alphas used for fitting, for each l1_ratio."
msgstr ""

#: :138
msgid "**n_iter_** : int"
msgstr ""

#: :140
msgid ""
"number of iterations run by the coordinate descent solver to reach the "
"specified tolerance for the optimal alpha."
msgstr ""

#: :145
msgid ":obj:`enet_path`, :obj:`ElasticNet`"
msgstr ""

#: :148 :123
msgid "Notes"
msgstr ""

#: :149
msgid ""
"See examples/linear_model/lasso_path_with_crossvalidation.py for an "
"example."
msgstr ""

#: :152
msgid ""
"To avoid unnecessary memory duplication the X argument of the fit method "
"should be directly passed as a Fortran-contiguous numpy array."
msgstr ""

#: :155
msgid ""
"The parameter l1_ratio corresponds to alpha in the glmnet R package while"
" alpha corresponds to the lambda parameter in glmnet. More specifically, "
"the optimization objective is::"
msgstr ""

#: :163
msgid ""
"If you are interested in controlling the L1 and L2 penalty separately, "
"keep in mind that this is equivalent to::"
msgstr ""

#: :168
msgid "for::"
msgstr ""

#: :173
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`decision_function "
"<sklearn.linear_model.ElasticNetCV.decision_function>`\\ (\\*args, "
"\\*\\*kwargs)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "DEPRECATED:  and will be removed in 0.19."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.linear_model.ElasticNetCV.fit>`\\ (X, y)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit linear model with coordinate descent"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_params <sklearn.linear_model.ElasticNetCV.get_params>`\\ "
"([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`path <sklearn.linear_model.ElasticNetCV.path>`\\ (X, y[, l1_ratio, "
"eps, n_alphas, ...])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Compute elastic net path with coordinate descent"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.linear_model.ElasticNetCV.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict using the linear model"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`score <sklearn.linear_model.ElasticNetCV.score>`\\ (X, y[, "
"sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Returns the coefficient of determination R^2 of the prediction."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`set_params <sklearn.linear_model.ElasticNetCV.set_params>`\\ "
"(\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: :5
msgid "Decision function of the linear model."
msgstr ""

#: :9 :7
msgid "**X** : {array-like, sparse matrix}, shape = (n_samples, n_features)"
msgstr ""

#: :11 :9
msgid "Samples."
msgstr ""

#: :15 :13
msgid "**C** : array, shape = (n_samples,)"
msgstr ""

#: :17 :15
msgid "Returns predicted values."
msgstr ""

#: :5
msgid "Fit is on grid of alphas and best alpha estimated by cross-validation."
msgstr ""

#: :9 :29
msgid "**X** : {array-like}, shape (n_samples, n_features)"
msgstr ""

#: :11
msgid ""
"Training data. Pass directly as float64, Fortran-contiguous data to avoid"
" unnecessary memory duplication. If y is mono-output, X can be sparse."
msgstr ""

#: :15
msgid "**y** : array-like, shape (n_samples,) or (n_samples, n_targets)"
msgstr ""

#: :17 :37
msgid "Target values"
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :5
msgid "The elastic net optimization function varies for mono and multi-outputs."
msgstr ""

#: :7
msgid "For mono-output tasks it is::"
msgstr ""

#: :13
msgid "For multi-output tasks it is::"
msgstr ""

#: :19
msgid "Where::"
msgstr ""

#: :23
msgid "i.e. the sum of norm of each row."
msgstr ""

#: :31
msgid ""
"Training data. Pass directly as Fortran-contiguous data to avoid "
"unnecessary memory duplication. If ``y`` is mono-output then ``X`` can be"
" sparse."
msgstr ""

#: :35
msgid "**y** : ndarray, shape (n_samples,) or (n_samples, n_outputs)"
msgstr ""

#: :39
msgid "**l1_ratio** : float, optional"
msgstr ""

#: :41
msgid ""
"float between 0 and 1 passed to elastic net (scaling between l1 and l2 "
"penalties). ``l1_ratio=1`` corresponds to the Lasso"
msgstr ""

#: :44
msgid "**eps** : float"
msgstr ""

#: :46
msgid ""
"Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = "
"1e-3``"
msgstr ""

#: :51
msgid "Number of alphas along the regularization path"
msgstr ""

#: :53
msgid "**alphas** : ndarray, optional"
msgstr ""

#: :64
msgid "**Xy** : array-like, optional"
msgstr ""

#: :66
msgid ""
"Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the "
"Gram matrix is precomputed."
msgstr ""

#: :73
msgid "**coef_init** : array, shape (n_features, ) | None"
msgstr ""

#: :75
msgid "The initial values of the coefficients."
msgstr ""

#: :81
msgid "**params** : kwargs"
msgstr ""

#: :83
msgid "keyword arguments passed to the coordinate descent solver."
msgstr ""

#: :85
msgid "**return_n_iter** : bool"
msgstr ""

#: :87
msgid "whether to return the number of iterations or not."
msgstr ""

#: :89
msgid "**positive** : bool, default False"
msgstr ""

#: :91
msgid "If set to True, forces coefficients to be positive."
msgstr ""

#: :93
msgid "**check_input** : bool, default True"
msgstr ""

#: :95
msgid ""
"Skip input validation checks, including the Gram matrix when provided "
"assuming there are handled by the caller when check_input=False."
msgstr ""

#: :100
msgid "**alphas** : array, shape (n_alphas,)"
msgstr ""

#: :102
msgid "The alphas along the path where models are computed."
msgstr ""

#: :104
msgid ""
"**coefs** : array, shape (n_features, n_alphas) or             "
"(n_outputs, n_features, n_alphas)"
msgstr ""

#: :106
msgid "Coefficients along the path."
msgstr ""

#: :108
msgid "**dual_gaps** : array, shape (n_alphas,)"
msgstr ""

#: :110
msgid "The dual gaps at the end of the optimization for each alpha."
msgstr ""

#: :112
msgid "**n_iters** : array-like, shape (n_alphas,)"
msgstr ""

#: :114
msgid ""
"The number of iterations taken by the coordinate descent optimizer to "
"reach the specified tolerance for each alpha. (Is returned when "
"``return_n_iter`` is set to True)."
msgstr ""

#: :120
msgid ""
":obj:`MultiTaskElasticNet`, :obj:`MultiTaskElasticNetCV`, "
":obj:`ElasticNet`, :obj:`ElasticNetCV`"
msgstr ""

#: :124
msgid "See examples/plot_lasso_coordinate_descent_path.py for an example."
msgstr ""

#: :5
msgid ""
"The coefficient R^2 is defined as (1 - u/v), where u is the regression "
"sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum "
"of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is "
"1.0 and it can be negative (because the model can be arbitrarily worse). "
"A constant model that always predicts the expected value of y, "
"disregarding the input features, would get a R^2 score of 0.0."
msgstr ""

#: :15
msgid "**X** : array-like, shape = (n_samples, n_features)"
msgstr ""

#: :17
msgid "Test samples."
msgstr ""

#: :19
msgid "**y** : array-like, shape = (n_samples) or (n_samples, n_outputs)"
msgstr ""

#: :21
msgid "True values for X."
msgstr ""

#: :23
msgid "**sample_weight** : array-like, shape = [n_samples], optional"
msgstr ""

#: :25
msgid "Sample weights."
msgstr ""

#: :29
msgid "**score** : float"
msgstr ""

#: :31
msgid "R^2 of self.predict(X) wrt. y."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

