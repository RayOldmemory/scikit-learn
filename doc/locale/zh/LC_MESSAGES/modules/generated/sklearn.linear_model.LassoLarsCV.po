# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.linear_model.LassoLarsCV.rst:2
msgid ":mod:`sklearn.linear_model`.LassoLarsCV"
msgstr ""

#: :3
msgid "Cross-validated Lasso, using the LARS algorithm"
msgstr ""

#: :5
msgid "The optimization objective for Lasso is::"
msgstr ""

#: :9
msgid "Read more in the :ref:`User Guide <least_angle_regression>`."
msgstr ""

#: :13
msgid "**fit_intercept** : boolean"
msgstr ""

#: :15
msgid ""
"whether to calculate the intercept for this model. If set to false, no "
"intercept will be used in calculations (e.g. data is expected to be "
"already centered)."
msgstr ""

#: :19
msgid "**positive** : boolean (default=False)"
msgstr ""

#: :21
msgid ""
"Restrict coefficients to be >= 0. Be aware that you might want to remove "
"fit_intercept which is set True by default. Under the positive "
"restriction the model coefficients do not converge to the ordinary-least-"
"squares solution for small values of alpha. Only coeffiencts up to the "
"smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True)"
" reached by the stepwise Lars-Lasso algorithm are typically in congruence"
" with the solution of the coordinate descent Lasso estimator. As a "
"consequence using LassoLarsCV only makes sense for problems where a "
"sparse solution is expected and/or reached."
msgstr ""

#: :32
msgid "**verbose** : boolean or integer, optional"
msgstr ""

#: :34
msgid "Sets the verbosity amount"
msgstr ""

#: :36
msgid "**normalize** : boolean, optional, default False"
msgstr ""

#: :38
msgid "If True, the regressors X will be normalized before regression."
msgstr ""

#: :40
msgid "**precompute** : True | False | 'auto' | array-like"
msgstr ""

#: :42
msgid ""
"Whether to use a precomputed Gram matrix to speed up calculations. If set"
" to ``'auto'`` let us decide. The Gram matrix can also be passed as "
"argument."
msgstr ""

#: :46
msgid "**max_iter** : integer, optional"
msgstr ""

#: :48
msgid "Maximum number of iterations to perform."
msgstr ""

#: :50
msgid "**cv** : int, cross-validation generator or an iterable, optional"
msgstr ""

#: :52
msgid ""
"Determines the cross-validation splitting strategy. Possible inputs for "
"cv are:"
msgstr ""

#: :55
msgid "None, to use the default 3-fold cross-validation,"
msgstr ""

#: :56
msgid "integer, to specify the number of folds."
msgstr ""

#: :57
msgid "An object to be used as a cross-validation generator."
msgstr ""

#: :58
msgid "An iterable yielding train/test splits."
msgstr ""

#: :60
msgid "For integer/None inputs, :class:`KFold` is used."
msgstr ""

#: :62
msgid ""
"Refer :ref:`User Guide <cross_validation>` for the various cross-"
"validation strategies that can be used here."
msgstr ""

#: :65
msgid "**max_n_alphas** : integer, optional"
msgstr ""

#: :67
msgid ""
"The maximum number of points on the path used to compute the residuals in"
" the cross-validation"
msgstr ""

#: :70
msgid "**n_jobs** : integer, optional"
msgstr ""

#: :72
msgid ""
"Number of CPUs to use during the cross validation. If ``-1``, use all the"
" CPUs"
msgstr ""

#: :75
msgid "**eps** : float, optional"
msgstr ""

#: :77
msgid ""
"The machine-precision regularization in the computation of the Cholesky "
"diagonal factors. Increase this for very ill-conditioned systems."
msgstr ""

#: :81
msgid "**copy_X** : boolean, optional, default True"
msgstr ""

#: :83
msgid "If True, X will be copied; else, it may be overwritten."
msgstr ""

#: :87
msgid "**coef_** : array, shape (n_features,)"
msgstr ""

#: :89
msgid "parameter vector (w in the formulation formula)"
msgstr ""

#: :91
msgid "**intercept_** : float"
msgstr ""

#: :93
msgid "independent term in decision function."
msgstr ""

#: :95
msgid "**coef_path_** : array, shape (n_features, n_alphas)"
msgstr ""

#: :97
msgid "the varying values of the coefficients along the path"
msgstr ""

#: :99
msgid "**alpha_** : float"
msgstr ""

#: :101
msgid "the estimated regularization parameter alpha"
msgstr ""

#: :103
msgid "**alphas_** : array, shape (n_alphas,)"
msgstr ""

#: :105
msgid "the different values of alpha along the path"
msgstr ""

#: :107
msgid "**cv_alphas_** : array, shape (n_cv_alphas,)"
msgstr ""

#: :109
msgid "all the values of alpha along the path for the different folds"
msgstr ""

#: :111
msgid "**cv_mse_path_** : array, shape (n_folds, n_cv_alphas)"
msgstr ""

#: :113
msgid ""
"the mean square error on left-out for each fold along the path (alpha "
"values given by ``cv_alphas``)"
msgstr ""

#: :116
msgid "**n_iter_** : array-like or int"
msgstr ""

#: :118
msgid "the number of iterations run by Lars with the optimal alpha."
msgstr ""

#: :122
msgid ":obj:`lars_path`, :obj:`LassoLars`, :obj:`LarsCV`, :obj:`LassoCV`"
msgstr ""

#: :125
msgid "Notes"
msgstr ""

#: :126
msgid ""
"The object solves the same problem as the LassoCV object. However, unlike"
" the LassoCV, it find the relevant alphas values by itself. In general, "
"because of this property, it will be more stable. However, it is more "
"fragile to heavily multicollinear datasets."
msgstr ""

#: :131
msgid ""
"It is more efficient than the LassoCV if only a small number of features "
"are selected compared to the total number, for instance if there are very"
" few samples compared to the number of features."
msgstr ""

#: :136
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`decision_function "
"<sklearn.linear_model.LassoLarsCV.decision_function>`\\ (\\*args, "
"\\*\\*kwargs)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "DEPRECATED:  and will be removed in 0.19."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.linear_model.LassoLarsCV.fit>`\\ (X, y)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit the model using X, y as training data."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`get_params <sklearn.linear_model.LassoLarsCV.get_params>`\\ ([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.linear_model.LassoLarsCV.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict using the linear model"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`score <sklearn.linear_model.LassoLarsCV.score>`\\ (X, y[, "
"sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Returns the coefficient of determination R^2 of the prediction."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`set_params <sklearn.linear_model.LassoLarsCV.set_params>`\\ "
"(\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: :5
msgid "Decision function of the linear model."
msgstr ""

#: :9 :7
msgid "**X** : {array-like, sparse matrix}, shape = (n_samples, n_features)"
msgstr ""

#: :11 :9
msgid "Samples."
msgstr ""

#: :15 :13
msgid "**C** : array, shape = (n_samples,)"
msgstr ""

#: :17 :15
msgid "Returns predicted values."
msgstr ""

#: :7
msgid "**X** : array-like, shape (n_samples, n_features)"
msgstr ""

#: :9
msgid "Training data."
msgstr ""

#: :11
msgid "**y** : array-like, shape (n_samples,)"
msgstr ""

#: :13
msgid "Target values."
msgstr ""

#: :17
msgid "**self** : object"
msgstr ""

#: :19
msgid "returns an instance of self."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :5
msgid ""
"The coefficient R^2 is defined as (1 - u/v), where u is the regression "
"sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum "
"of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is "
"1.0 and it can be negative (because the model can be arbitrarily worse). "
"A constant model that always predicts the expected value of y, "
"disregarding the input features, would get a R^2 score of 0.0."
msgstr ""

#: :15
msgid "**X** : array-like, shape = (n_samples, n_features)"
msgstr ""

#: :17
msgid "Test samples."
msgstr ""

#: :19
msgid "**y** : array-like, shape = (n_samples) or (n_samples, n_outputs)"
msgstr ""

#: :21
msgid "True values for X."
msgstr ""

#: :23
msgid "**sample_weight** : array-like, shape = [n_samples], optional"
msgstr ""

#: :25
msgid "Sample weights."
msgstr ""

#: :29
msgid "**score** : float"
msgstr ""

#: :31
msgid "R^2 of self.predict(X) wrt. y."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LassoLarsCV.examples:3
msgid "Examples using ``sklearn.linear_model.LassoLarsCV``"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LassoLarsCV.examples:25
msgid ":ref:`example_linear_model_plot_lasso_model_selection.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LassoLarsCV.examples:45
msgid ":ref:`example_linear_model_plot_sparse_recovery.py`"
msgstr ""

