# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.rst:2
msgid ":mod:`sklearn.linear_model`.LogisticRegression"
msgstr ""

#: :3
msgid "Logistic Regression (aka logit, MaxEnt) classifier."
msgstr ""

#: :5
msgid ""
"In the multiclass case, the training algorithm uses the one-vs-rest (OvR)"
" scheme if the 'multi_class' option is set to 'ovr' and uses the cross-"
"entropy loss, if the 'multi_class' option is set to 'multinomial'. "
"(Currently the 'multinomial' option is supported only by the 'lbfgs' and "
"'newton-cg' solvers.)"
msgstr ""

#: :11
msgid ""
"This class implements regularized logistic regression using the "
"`liblinear` library, newton-cg and lbfgs solvers. It can handle both "
"dense and sparse input. Use C-ordered arrays or CSR matrices containing "
"64-bit floats for optimal performance; any other input format will be "
"converted (and copied)."
msgstr ""

#: :17
msgid ""
"The newton-cg and lbfgs solvers support only L2 regularization with "
"primal formulation. The liblinear solver supports both L1 and L2 "
"regularization, with a dual formulation only for the L2 penalty."
msgstr ""

#: :21
msgid "Read more in the :ref:`User Guide <logistic_regression>`."
msgstr ""

#: :25
msgid "**penalty** : str, 'l1' or 'l2'"
msgstr ""

#: :27
msgid ""
"Used to specify the norm used in the penalization. The newton-cg and "
"lbfgs solvers support only l2 penalties."
msgstr ""

#: :30
msgid "**dual** : bool"
msgstr ""

#: :32
msgid ""
"Dual or primal formulation. Dual formulation is only implemented for l2 "
"penalty with liblinear solver. Prefer dual=False when n_samples > "
"n_features."
msgstr ""

#: :36
msgid "**C** : float, optional (default=1.0)"
msgstr ""

#: :38
msgid ""
"Inverse of regularization strength; must be a positive float. Like in "
"support vector machines, smaller values specify stronger regularization."
msgstr ""

#: :42
msgid "**fit_intercept** : bool, default: True"
msgstr ""

#: :44
msgid ""
"Specifies if a constant (a.k.a. bias or intercept) should be added to the"
" decision function."
msgstr ""

#: :47
msgid "**intercept_scaling** : float, default: 1"
msgstr ""

#: :49
msgid ""
"Useful only if solver is liblinear. when self.fit_intercept is True, "
"instance vector x becomes [x, self.intercept_scaling], i.e. a "
"\"synthetic\" feature with constant value equals to intercept_scaling is "
"appended to the instance vector. The intercept becomes intercept_scaling "
"* synthetic feature weight Note! the synthetic feature weight is subject "
"to l1/l2 regularization as all other features. To lessen the effect of "
"regularization on synthetic feature weight (and therefore on the "
"intercept) intercept_scaling has to be increased."
msgstr ""

#: :60
msgid "**class_weight** : dict or 'balanced', optional"
msgstr ""

#: :62
msgid ""
"Weights associated with classes in the form ``{class_label: weight}``. If"
" not given, all classes are supposed to have weight one."
msgstr ""

#: :65
msgid ""
"The \"balanced\" mode uses the values of y to automatically adjust "
"weights inversely proportional to class frequencies in the input data as "
"``n_samples / (n_classes * np.bincount(y))``"
msgstr ""

#: :69
msgid ""
"Note that these weights will be multiplied with sample_weight (passed "
"through the fit method) if sample_weight is specified."
msgstr ""

#: :72
msgid "*class_weight='balanced'* instead of deprecated *class_weight='auto'*."
msgstr ""

#: :75
msgid "**max_iter** : int"
msgstr ""

#: :77
msgid ""
"Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of "
"iterations taken for the solvers to converge."
msgstr ""

#: :80
msgid "**random_state** : int seed, RandomState instance, or None (default)"
msgstr ""

#: :82
msgid ""
"The seed of the pseudo random number generator to use when shuffling the "
"data."
msgstr ""

#: :85
msgid "**solver** : {'newton-cg', 'lbfgs', 'liblinear', 'sag'}"
msgstr ""

#: :87
msgid "Algorithm to use in the optimization problem."
msgstr ""

#: :89
msgid "For small datasets, 'liblinear' is a good choice, whereas 'sag' is"
msgstr ""

#: :90
msgid "faster for large ones."
msgstr ""

#: :92
msgid "For multiclass problems, only 'newton-cg' and 'lbfgs' handle"
msgstr ""

#: :92
msgid ""
"multinomial loss; 'sag' and 'liblinear' are limited to one-versus-rest "
"schemes."
msgstr ""

#: :94
msgid "'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty."
msgstr ""

#: :96
msgid ""
"Note that 'sag' fast convergence is only guaranteed on features with "
"approximately the same scale. You can preprocess the data with a scaler "
"from sklearn.preprocessing."
msgstr ""

#: :100
msgid "Stochastic Average Gradient descent solver."
msgstr ""

#: :103
msgid "**tol** : float, optional"
msgstr ""

#: :105
msgid "Tolerance for stopping criteria."
msgstr ""

#: :107
msgid "**multi_class** : str, {'ovr', 'multinomial'}"
msgstr ""

#: :109
msgid ""
"Multiclass option can be either 'ovr' or 'multinomial'. If the option "
"chosen is 'ovr', then a binary problem is fit for each label. Else the "
"loss minimised is the multinomial loss fit across the entire probability "
"distribution. Works only for the 'lbfgs' solver."
msgstr ""

#: :115
msgid "**verbose** : int"
msgstr ""

#: :117
msgid ""
"For the liblinear and lbfgs solvers set verbose to any positive number "
"for verbosity."
msgstr ""

#: :120
msgid "**warm_start** : bool, optional"
msgstr ""

#: :122
msgid ""
"When set to True, reuse the solution of the previous call to fit as "
"initialization, otherwise, just erase the previous solution. Useless for "
"liblinear solver."
msgstr ""

#: :126
msgid "*warm_start* to support *lbfgs*, *newton-cg*, *sag* solvers."
msgstr ""

#: :129
msgid "**n_jobs** : int, optional"
msgstr ""

#: :131
msgid ""
"Number of CPU cores used during the cross-validation loop. If given a "
"value of -1, all cores are used."
msgstr ""

#: :136
msgid "**coef_** : array, shape (n_classes, n_features)"
msgstr ""

#: :138
msgid "Coefficient of the features in the decision function."
msgstr ""

#: :140
msgid "**intercept_** : array, shape (n_classes,)"
msgstr ""

#: :142
msgid ""
"Intercept (a.k.a. bias) added to the decision function. If "
"`fit_intercept` is set to False, the intercept is set to zero."
msgstr ""

#: :145
msgid "**n_iter_** : array, shape (n_classes,) or (1, )"
msgstr ""

#: :147
msgid ""
"Actual number of iterations for all classes. If binary or multinomial, it"
" returns only 1 element. For liblinear solver, only the maximum number of"
" iteration across all classes is given."
msgstr ""

#: :154
msgid ":obj:`SGDClassifier`"
msgstr ""

#: :154
msgid ""
"incrementally trained logistic regression (when given the parameter "
"``loss=\"log\"``)."
msgstr ""

#: :156
msgid ":obj:`sklearn.svm.LinearSVC`"
msgstr ""

#: :157
msgid "learns SVM models using the same algorithm."
msgstr ""

#: :160 :16
msgid "Notes"
msgstr ""

#: :161
msgid ""
"The underlying C implementation uses a random number generator to select "
"features when fitting the model. It is thus not uncommon, to have "
"slightly different results for the same input data. If that happens, try "
"with a smaller tol parameter."
msgstr ""

#: :166
msgid ""
"Predict output may not match that of standalone liblinear in certain "
"cases. See :ref:`differences from liblinear <liblinear_differences>` in "
"the narrative documentation."
msgstr ""

#: :171
msgid "References"
msgstr ""

#: :173
msgid "LIBLINEAR -- A Library for Large Linear Classification"
msgstr ""

#: :173
msgid "http://www.csie.ntu.edu.tw/~cjlin/liblinear/"
msgstr ""

#: :178
msgid "Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent"
msgstr ""

#: :176
msgid ""
"methods for logistic regression and maximum entropy models. Machine "
"Learning 85(1-2):41-75. "
"http://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf"
msgstr ""

#: :183
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`decision_function "
"<sklearn.linear_model.LogisticRegression.decision_function>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict confidence scores for samples."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`densify <sklearn.linear_model.LogisticRegression.densify>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Convert coefficient matrix to dense array format."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit <sklearn.linear_model.LogisticRegression.fit>`\\ (X, y[, "
"sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit the model according to the given training data."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fit_transform "
"<sklearn.linear_model.LogisticRegression.fit_transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit to data, then transform it."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`get_params <sklearn.linear_model.LogisticRegression.get_params>`\\ "
"([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.linear_model.LogisticRegression.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict class labels for samples in X."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`predict_log_proba "
"<sklearn.linear_model.LogisticRegression.predict_log_proba>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Log of probability estimates."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`predict_proba "
"<sklearn.linear_model.LogisticRegression.predict_proba>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Probability estimates."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`score <sklearn.linear_model.LogisticRegression.score>`\\ (X, y[, "
"sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Returns the mean accuracy on the given test data and labels."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`set_params <sklearn.linear_model.LogisticRegression.set_params>`\\ "
"(\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`sparsify <sklearn.linear_model.LogisticRegression.sparsify>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Convert coefficient matrix to sparse format."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`transform <sklearn.linear_model.LogisticRegression.transform>`\\ "
"(\\*args, \\*\\*kwargs)"
msgstr ""

#: ../../<autosummary>:1
msgid ""
"DEPRECATED: Support to use estimators as feature selectors will be "
"removed in version 0.19."
msgstr ""

#: :5
msgid ""
"The confidence score for a sample is the signed distance of that sample "
"to the hyperplane."
msgstr ""

#: :10
msgid "**X** : {array-like, sparse matrix}, shape = (n_samples, n_features)"
msgstr ""

#: :12 :9
msgid "Samples."
msgstr ""

#: :16
msgid ""
"**array, shape=(n_samples,) if n_classes == 2 else (n_samples, "
"n_classes)** :"
msgstr ""

#: :18
msgid ""
"Confidence scores per (sample, class) combination. In the binary case, "
"confidence score for self.classes_[1] where >0 means this class would be "
"predicted."
msgstr ""

#: :5
msgid ""
"Converts the ``coef_`` member (back) to a numpy.ndarray. This is the "
"default format of ``coef_`` and is required for fitting, so calling this "
"method is only required on models that have previously been sparsified; "
"otherwise, it is a no-op."
msgstr ""

#: :12 :13
msgid "**self: estimator** :"
msgstr ""

#: :7
msgid "**X** : {array-like, sparse matrix}, shape (n_samples, n_features)"
msgstr ""

#: :9
msgid ""
"Training vector, where n_samples in the number of samples and n_features "
"is the number of features."
msgstr ""

#: :12
msgid "**y** : array-like, shape (n_samples,)"
msgstr ""

#: :14
msgid "Target vector relative to X."
msgstr ""

#: :16
msgid "**sample_weight** : array-like, shape (n_samples,) optional"
msgstr ""

#: :18
msgid ""
"Array of weights that are assigned to individual samples. If not "
"provided, then each sample is given unit weight."
msgstr ""

#: :21
msgid "*sample_weight* support to LogisticRegression."
msgstr ""

#: :26
msgid "**self** : object"
msgstr ""

#: :28
msgid "Returns self."
msgstr ""

#: :5
msgid ""
"Fits transformer to X and y with optional parameters fit_params and "
"returns a transformed version of X."
msgstr ""

#: :10
msgid "**X** : numpy array of shape [n_samples, n_features]"
msgstr ""

#: :12
msgid "Training set."
msgstr ""

#: :14
msgid "**y** : numpy array of shape [n_samples]"
msgstr ""

#: :16
msgid "Target values."
msgstr ""

#: :20
msgid "**X_new** : numpy array of shape [n_samples, n_features_new]"
msgstr ""

#: :22
msgid "Transformed array."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :7
msgid "**X** : {array-like, sparse matrix}, shape = [n_samples, n_features]"
msgstr ""

#: :13
msgid "**C** : array, shape = [n_samples]"
msgstr ""

#: :15
msgid "Predicted class label per sample."
msgstr ""

#: :5
msgid ""
"The returned estimates for all classes are ordered by the label of "
"classes."
msgstr ""

#: :10 :17
msgid "**X** : array-like, shape = [n_samples, n_features]"
msgstr ""

#: :14 :21
msgid "**T** : array-like, shape = [n_samples, n_classes]"
msgstr ""

#: :16
msgid ""
"Returns the log-probability of the sample for each class in the model, "
"where classes are ordered as they are in ``self.classes_``."
msgstr ""

#: :8
msgid ""
"For a multi_class problem, if multi_class is set to be \"multinomial\" "
"the softmax function is used to find the predicted probability of each "
"class. Else use a one-vs-rest approach, i.e calculate the probability of "
"each class assuming it to be positive using the logistic function. and "
"normalize these values across all the classes."
msgstr ""

#: :23
msgid ""
"Returns the probability of the sample for each class in the model, where "
"classes are ordered as they are in ``self.classes_``."
msgstr ""

#: :5
msgid ""
"In multi-label classification, this is the subset accuracy which is a "
"harsh metric since you require for each sample that each label set be "
"correctly predicted."
msgstr ""

#: :11
msgid "**X** : array-like, shape = (n_samples, n_features)"
msgstr ""

#: :13
msgid "Test samples."
msgstr ""

#: :15
msgid "**y** : array-like, shape = (n_samples) or (n_samples, n_outputs)"
msgstr ""

#: :17
msgid "True labels for X."
msgstr ""

#: :19
msgid "**sample_weight** : array-like, shape = [n_samples], optional"
msgstr ""

#: :21
msgid "Sample weights."
msgstr ""

#: :25
msgid "**score** : float"
msgstr ""

#: :27
msgid "Mean accuracy of self.predict(X) wrt. y."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: :5
msgid ""
"Converts the ``coef_`` member to a scipy.sparse matrix, which for "
"L1-regularized models can be much more memory- and storage-efficient than"
" the usual numpy.ndarray representation."
msgstr ""

#: :9
msgid "The ``intercept_`` member is not converted."
msgstr ""

#: :17
#, python-format
msgid ""
"For non-sparse models, i.e. when there are not many zeros in ``coef_``, "
"this may actually *increase* memory usage, so use this method with care. "
"A rule of thumb is that the number of zero elements, which can be "
"computed with ``(coef_ == 0).sum()``, must be more than 50% for this to "
"provide significant benefits."
msgstr ""

#: :23
msgid ""
"After calling this method, further fitting with the partial_fit method "
"(if any) will not work until you call densify."
msgstr ""

#: :3
msgid ""
"DEPRECATED: Support to use estimators as feature selectors will be "
"removed in version 0.19. Use SelectFromModel instead."
msgstr ""

#: :5
msgid "Reduce X to its most important features."
msgstr ""

#: :7
msgid ""
"Uses ``coef_`` or ``feature_importances_`` to determine the most "
"important features.  For models with a ``coef_`` for each class, the "
"absolute sum over the classes is used."
msgstr ""

#: :13
msgid "**X** : array or scipy sparse matrix of shape [n_samples, n_features]"
msgstr ""

#: :15
msgid "The input samples."
msgstr ""

#: :24
msgid "threshold"
msgstr ""

#: :23
msgid "string, float or None, optional (default=None)"
msgstr ""

#: :18
msgid ""
"The threshold value to use for feature selection. Features whose "
"importance is greater or equal are kept while the others are discarded. "
"If \"median\" (resp. \"mean\"), then the threshold value is the median "
"(resp. the mean) of the feature importances. A scaling factor (e.g., "
"\"1.25*mean\") may also be used. If None and if available, the object "
"attribute ``threshold`` is used. Otherwise, \"mean\" is used by default."
msgstr ""

#: :28
msgid "**X_r** : array of shape [n_samples, n_selected_features]"
msgstr ""

#: :30
msgid "The input samples with only the selected features."
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:3
msgid "Examples using ``sklearn.linear_model.LogisticRegression``"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:25
msgid ":ref:`example_plot_digits_pipe.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:45
msgid ":ref:`example_calibration_plot_compare_calibration.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:65
msgid ":ref:`example_calibration_plot_calibration_curve.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:85
msgid ":ref:`example_classification_plot_classification_probability.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:105
msgid ":ref:`example_ensemble_plot_voting_probas.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:125
msgid ":ref:`example_ensemble_plot_feature_transformation.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:145
msgid ":ref:`example_exercises_digits_classification_exercise.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:165
msgid ":ref:`example_linear_model_plot_iris_logistic.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:185
msgid ":ref:`example_linear_model_plot_logistic_path.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:205
msgid ":ref:`example_linear_model_plot_sgd_comparison.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:225
msgid ":ref:`example_linear_model_plot_logistic.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:245
msgid ":ref:`example_linear_model_plot_logistic_l1_l2_sparsity.py`"
msgstr ""

#: ../../modules/generated/sklearn.linear_model.LogisticRegression.examples:265
msgid ":ref:`example_neural_networks_plot_rbm_logistic_classification.py`"
msgstr ""

