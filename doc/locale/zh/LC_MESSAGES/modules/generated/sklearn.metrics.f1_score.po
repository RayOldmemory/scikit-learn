# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.metrics.f1_score.rst:2
msgid ":mod:`sklearn.metrics`.f1_score"
msgstr ""

#: :3
msgid "Compute the F1 score, also known as balanced F-score or F-measure"
msgstr ""

#: :5
msgid ""
"The F1 score can be interpreted as a weighted average of the precision "
"and recall, where an F1 score reaches its best value at 1 and worst score"
" at 0. The relative contribution of precision and recall to the F1 score "
"are equal. The formula for the F1 score is::"
msgstr ""

#: :12
msgid ""
"In the multi-class and multi-label case, this is the weighted average of "
"the F1 score of each class."
msgstr ""

#: :15
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: :19
msgid "**y_true** : 1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: :21
msgid "Ground truth (correct) target values."
msgstr ""

#: :23
msgid "**y_pred** : 1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: :25
msgid "Estimated targets as returned by a classifier."
msgstr ""

#: :27
msgid "**labels** : list, optional"
msgstr ""

#: :29
msgid ""
"The set of labels to include when ``average != 'binary'``, and their "
"order if ``average is None``. Labels present in the data can be excluded,"
" for example to calculate a multiclass average ignoring a majority "
"negative class, while labels not present in the data will result in 0 "
"components in a macro average. For multilabel targets, labels are column "
"indices. By default, all labels in ``y_true`` and ``y_pred`` are used in "
"sorted order."
msgstr ""

#: :37
msgid "parameter *labels* improved for multiclass problem."
msgstr ""

#: :40
msgid "**pos_label** : str or int, 1 by default"
msgstr ""

#: :42
msgid ""
"The class to report if ``average='binary'``. Until version 0.18 it is "
"necessary to set ``pos_label=None`` if seeking to use another averaging "
"method over binary targets."
msgstr ""

#: :46
msgid ""
"**average** : string, [None, 'binary' (default), 'micro', 'macro', "
"'samples',                        'weighted']"
msgstr ""

#: :48
msgid ""
"This parameter is required for multiclass/multilabel targets. If "
"``None``, the scores for each class are returned. Otherwise, this "
"determines the type of averaging performed on the data:"
msgstr ""

#: :53
msgid "``'binary'``:"
msgstr ""

#: :53
msgid ""
"Only report results for the class specified by ``pos_label``. This is "
"applicable only if targets (``y_{true,pred}``) are binary."
msgstr ""

#: :56
msgid "``'micro'``:"
msgstr ""

#: :56
msgid ""
"Calculate metrics globally by counting the total true positives, false "
"negatives and false positives."
msgstr ""

#: :59
msgid "``'macro'``:"
msgstr ""

#: :59
msgid ""
"Calculate metrics for each label, and find their unweighted mean.  This "
"does not take label imbalance into account."
msgstr ""

#: :64
msgid "``'weighted'``:"
msgstr ""

#: :62
msgid ""
"Calculate metrics for each label, and find their average, weighted by "
"support (the number of true instances for each label). This alters "
"'macro' to account for label imbalance; it can result in an F-score that "
"is not between precision and recall."
msgstr ""

#: :69
msgid "``'samples'``:"
msgstr ""

#: :67
msgid ""
"Calculate metrics for each instance, and find their average (only "
"meaningful for multilabel classification where this differs from "
":func:`accuracy_score`)."
msgstr ""

#: :71
msgid ""
"Note that if ``pos_label`` is given in binary classification with "
"`average != 'binary'`, only that positive class is reported. This "
"behavior is deprecated and will change in version 0.18."
msgstr ""

#: :75
msgid "**sample_weight** : array-like of shape = [n_samples], optional"
msgstr ""

#: :77
msgid "Sample weights."
msgstr ""

#: :81
msgid "**f1_score** : float or array of float, shape = [n_unique_labels]"
msgstr ""

#: :83
msgid ""
"F1 score of the positive class in binary classification or weighted "
"average of the F1 scores of each class for the multiclass task."
msgstr ""

#: :87
msgid "References"
msgstr ""

#: :88
msgid ""
"`Wikipedia entry for the F1-score "
"<http://en.wikipedia.org/wiki/F1_score>`_"
msgstr ""

#: :95
msgid "Examples"
msgstr ""

#: ../../modules/generated/sklearn.metrics.f1_score.examples:3
msgid "Examples using ``sklearn.metrics.f1_score``"
msgstr ""

#: ../../modules/generated/sklearn.metrics.f1_score.examples:25
msgid ":ref:`example_calibration_plot_calibration_curve.py`"
msgstr ""

