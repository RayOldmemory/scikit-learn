# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.metrics.hamming_loss.rst:2
msgid ":mod:`sklearn.metrics`.hamming_loss"
msgstr ""

#: :3
msgid "Compute the average Hamming loss."
msgstr ""

#: :5
msgid "The Hamming loss is the fraction of labels that are incorrectly predicted."
msgstr ""

#: :7
msgid "Read more in the :ref:`User Guide <hamming_loss>`."
msgstr ""

#: :11
msgid "**y_true** : 1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: :13
msgid "Ground truth (correct) labels."
msgstr ""

#: :15
msgid "**y_pred** : 1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: :17
msgid "Predicted labels, as returned by a classifier."
msgstr ""

#: :19
msgid "**classes** : array, shape = [n_labels], optional"
msgstr ""

#: :21
msgid "Integer array of labels."
msgstr ""

#: :25
msgid "**loss** : float or int,"
msgstr ""

#: :27
msgid ""
"Return the average Hamming loss between element of ``y_true`` and "
"``y_pred``."
msgstr ""

#: :32
msgid ""
":obj:`accuracy_score`, :obj:`jaccard_similarity_score`, "
":obj:`zero_one_loss`"
msgstr ""

#: :35
msgid "Notes"
msgstr ""

#: :36
msgid ""
"In multiclass classification, the Hamming loss correspond to the Hamming "
"distance between ``y_true`` and ``y_pred`` which is equivalent to the "
"subset ``zero_one_loss`` function."
msgstr ""

#: :40
msgid ""
"In multilabel classification, the Hamming loss is different from the "
"subset zero-one loss. The zero-one loss considers the entire set of "
"labels for a given sample incorrect if it does entirely match the true "
"set of labels. Hamming loss is more forgiving in that it penalizes the "
"individual labels."
msgstr ""

#: :46
msgid ""
"The Hamming loss is upperbounded by the subset zero-one loss. When "
"normalized over samples, the Hamming loss is always between 0 and 1."
msgstr ""

#: :50
msgid "References"
msgstr ""

#: :51
msgid ""
"Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An "
"Overview. International Journal of Data Warehousing & Mining, 3(3), 1-13,"
" July-September 2007."
msgstr ""

#: :55
msgid ""
"`Wikipedia entry on the Hamming distance "
"<http://en.wikipedia.org/wiki/Hamming_distance>`_"
msgstr ""

#: :63
msgid "Examples"
msgstr ""

#: :70
msgid "In the multilabel case with binary label indicators:"
msgstr ""

#: ../../modules/generated/sklearn.metrics.hamming_loss.examples:3
msgid "Examples using ``sklearn.metrics.hamming_loss``"
msgstr ""

#: ../../modules/generated/sklearn.metrics.hamming_loss.examples:25
msgid ":ref:`example_applications_plot_model_complexity_influence.py`"
msgstr ""

