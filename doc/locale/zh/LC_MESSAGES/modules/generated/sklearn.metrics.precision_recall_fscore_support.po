# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.metrics.precision_recall_fscore_support.rst:2
msgid ":mod:`sklearn.metrics`.precision_recall_fscore_support"
msgstr ""

#: :3
msgid "Compute precision, recall, F-measure and support for each class"
msgstr ""

#: :5
msgid ""
"The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number "
"of true positives and ``fp`` the number of false positives. The precision"
" is intuitively the ability of the classifier not to label as positive a "
"sample that is negative."
msgstr ""

#: :10
msgid ""
"The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of "
"true positives and ``fn`` the number of false negatives. The recall is "
"intuitively the ability of the classifier to find all the positive "
"samples."
msgstr ""

#: :14
msgid ""
"The F-beta score can be interpreted as a weighted harmonic mean of the "
"precision and recall, where an F-beta score reaches its best value at 1 "
"and worst score at 0."
msgstr ""

#: :18
msgid ""
"The F-beta score weights recall more than precision by a factor of "
"``beta``. ``beta == 1.0`` means recall and precision are equally "
"important."
msgstr ""

#: :21
msgid "The support is the number of occurrences of each class in ``y_true``."
msgstr ""

#: :23
msgid ""
"If ``pos_label is None`` and in binary classification, this function "
"returns the average precision, recall and F-measure if ``average`` is one"
" of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``."
msgstr ""

#: :27
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: :31
msgid "**y_true** : 1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: :33
msgid "Ground truth (correct) target values."
msgstr ""

#: :35
msgid "**y_pred** : 1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: :37
msgid "Estimated targets as returned by a classifier."
msgstr ""

#: :39
msgid "**beta** : float, 1.0 by default"
msgstr ""

#: :41
msgid "The strength of recall versus precision in the F-score."
msgstr ""

#: :43
msgid "**labels** : list, optional"
msgstr ""

#: :45
msgid ""
"The set of labels to include when ``average != 'binary'``, and their "
"order if ``average is None``. Labels present in the data can be excluded,"
" for example to calculate a multiclass average ignoring a majority "
"negative class, while labels not present in the data will result in 0 "
"components in a macro average. For multilabel targets, labels are column "
"indices. By default, all labels in ``y_true`` and ``y_pred`` are used in "
"sorted order."
msgstr ""

#: :53
msgid "**pos_label** : str or int, 1 by default"
msgstr ""

#: :55
msgid ""
"The class to report if ``average='binary'``. Until version 0.18 it is "
"necessary to set ``pos_label=None`` if seeking to use another averaging "
"method over binary targets."
msgstr ""

#: :59
msgid ""
"**average** : string, [None (default), 'binary', 'micro', 'macro', "
"'samples',                        'weighted']"
msgstr ""

#: :61
msgid ""
"If ``None``, the scores for each class are returned. Otherwise, this "
"determines the type of averaging performed on the data:"
msgstr ""

#: :65
msgid "``'binary'``:"
msgstr ""

#: :65
msgid ""
"Only report results for the class specified by ``pos_label``. This is "
"applicable only if targets (``y_{true,pred}``) are binary."
msgstr ""

#: :68
msgid "``'micro'``:"
msgstr ""

#: :68
msgid ""
"Calculate metrics globally by counting the total true positives, false "
"negatives and false positives."
msgstr ""

#: :71
msgid "``'macro'``:"
msgstr ""

#: :71
msgid ""
"Calculate metrics for each label, and find their unweighted mean.  This "
"does not take label imbalance into account."
msgstr ""

#: :76
msgid "``'weighted'``:"
msgstr ""

#: :74
msgid ""
"Calculate metrics for each label, and find their average, weighted by "
"support (the number of true instances for each label). This alters "
"'macro' to account for label imbalance; it can result in an F-score that "
"is not between precision and recall."
msgstr ""

#: :81
msgid "``'samples'``:"
msgstr ""

#: :79
msgid ""
"Calculate metrics for each instance, and find their average (only "
"meaningful for multilabel classification where this differs from "
":func:`accuracy_score`)."
msgstr ""

#: :83
msgid ""
"Note that if ``pos_label`` is given in binary classification with "
"`average != 'binary'`, only that positive class is reported. This "
"behavior is deprecated and will change in version 0.18."
msgstr ""

#: :87
msgid "**warn_for** : tuple or set, for internal use"
msgstr ""

#: :89
msgid ""
"This determines which warnings will be made in the case that this "
"function is being used to return only one of its metrics."
msgstr ""

#: :92
msgid "**sample_weight** : array-like of shape = [n_samples], optional"
msgstr ""

#: :94
msgid "Sample weights."
msgstr ""

#: :98
msgid ""
"**precision: float (if average is not None) or array of float, shape ="
"        [n_unique_labels]** :"
msgstr ""

#: :100
msgid ""
"**recall: float (if average is not None) or array of float, , shape ="
"        [n_unique_labels]** :"
msgstr ""

#: :102
msgid ""
"**fbeta_score: float (if average is not None) or array of float, shape ="
"        [n_unique_labels]** :"
msgstr ""

#: :104
msgid ""
"**support: int (if average is not None) or array of int, shape =        "
"[n_unique_labels]** :"
msgstr ""

#: :106
msgid "The number of occurrences of each label in ``y_true``."
msgstr ""

#: :109
msgid "References"
msgstr ""

#: :110
msgid ""
"`Wikipedia entry for the Precision and recall "
"<http://en.wikipedia.org/wiki/Precision_and_recall>`_"
msgstr ""

#: :113
msgid ""
"`Wikipedia entry for the F1-score "
"<http://en.wikipedia.org/wiki/F1_score>`_"
msgstr ""

#: :116
msgid ""
"`Discriminative Methods for Multi-labeled Classification Advances in "
"Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu "
"Godbole, Sunita Sarawagi <http://www.godbole.net/shantanu/pubs"
"/multilabelsvm-pakdd04.pdf>`"
msgstr ""

#: :126
msgid "Examples"
msgstr ""

#: :140
msgid ""
"It is possible to compute per-label precisions, recalls, F1-scores and "
"supports instead of averaging: >>> "
"precision_recall_fscore_support(y_true, y_pred, average=None, ... "
"labels=['pig', 'dog', 'cat']) ... # doctest: "
"+ELLIPSIS,+NORMALIZE_WHITESPACE (array([ 0. ,  0. ,  0.66...]),"
msgstr ""

#: :146
msgid "array([ 0.,  0.,  1.]), array([ 0. ,  0. ,  0.8]), array([2, 2, 2]))"
msgstr ""

