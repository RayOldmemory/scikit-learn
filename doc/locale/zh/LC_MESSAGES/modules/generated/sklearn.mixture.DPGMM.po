# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.mixture.DPGMM.rst:2
msgid ":mod:`sklearn.mixture`.DPGMM"
msgstr ""

#: :3
msgid "Variational Inference for the Infinite Gaussian Mixture Model."
msgstr ""

#: :5
msgid ""
"DPGMM stands for Dirichlet Process Gaussian Mixture Model, and it is an "
"infinite mixture model with the Dirichlet Process as a prior distribution"
" on the number of clusters. In practice the approximate inference "
"algorithm uses a truncated distribution with a fixed maximum number of "
"components, but almost always the number of components actually used "
"depends on the data."
msgstr ""

#: :12
msgid ""
"Stick-breaking Representation of a Gaussian mixture model probability "
"distribution. This class allows for easy and efficient inference of an "
"approximate posterior distribution over the parameters of a Gaussian "
"mixture model with a variable number of components (smaller than the "
"truncation parameter n_components)."
msgstr ""

#: :18
msgid ""
"Initialization is with normally-distributed means and identity "
"covariance, for proper convergence."
msgstr ""

#: :21
msgid "Read more in the :ref:`User Guide <dpgmm>`."
msgstr ""

#: :25
msgid "**n_components: int, default 1** :"
msgstr ""

#: :27 :76
msgid "Number of mixture components."
msgstr ""

#: :29
msgid "**covariance_type: string, default 'diag'** :"
msgstr ""

#: :31
msgid ""
"String describing the type of covariance parameters to use.  Must be one "
"of 'spherical', 'tied', 'diag', 'full'."
msgstr ""

#: :34
msgid "**alpha: float, default 1** :"
msgstr ""

#: :36
msgid ""
"Real number representing the concentration parameter of the dirichlet "
"process. Intuitively, the Dirichlet Process is as likely to start a new "
"cluster for a point as it is to add that point to a cluster with alpha "
"elements. A higher alpha means more clusters, as the expected number of "
"clusters is ``alpha*log(N)``."
msgstr ""

#: :43
msgid "**tol** : float, default 1e-3"
msgstr ""

#: :45
msgid "Convergence threshold."
msgstr ""

#: :47
msgid "**n_iter** : int, default 10"
msgstr ""

#: :49
msgid "Maximum number of iterations to perform before convergence."
msgstr ""

#: :51
msgid "**params** : string, default 'wmc'"
msgstr ""

#: :53
msgid ""
"Controls which parameters are updated in the training process.  Can "
"contain any combination of 'w' for weights, 'm' for means, and 'c' for "
"covars."
msgstr ""

#: :57
msgid "**init_params** : string, default 'wmc'"
msgstr ""

#: :59
msgid ""
"Controls which parameters are updated in the initialization process.  Can"
" contain any combination of 'w' for weights, 'm' for means, and 'c' for "
"covars.  Defaults to 'wmc'."
msgstr ""

#: :63
msgid "**verbose** : int, default 0"
msgstr ""

#: :65
msgid "Controls output verbosity."
msgstr ""

#: :69
msgid "**covariance_type** : string"
msgstr ""

#: :71
msgid ""
"String describing the type of covariance parameters used by the DP-GMM.  "
"Must be one of 'spherical', 'tied', 'diag', 'full'."
msgstr ""

#: :74
msgid "**n_components** : int"
msgstr ""

#: :78
msgid "**weights_** : array, shape (`n_components`,)"
msgstr ""

#: :80
msgid "Mixing weights for each mixture component."
msgstr ""

#: :82
msgid "**means_** : array, shape (`n_components`, `n_features`)"
msgstr ""

#: :84
msgid "Mean parameters for each mixture component."
msgstr ""

#: :86
msgid "**precs_** : array"
msgstr ""

#: :88
msgid ""
"Precision (inverse covariance) parameters for each mixture component.  "
"The shape depends on `covariance_type`::"
msgstr ""

#: :96
msgid "**converged_** : bool"
msgstr ""

#: :98
msgid "True when convergence was reached in fit(), False otherwise."
msgstr ""

#: :103
msgid ":obj:`GMM`"
msgstr ""

#: :103
msgid "Finite Gaussian mixture model fit with EM"
msgstr ""

#: :105
msgid ":obj:`VBGMM`"
msgstr ""

#: :106
msgid ""
"Finite Gaussian mixture model fit with a variational algorithm, better "
"for situations where there might be too little data to get a good "
"estimate of the covariance matrix."
msgstr ""

#: :109
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`aic <sklearn.mixture.DPGMM.aic>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1
msgid "Akaike information criterion for the current model fit"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`bic <sklearn.mixture.DPGMM.bic>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1
msgid "Bayesian information criterion for the current model fit"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.mixture.DPGMM.fit>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Estimate model parameters with the EM algorithm."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit_predict <sklearn.mixture.DPGMM.fit_predict>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit and then predict labels for data."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`get_params <sklearn.mixture.DPGMM.get_params>`\\ ([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`lower_bound <sklearn.mixture.DPGMM.lower_bound>`\\ (X, z)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "returns a lower bound on model evidence based on X and membership"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.mixture.DPGMM.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict label for data."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict_proba <sklearn.mixture.DPGMM.predict_proba>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict posterior probability of data under each Gaussian in the model."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`sample <sklearn.mixture.DPGMM.sample>`\\ ([n_samples, random_state])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Generate random samples from the model."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`score <sklearn.mixture.DPGMM.score>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Compute the log probability under the model."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`score_samples <sklearn.mixture.DPGMM.score_samples>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Return the likelihood of the data under the model."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`set_params <sklearn.mixture.DPGMM.set_params>`\\ (\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: :3
msgid ""
"Akaike information criterion for the current model fit and the proposed "
"data"
msgstr ""

#: :8
msgid "**X** : array of shape(n_samples, n_dimensions)"
msgstr ""

#: :12
msgid "**aic: float (the lower the better)** :"
msgstr ""

#: :3
msgid ""
"Bayesian information criterion for the current model fit and the proposed"
" data"
msgstr ""

#: :12
msgid "**bic: float (the lower the better)** :"
msgstr ""

#: :5
msgid ""
"A initialization step is performed before entering the expectation-"
"maximization (EM) algorithm. If you want to avoid this step, set the "
"keyword argument init_params to the empty string '' when creating the GMM"
" object. Likewise, if you would like just to do an initialization, set "
"n_iter=0."
msgstr ""

#: :13
msgid "**X** : array_like, shape (n, n_features)"
msgstr ""

#: :15 :9 :16
msgid ""
"List of n_features-dimensional data points.  Each row corresponds to a "
"single data point."
msgstr ""

#: :20 :12
msgid "**self** :"
msgstr ""

#: :5
msgid ""
"Warning: due to the final maximization step in the EM algorithm, with low"
" iterations the prediction may not be 100% accurate"
msgstr ""

#: :8
msgid "*fit_predict* method in Gaussian Mixture Model."
msgstr ""

#: :13 :7 :8
msgid "**X** : array-like, shape = [n_samples, n_features]"
msgstr ""

#: :17 :11
msgid "**C** : array, shape = (n_samples,) component memberships"
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :12
msgid "**responsibilities** : array-like, shape = (n_samples, n_components)"
msgstr ""

#: :14
msgid ""
"Returns the probability of the sample for each Gaussian (state) in the "
"model."
msgstr ""

#: :7
msgid "**n_samples** : int, optional"
msgstr ""

#: :9
msgid "Number of samples to generate. Defaults to 1."
msgstr ""

#: :13 :7 :14
msgid "**X** : array_like, shape (n_samples, n_features)"
msgstr ""

#: :15
msgid "List of samples"
msgstr ""

#: :14 :21
msgid "**logprob** : array_like, shape (n_samples,)"
msgstr ""

#: :16 :23
msgid "Log probabilities of each data point in X"
msgstr ""

#: :5
msgid ""
"Compute the bound on log probability of X under the model and return the "
"posterior distribution (responsibilities) of each mixture component for "
"each element of X."
msgstr ""

#: :9
msgid ""
"This is done by computing the parameters for the mean-field of z for each"
" observation."
msgstr ""

#: :25
msgid "**responsibilities: array_like, shape (n_samples, n_components)** :"
msgstr ""

#: :27
msgid "Posterior probabilities of each mixture component for each observation"
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: ../../modules/generated/sklearn.mixture.DPGMM.examples:3
msgid "Examples using ``sklearn.mixture.DPGMM``"
msgstr ""

#: ../../modules/generated/sklearn.mixture.DPGMM.examples:25
msgid ":ref:`example_mixture_plot_gmm.py`"
msgstr ""

#: ../../modules/generated/sklearn.mixture.DPGMM.examples:45
msgid ":ref:`example_mixture_plot_gmm_sin.py`"
msgstr ""

