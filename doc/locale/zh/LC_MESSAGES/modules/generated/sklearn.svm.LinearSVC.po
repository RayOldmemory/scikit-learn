# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.svm.LinearSVC.rst:2
msgid ":mod:`sklearn.svm`.LinearSVC"
msgstr ""

#: :3
msgid "Linear Support Vector Classification."
msgstr ""

#: :5
msgid ""
"Similar to SVC with parameter kernel='linear', but implemented in terms "
"of liblinear rather than libsvm, so it has more flexibility in the choice"
" of penalties and loss functions and should scale better to large numbers"
" of samples."
msgstr ""

#: :10
msgid ""
"This class supports both dense and sparse input and the multiclass "
"support is handled according to a one-vs-the-rest scheme."
msgstr ""

#: :13
msgid "Read more in the :ref:`User Guide <svm_classification>`."
msgstr ""

#: :17
msgid "**C** : float, optional (default=1.0)"
msgstr ""

#: :19
msgid "Penalty parameter C of the error term."
msgstr ""

#: :21
msgid "**loss** : string, 'hinge' or 'squared_hinge' (default='squared_hinge')"
msgstr ""

#: :23
msgid ""
"Specifies the loss function. 'hinge' is the standard SVM loss (used e.g. "
"by the SVC class) while 'squared_hinge' is the square of the hinge loss."
msgstr ""

#: :27
msgid "**penalty** : string, 'l1' or 'l2' (default='l2')"
msgstr ""

#: :29
msgid ""
"Specifies the norm used in the penalization. The 'l2' penalty is the "
"standard used in SVC. The 'l1' leads to ``coef_`` vectors that are "
"sparse."
msgstr ""

#: :33
msgid "**dual** : bool, (default=True)"
msgstr ""

#: :35
msgid ""
"Select the algorithm to either solve the dual or primal optimization "
"problem. Prefer dual=False when n_samples > n_features."
msgstr ""

#: :38
msgid "**tol** : float, optional (default=1e-4)"
msgstr ""

#: :40
msgid "Tolerance for stopping criteria."
msgstr ""

#: :42
msgid "**multi_class: string, 'ovr' or 'crammer_singer' (default='ovr')** :"
msgstr ""

#: :44
msgid ""
"Determines the multi-class strategy if `y` contains more than two "
"classes. ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while "
"``\"crammer_singer\"`` optimizes a joint objective over all classes. "
"While `crammer_singer` is interesting from a theoretical perspective as "
"it is consistent, it is seldom used in practice as it rarely leads to "
"better accuracy and is more expensive to compute. If "
"``\"crammer_singer\"`` is chosen, the options loss, penalty and dual will"
" be ignored."
msgstr ""

#: :54
msgid "**fit_intercept** : boolean, optional (default=True)"
msgstr ""

#: :56
msgid ""
"Whether to calculate the intercept for this model. If set to false, no "
"intercept will be used in calculations (i.e. data is expected to be "
"already centered)."
msgstr ""

#: :60
msgid "**intercept_scaling** : float, optional (default=1)"
msgstr ""

#: :62
msgid ""
"When self.fit_intercept is True, instance vector x becomes ``[x, "
"self.intercept_scaling]``, i.e. a \"synthetic\" feature with constant "
"value equals to intercept_scaling is appended to the instance vector. The"
" intercept becomes intercept_scaling * synthetic feature weight Note! the"
" synthetic feature weight is subject to l1/l2 regularization as all other"
" features. To lessen the effect of regularization on synthetic feature "
"weight (and therefore on the intercept) intercept_scaling has to be "
"increased."
msgstr ""

#: :72
msgid "**class_weight** : {dict, 'balanced'}, optional"
msgstr ""

#: :74
msgid ""
"Set the parameter C of class i to ``class_weight[i]*C`` for SVC. If not "
"given, all classes are supposed to have weight one. The \"balanced\" mode"
" uses the values of y to automatically adjust weights inversely "
"proportional to class frequencies in the input data as ``n_samples / "
"(n_classes * np.bincount(y))``"
msgstr ""

#: :81
msgid "**verbose** : int, (default=0)"
msgstr ""

#: :83
msgid ""
"Enable verbose output. Note that this setting takes advantage of a per-"
"process runtime setting in liblinear that, if enabled, may not work "
"properly in a multithreaded context."
msgstr ""

#: :87
msgid "**random_state** : int seed, RandomState instance, or None (default=None)"
msgstr ""

#: :89
msgid ""
"The seed of the pseudo random number generator to use when shuffling the "
"data."
msgstr ""

#: :92
msgid "**max_iter** : int, (default=1000)"
msgstr ""

#: :94
msgid "The maximum number of iterations to be run."
msgstr ""

#: :98
msgid ""
"**coef_** : array, shape = [n_features] if n_classes == 2 else "
"[n_classes, n_features]"
msgstr ""

#: :100
msgid ""
"Weights assigned to the features (coefficients in the primal problem). "
"This is only available in the case of a linear kernel."
msgstr ""

#: :103
msgid ""
"``coef_`` is a readonly property derived from ``raw_coef_`` that follows "
"the internal memory layout of liblinear."
msgstr ""

#: :106
msgid "**intercept_** : array, shape = [1] if n_classes == 2 else [n_classes]"
msgstr ""

#: :108
msgid "Constants in decision function."
msgstr ""

#: :113
msgid ":obj:`SVC`"
msgstr ""

#: :113
msgid ""
"Implementation of Support Vector Machine classifier using libsvm: the "
"kernel can be non-linear but its SMO algorithm does not scale to large "
"number of samples as LinearSVC does. Furthermore SVC multi-class mode is "
"implemented using one vs one scheme while LinearSVC uses one vs the rest."
" It is possible to implement one vs the rest with SVC by using the "
":class:`sklearn.multiclass.OneVsRestClassifier` wrapper. Finally SVC can "
"fit dense data without memory copy if the input is C-contiguous. Sparse "
"data will still incur memory copy though."
msgstr ""

#: :115
msgid ":obj:`sklearn.linear_model.SGDClassifier`"
msgstr ""

#: :116
msgid ""
"SGDClassifier can optimize the same cost function as LinearSVC by "
"adjusting the penalty and loss parameters. In addition it requires less "
"memory, allows incremental (online) learning, and implements various loss"
" functions and regularization regimes."
msgstr ""

#: :119 :16
msgid "Notes"
msgstr ""

#: :120
msgid ""
"The underlying C implementation uses a random number generator to select "
"features when fitting the model. It is thus not uncommon to have slightly"
" different results for the same input data. If that happens, try with a "
"smaller ``tol`` parameter."
msgstr ""

#: :125
msgid ""
"The underlying implementation, liblinear, uses a sparse internal "
"representation for the data that will incur a memory copy."
msgstr ""

#: :128
msgid ""
"Predict output may not match that of standalone liblinear in certain "
"cases. See :ref:`differences from liblinear <liblinear_differences>` in "
"the narrative documentation."
msgstr ""

#: :133
msgid "References"
msgstr ""

#: :134
msgid ""
"`LIBLINEAR: A Library for Large Linear Classification "
"<http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__"
msgstr ""

#: :140
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`decision_function <sklearn.svm.LinearSVC.decision_function>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict confidence scores for samples."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`densify <sklearn.svm.LinearSVC.densify>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Convert coefficient matrix to dense array format."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.svm.LinearSVC.fit>`\\ (X, y)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit the model according to the given training data."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit_transform <sklearn.svm.LinearSVC.fit_transform>`\\ (X[, y])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit to data, then transform it."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`get_params <sklearn.svm.LinearSVC.get_params>`\\ ([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.svm.LinearSVC.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict class labels for samples in X."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`score <sklearn.svm.LinearSVC.score>`\\ (X, y[, sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Returns the mean accuracy on the given test data and labels."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`set_params <sklearn.svm.LinearSVC.set_params>`\\ (\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`sparsify <sklearn.svm.LinearSVC.sparsify>`\\ ()"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Convert coefficient matrix to sparse format."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`transform <sklearn.svm.LinearSVC.transform>`\\ (\\*args, "
"\\*\\*kwargs)"
msgstr ""

#: ../../<autosummary>:1
msgid ""
"DEPRECATED: Support to use estimators as feature selectors will be "
"removed in version 0.19."
msgstr ""

#: :5
msgid ""
"The confidence score for a sample is the signed distance of that sample "
"to the hyperplane."
msgstr ""

#: :10
msgid "**X** : {array-like, sparse matrix}, shape = (n_samples, n_features)"
msgstr ""

#: :12 :9
msgid "Samples."
msgstr ""

#: :16
msgid ""
"**array, shape=(n_samples,) if n_classes == 2 else (n_samples, "
"n_classes)** :"
msgstr ""

#: :18
msgid ""
"Confidence scores per (sample, class) combination. In the binary case, "
"confidence score for self.classes_[1] where >0 means this class would be "
"predicted."
msgstr ""

#: :5
msgid ""
"Converts the ``coef_`` member (back) to a numpy.ndarray. This is the "
"default format of ``coef_`` and is required for fitting, so calling this "
"method is only required on models that have previously been sparsified; "
"otherwise, it is a no-op."
msgstr ""

#: :12 :13
msgid "**self: estimator** :"
msgstr ""

#: :7
msgid "**X** : {array-like, sparse matrix}, shape = [n_samples, n_features]"
msgstr ""

#: :9
msgid ""
"Training vector, where n_samples in the number of samples and n_features "
"is the number of features."
msgstr ""

#: :12
msgid "**y** : array-like, shape = [n_samples]"
msgstr ""

#: :14
msgid "Target vector relative to X"
msgstr ""

#: :18
msgid "**self** : object"
msgstr ""

#: :20
msgid "Returns self."
msgstr ""

#: :5
msgid ""
"Fits transformer to X and y with optional parameters fit_params and "
"returns a transformed version of X."
msgstr ""

#: :10
msgid "**X** : numpy array of shape [n_samples, n_features]"
msgstr ""

#: :12
msgid "Training set."
msgstr ""

#: :14
msgid "**y** : numpy array of shape [n_samples]"
msgstr ""

#: :16
msgid "Target values."
msgstr ""

#: :20
msgid "**X_new** : numpy array of shape [n_samples, n_features_new]"
msgstr ""

#: :22
msgid "Transformed array."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :13
msgid "**C** : array, shape = [n_samples]"
msgstr ""

#: :15
msgid "Predicted class label per sample."
msgstr ""

#: :5
msgid ""
"In multi-label classification, this is the subset accuracy which is a "
"harsh metric since you require for each sample that each label set be "
"correctly predicted."
msgstr ""

#: :11
msgid "**X** : array-like, shape = (n_samples, n_features)"
msgstr ""

#: :13
msgid "Test samples."
msgstr ""

#: :15
msgid "**y** : array-like, shape = (n_samples) or (n_samples, n_outputs)"
msgstr ""

#: :17
msgid "True labels for X."
msgstr ""

#: :19
msgid "**sample_weight** : array-like, shape = [n_samples], optional"
msgstr ""

#: :21
msgid "Sample weights."
msgstr ""

#: :25
msgid "**score** : float"
msgstr ""

#: :27
msgid "Mean accuracy of self.predict(X) wrt. y."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

#: :5
msgid ""
"Converts the ``coef_`` member to a scipy.sparse matrix, which for "
"L1-regularized models can be much more memory- and storage-efficient than"
" the usual numpy.ndarray representation."
msgstr ""

#: :9
msgid "The ``intercept_`` member is not converted."
msgstr ""

#: :17
#, python-format
msgid ""
"For non-sparse models, i.e. when there are not many zeros in ``coef_``, "
"this may actually *increase* memory usage, so use this method with care. "
"A rule of thumb is that the number of zero elements, which can be "
"computed with ``(coef_ == 0).sum()``, must be more than 50% for this to "
"provide significant benefits."
msgstr ""

#: :23
msgid ""
"After calling this method, further fitting with the partial_fit method "
"(if any) will not work until you call densify."
msgstr ""

#: :3
msgid ""
"DEPRECATED: Support to use estimators as feature selectors will be "
"removed in version 0.19. Use SelectFromModel instead."
msgstr ""

#: :5
msgid "Reduce X to its most important features."
msgstr ""

#: :7
msgid ""
"Uses ``coef_`` or ``feature_importances_`` to determine the most "
"important features.  For models with a ``coef_`` for each class, the "
"absolute sum over the classes is used."
msgstr ""

#: :13
msgid "**X** : array or scipy sparse matrix of shape [n_samples, n_features]"
msgstr ""

#: :15
msgid "The input samples."
msgstr ""

#: :24
msgid "threshold"
msgstr ""

#: :23
msgid "string, float or None, optional (default=None)"
msgstr ""

#: :18
msgid ""
"The threshold value to use for feature selection. Features whose "
"importance is greater or equal are kept while the others are discarded. "
"If \"median\" (resp. \"mean\"), then the threshold value is the median "
"(resp. the mean) of the feature importances. A scaling factor (e.g., "
"\"1.25*mean\") may also be used. If None and if available, the object "
"attribute ``threshold`` is used. Otherwise, \"mean\" is used by default."
msgstr ""

#: :28
msgid "**X_r** : array of shape [n_samples, n_selected_features]"
msgstr ""

#: :30
msgid "The input samples with only the selected features."
msgstr ""

#: ../../modules/generated/sklearn.svm.LinearSVC.examples:3
msgid "Examples using ``sklearn.svm.LinearSVC``"
msgstr ""

#: ../../modules/generated/sklearn.svm.LinearSVC.examples:25
msgid ":ref:`example_plot_kernel_approximation.py`"
msgstr ""

#: ../../modules/generated/sklearn.svm.LinearSVC.examples:45
msgid ":ref:`example_calibration_plot_compare_calibration.py`"
msgstr ""

#: ../../modules/generated/sklearn.svm.LinearSVC.examples:65
msgid ":ref:`example_calibration_plot_calibration_curve.py`"
msgstr ""

#: ../../modules/generated/sklearn.svm.LinearSVC.examples:85
msgid ":ref:`example_svm_plot_iris.py`"
msgstr ""

#: ../../modules/generated/sklearn.svm.LinearSVC.examples:105
msgid ":ref:`example_svm_plot_svm_scale_c.py`"
msgstr ""

#: ../../modules/generated/sklearn.svm.LinearSVC.examples:125
msgid ":ref:`example_text_document_classification_20newsgroups.py`"
msgstr ""

