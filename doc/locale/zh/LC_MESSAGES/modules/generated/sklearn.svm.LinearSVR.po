# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/generated/sklearn.svm.LinearSVR.rst:2
msgid ":mod:`sklearn.svm`.LinearSVR"
msgstr ""

#: :3
msgid "Linear Support Vector Regression."
msgstr ""

#: :5
msgid ""
"Similar to SVR with parameter kernel='linear', but implemented in terms "
"of liblinear rather than libsvm, so it has more flexibility in the choice"
" of penalties and loss functions and should scale better to large numbers"
" of samples."
msgstr ""

#: :10
msgid "This class supports both dense and sparse input."
msgstr ""

#: :12
msgid "Read more in the :ref:`User Guide <svm_regression>`."
msgstr ""

#: :16
msgid "**C** : float, optional (default=1.0)"
msgstr ""

#: :18
msgid ""
"Penalty parameter C of the error term. The penalty is a squared l2 "
"penalty. The bigger this parameter, the less regularization is used."
msgstr ""

#: :21
msgid ""
"**loss** : string, 'epsilon_insensitive' or 'squared_epsilon_insensitive'"
" (default='epsilon_insensitive')"
msgstr ""

#: :23
msgid ""
"Specifies the loss function. 'l1' is the epsilon-insensitive loss "
"(standard SVR) while 'l2' is the squared epsilon-insensitive loss."
msgstr ""

#: :26
msgid "**epsilon** : float, optional (default=0.1)"
msgstr ""

#: :28
msgid ""
"Epsilon parameter in the epsilon-insensitive loss function. Note that the"
" value of this parameter depends on the scale of the target variable y. "
"If unsure, set ``epsilon=0``."
msgstr ""

#: :32
msgid "**dual** : bool, (default=True)"
msgstr ""

#: :34
msgid ""
"Select the algorithm to either solve the dual or primal optimization "
"problem. Prefer dual=False when n_samples > n_features."
msgstr ""

#: :37
msgid "**tol** : float, optional (default=1e-4)"
msgstr ""

#: :39
msgid "Tolerance for stopping criteria."
msgstr ""

#: :41
msgid "**fit_intercept** : boolean, optional (default=True)"
msgstr ""

#: :43
msgid ""
"Whether to calculate the intercept for this model. If set to false, no "
"intercept will be used in calculations (i.e. data is expected to be "
"already centered)."
msgstr ""

#: :47
msgid "**intercept_scaling** : float, optional (default=1)"
msgstr ""

#: :49
msgid ""
"When self.fit_intercept is True, instance vector x becomes [x, "
"self.intercept_scaling], i.e. a \"synthetic\" feature with constant value"
" equals to intercept_scaling is appended to the instance vector. The "
"intercept becomes intercept_scaling * synthetic feature weight Note! the "
"synthetic feature weight is subject to l1/l2 regularization as all other "
"features. To lessen the effect of regularization on synthetic feature "
"weight (and therefore on the intercept) intercept_scaling has to be "
"increased."
msgstr ""

#: :59
msgid "**verbose** : int, (default=0)"
msgstr ""

#: :61
msgid ""
"Enable verbose output. Note that this setting takes advantage of a per-"
"process runtime setting in liblinear that, if enabled, may not work "
"properly in a multithreaded context."
msgstr ""

#: :65
msgid "**random_state** : int seed, RandomState instance, or None (default=None)"
msgstr ""

#: :67
msgid ""
"The seed of the pseudo random number generator to use when shuffling the "
"data."
msgstr ""

#: :70
msgid "**max_iter** : int, (default=1000)"
msgstr ""

#: :72
msgid "The maximum number of iterations to be run."
msgstr ""

#: :76
msgid ""
"**coef_** : array, shape = [n_features] if n_classes == 2 else "
"[n_classes, n_features]"
msgstr ""

#: :78
msgid ""
"Weights assigned to the features (coefficients in the primal problem). "
"This is only available in the case of a linear kernel."
msgstr ""

#: :81
msgid ""
"`coef_` is a readonly property derived from `raw_coef_` that follows the "
"internal memory layout of liblinear."
msgstr ""

#: :84
msgid "**intercept_** : array, shape = [1] if n_classes == 2 else [n_classes]"
msgstr ""

#: :86
msgid "Constants in decision function."
msgstr ""

#: :91
msgid ":obj:`LinearSVC`"
msgstr ""

#: :91
msgid ""
"Implementation of Support Vector Machine classifier using the same "
"library as this class (liblinear)."
msgstr ""

#: :94
msgid ":obj:`SVR`"
msgstr ""

#: :94
msgid ""
"Implementation of Support Vector Machine regression using libsvm: the "
"kernel can be non-linear but its SMO algorithm does not scale to large "
"number of samples as LinearSVC does."
msgstr ""

#: :96
msgid ":obj:`sklearn.linear_model.SGDRegressor`"
msgstr ""

#: :97
msgid ""
"SGDRegressor can optimize the same cost function as LinearSVR by "
"adjusting the penalty and loss parameters. In addition it requires less "
"memory, allows incremental (online) learning, and implements various loss"
" functions and regularization regimes."
msgstr ""

#: :100
msgid "Methods"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`decision_function <sklearn.svm.LinearSVR.decision_function>`\\ "
"(\\*args, \\*\\*kwargs)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "DEPRECATED:  and will be removed in 0.19."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`fit <sklearn.svm.LinearSVR.fit>`\\ (X, y)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Fit the model according to the given training data."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`get_params <sklearn.svm.LinearSVR.get_params>`\\ ([deep])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Get parameters for this estimator."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`predict <sklearn.svm.LinearSVR.predict>`\\ (X)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Predict using the linear model"
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`score <sklearn.svm.LinearSVR.score>`\\ (X, y[, sample_weight])"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Returns the coefficient of determination R^2 of the prediction."
msgstr ""

#: ../../<autosummary>:1
msgid ":obj:`set_params <sklearn.svm.LinearSVR.set_params>`\\ (\\*\\*params)"
msgstr ""

#: ../../<autosummary>:1 :3
msgid "Set the parameters of this estimator."
msgstr ""

#: :5
msgid "Decision function of the linear model."
msgstr ""

#: :9 :7
msgid "**X** : {array-like, sparse matrix}, shape = (n_samples, n_features)"
msgstr ""

#: :11 :9
msgid "Samples."
msgstr ""

#: :15 :13
msgid "**C** : array, shape = (n_samples,)"
msgstr ""

#: :17 :15
msgid "Returns predicted values."
msgstr ""

#: :7
msgid "**X** : {array-like, sparse matrix}, shape = [n_samples, n_features]"
msgstr ""

#: :9
msgid ""
"Training vector, where n_samples in the number of samples and n_features "
"is the number of features."
msgstr ""

#: :12
msgid "**y** : array-like, shape = [n_samples]"
msgstr ""

#: :14
msgid "Target vector relative to X"
msgstr ""

#: :18
msgid "**self** : object"
msgstr ""

#: :20
msgid "Returns self."
msgstr ""

#: :7
msgid "**deep: boolean, optional** :"
msgstr ""

#: :9
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: :14
msgid "**params** : mapping of string to any"
msgstr ""

#: :16
msgid "Parameter names mapped to their values."
msgstr ""

#: :5
msgid ""
"The coefficient R^2 is defined as (1 - u/v), where u is the regression "
"sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum "
"of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is "
"1.0 and it can be negative (because the model can be arbitrarily worse). "
"A constant model that always predicts the expected value of y, "
"disregarding the input features, would get a R^2 score of 0.0."
msgstr ""

#: :15
msgid "**X** : array-like, shape = (n_samples, n_features)"
msgstr ""

#: :17
msgid "Test samples."
msgstr ""

#: :19
msgid "**y** : array-like, shape = (n_samples) or (n_samples, n_outputs)"
msgstr ""

#: :21
msgid "True values for X."
msgstr ""

#: :23
msgid "**sample_weight** : array-like, shape = [n_samples], optional"
msgstr ""

#: :25
msgid "Sample weights."
msgstr ""

#: :29
msgid "**score** : float"
msgstr ""

#: :31
msgid "R^2 of self.predict(X) wrt. y."
msgstr ""

#: :5
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as pipelines). The former have parameters of the form "
"``<component>__<parameter>`` so that it's possible to update each "
"component of a nested object."
msgstr ""

#: :12
msgid "**self** :"
msgstr ""

