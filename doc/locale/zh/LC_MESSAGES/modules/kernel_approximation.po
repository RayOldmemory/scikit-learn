# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/kernel_approximation.rst:4
msgid "Kernel Approximation"
msgstr ""

#: ../../modules/kernel_approximation.rst:6
msgid ""
"This submodule contains functions that approximate the feature mappings "
"that correspond to certain kernels, as they are used for example in "
"support vector machines (see :ref:`svm`). The following feature functions"
" perform non-linear transformations of the input, which can serve as a "
"basis for linear classification or other algorithms."
msgstr ""

#: ../../modules/kernel_approximation.rst:15
msgid ""
"The advantage of using approximate explicit feature maps compared to the "
"`kernel trick <http://en.wikipedia.org/wiki/Kernel_trick>`_, which makes "
"use of feature maps implicitly, is that explicit mappings can be better "
"suited for online learning and can significantly reduce the cost of "
"learning with very large datasets. Standard kernelized SVMs do not scale "
"well to large datasets, but using an approximate kernel map it is "
"possible to use much more efficient linear SVMs. In particular, the "
"combination of kernel map approximations with :class:`SGDClassifier` can "
"make non-linear learning on large datasets possible."
msgstr ""

#: ../../modules/kernel_approximation.rst:25
msgid ""
"Since there has not been much empirical work using approximate "
"embeddings, it is advisable to compare results against exact kernel "
"methods when possible."
msgstr ""

#: ../../modules/kernel_approximation.rst:30
msgid ":ref:`polynomial_regression` for an exact polynomial transformation."
msgstr ""

#: ../../modules/kernel_approximation.rst:37
msgid "Nystroem Method for Kernel Approximation"
msgstr ""

#: ../../modules/kernel_approximation.rst:38
msgid ""
"The Nystroem method, as implemented in :class:`Nystroem` is a general "
"method for low-rank approximations of kernels. It achieves this by "
"essentially subsampling the data on which the kernel is evaluated. By "
"default :class:`Nystroem` uses the ``rbf`` kernel, but it can use any "
"kernel function or a precomputed kernel matrix. The number of samples "
"used - which is also the dimensionality of the features computed - is "
"given by the parameter ``n_components``."
msgstr ""

#: ../../modules/kernel_approximation.rst:49
msgid "Radial Basis Function Kernel"
msgstr ""

#: ../../modules/kernel_approximation.rst:51
msgid ""
"The :class:`RBFSampler` constructs an approximate mapping for the radial "
"basis function kernel, also known as *Random Kitchen Sinks* [RR2007]_. "
"This transformation can be used to explicitly model a kernel map, prior "
"to applying a linear algorithm, for example a linear SVM::"
msgstr ""

#: ../../modules/kernel_approximation.rst:72
msgid ""
"The mapping relies on a Monte Carlo approximation to the kernel values. "
"The ``fit`` function performs the Monte Carlo sampling, whereas the "
"``transform`` method performs the mapping of the data.  Because of the "
"inherent randomness of the process, results may vary between different "
"calls to the ``fit`` function."
msgstr ""

#: ../../modules/kernel_approximation.rst:78
msgid ""
"The ``fit`` function takes two arguments: ``n_components``, which is the "
"target dimensionality of the feature transform, and ``gamma``, the "
"parameter of the RBF-kernel.  A higher ``n_components`` will result in a "
"better approximation of the kernel and will yield results more similar to"
" those produced by a kernel SVM. Note that \"fitting\" the feature "
"function does not actually depend on the data given to the ``fit`` "
"function. Only the dimensionality of the data is used. Details on the "
"method can be found in [RR2007]_."
msgstr ""

#: ../../modules/kernel_approximation.rst:87
msgid ""
"For a given value of ``n_components`` :class:`RBFSampler` is often less "
"accurate as :class:`Nystroem`. :class:`RBFSampler` is cheaper to compute,"
" though, making use of larger feature spaces more efficient."
msgstr ""

#: ../../modules/kernel_approximation.rst:96
msgid "Comparing an exact RBF kernel (left) with the approximation (right)"
msgstr ""

#: ../../modules/kernel_approximation.rst
msgid "Examples:"
msgstr ""

#: ../../modules/kernel_approximation.rst:100
msgid ":ref:`example_plot_kernel_approximation.py`"
msgstr ""

#: ../../modules/kernel_approximation.rst:105
msgid "Additive Chi Squared Kernel"
msgstr ""

#: ../../modules/kernel_approximation.rst:107
msgid ""
"The additive chi squared kernel is a kernel on histograms, often used in "
"computer vision."
msgstr ""

#: ../../modules/kernel_approximation.rst:109
msgid "The additive chi squared kernel as used here is given by"
msgstr ""

#: ../../modules/kernel_approximation.rst:115
msgid ""
"This is not exactly the same as "
":func:`sklearn.metrics.additive_chi2_kernel`. The authors of [VZ2010]_ "
"prefer the version above as it is always positive definite. Since the "
"kernel is additive, it is possible to treat all components :math:`x_i` "
"separately for embedding. This makes it possible to sample the Fourier "
"transform in regular intervals, instead of approximating using Monte "
"Carlo sampling."
msgstr ""

#: ../../modules/kernel_approximation.rst:123
msgid ""
"The class :class:`AdditiveChi2Sampler` implements this component wise "
"deterministic sampling. Each component is sampled :math:`n` times, "
"yielding :math:`2n+1` dimensions per input dimension (the multiple of two"
" stems from the real and complex part of the Fourier transform). In the "
"literature, :math:`n` is usually chosen to be 1 or 2, transforming the "
"dataset to size ``n_samples * 5 * n_features`` (in the case of "
":math:`n=2`)."
msgstr ""

#: ../../modules/kernel_approximation.rst:130
msgid ""
"The approximate feature map provided by :class:`AdditiveChi2Sampler` can "
"be combined with the approximate feature map provided by "
":class:`RBFSampler` to yield an approximate feature map for the "
"exponentiated chi squared kernel. See the [VZ2010]_ for details and "
"[VVZ2010]_ for combination with the :class:`RBFSampler`."
msgstr ""

#: ../../modules/kernel_approximation.rst:138
msgid "Skewed Chi Squared Kernel"
msgstr ""

#: ../../modules/kernel_approximation.rst:140
msgid "The skewed chi squared kernel is given by:"
msgstr ""

#: ../../modules/kernel_approximation.rst:147
msgid ""
"It has properties that are similar to the exponentiated chi squared "
"kernel often used in computer vision, but allows for a simple Monte Carlo"
" approximation of the feature map."
msgstr ""

#: ../../modules/kernel_approximation.rst:151
msgid ""
"The usage of the :class:`SkewedChi2Sampler` is the same as the usage "
"described above for the :class:`RBFSampler`. The only difference is in "
"the free parameter, that is called :math:`c`. For a motivation for this "
"mapping and the mathematical details see [LS2010]_."
msgstr ""

#: ../../modules/kernel_approximation.rst:158
msgid "Mathematical Details"
msgstr ""

#: ../../modules/kernel_approximation.rst:160
msgid ""
"Kernel methods like support vector machines or kernelized PCA rely on a "
"property of reproducing kernel Hilbert spaces. For any positive definite "
"kernel function :math:`k` (a so called Mercer kernel), it is guaranteed "
"that there exists a mapping :math:`\\phi` into a Hilbert space "
":math:`\\mathcal{H}`, such that"
msgstr ""

#: ../../modules/kernel_approximation.rst:170
msgid ""
"Where :math:`\\langle \\cdot, \\cdot \\rangle` denotes the inner product "
"in the Hilbert space."
msgstr ""

#: ../../modules/kernel_approximation.rst:173
msgid ""
"If an algorithm, such as a linear support vector machine or PCA, relies "
"only on the scalar product of data points :math:`x_i`, one may use the "
"value of :math:`k(x_i, x_j)`, which corresponds to applying the algorithm"
" to the mapped data points :math:`\\phi(x_i)`. The advantage of using "
":math:`k` is that the mapping :math:`\\phi` never has to be calculated "
"explicitly, allowing for arbitrary large features (even infinite)."
msgstr ""

#: ../../modules/kernel_approximation.rst:181
msgid ""
"One drawback of kernel methods is, that it might be necessary to store "
"many kernel values :math:`k(x_i, x_j)` during optimization. If a "
"kernelized classifier is applied to new data :math:`y_j`, :math:`k(x_i, "
"y_j)` needs to be computed to make predictions, possibly for many "
"different :math:`x_i` in the training set."
msgstr ""

#: ../../modules/kernel_approximation.rst:187
msgid ""
"The classes in this submodule allow to approximate the embedding "
":math:`\\phi`, thereby working explicitly with the representations "
":math:`\\phi(x_i)`, which obviates the need to apply the kernel or store "
"training examples."
msgstr ""

#: ../../modules/kernel_approximation.rst
msgid "References:"
msgstr ""

#: ../../modules/kernel_approximation.rst:195
msgid ""
"`\"Random features for large-scale kernel machines\" "
"<http://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf>`_ Rahimi, "
"A. and Recht, B. - Advances in neural information processing 2007,"
msgstr ""

#: ../../modules/kernel_approximation.rst:198
msgid ""
"`\"Random Fourier approximations for skewed multiplicative histogram "
"kernels\" <http://sminchisescu.ins.uni-bonn.de/papers/lis_dagm10.pdf>`_ "
"Random Fourier approximations for skewed multiplicative histogram kernels"
" - Lecture Notes for Computer Sciencd (DAGM)"
msgstr ""

#: ../../modules/kernel_approximation.rst:202
msgid ""
"`\"Efficient additive kernels via explicit feature maps\" <http://eprints"
".pascal-network.org/archive/00006964/01/vedaldi10.pdf>`_ Vedaldi, A. and "
"Zisserman, A. - Computer Vision and Pattern Recognition 2010"
msgstr ""

#: ../../modules/kernel_approximation.rst:205
msgid ""
"`\"Generalized RBF feature maps for Efficient Detection\" <http://eprints"
".pascal-"
"network.org/archive/00007024/01/inproceedings.pdf.8a865c2a5421e40d.537265656b616e7468313047656e6572616c697a65642e706466.pdf>`_"
" Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010"
msgstr ""

