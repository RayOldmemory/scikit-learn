# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/linear_model.rst:5
msgid "Generalized Linear Models"
msgstr ""

#: ../../modules/linear_model.rst:9
msgid ""
"The following are a set of methods intended for regression in which the "
"target value is expected to be a linear combination of the input "
"variables. In mathematical notion, if :math:`\\hat{y}` is the predicted "
"value."
msgstr ""

#: ../../modules/linear_model.rst:16
msgid ""
"Across the module, we designate the vector :math:`w = (w_1, ..., w_p)` as"
" ``coef_`` and :math:`w_0` as ``intercept_``."
msgstr ""

#: ../../modules/linear_model.rst:19
msgid ""
"To perform classification with generalized linear models, see "
":ref:`Logistic_regression`."
msgstr ""

#: ../../modules/linear_model.rst:26
msgid "Ordinary Least Squares"
msgstr ""

#: ../../modules/linear_model.rst:28
msgid ""
":class:`LinearRegression` fits a linear model with coefficients :math:`w "
"= (w_1, ..., w_p)` to minimize the residual sum of squares between the "
"observed responses in the dataset, and the responses predicted by the "
"linear approximation. Mathematically it solves a problem of the form:"
msgstr ""

#: ../../modules/linear_model.rst:41
msgid ""
":class:`LinearRegression` will take in its ``fit`` method arrays X, y and"
" will store the coefficients :math:`w` of the linear model in its "
"``coef_`` member::"
msgstr ""

#: ../../modules/linear_model.rst:52
msgid ""
"However, coefficient estimates for Ordinary Least Squares rely on the "
"independence of the model terms. When terms are correlated and the "
"columns of the design matrix :math:`X` have an approximate linear "
"dependence, the design matrix becomes close to singular and as a result, "
"the least-squares estimate becomes highly sensitive to random errors in "
"the observed response, producing a large variance. This situation of "
"*multicollinearity* can arise, for example, when data are collected "
"without an experimental design."
msgstr ""

#: ../../modules/linear_model.rst
msgid "Examples:"
msgstr ""

#: ../../modules/linear_model.rst:63
msgid ":ref:`example_linear_model_plot_ols.py`"
msgstr ""

#: ../../modules/linear_model.rst:67
msgid "Ordinary Least Squares Complexity"
msgstr ""

#: ../../modules/linear_model.rst:69
msgid ""
"This method computes the least squares solution using a singular value "
"decomposition of X. If X is a matrix of size (n, p) this method has a "
"cost of :math:`O(n p^2)`, assuming that :math:`n \\geq p`."
msgstr ""

#: ../../modules/linear_model.rst:76
msgid "Ridge Regression"
msgstr ""

#: ../../modules/linear_model.rst:78
msgid ""
":class:`Ridge` regression addresses some of the problems of "
":ref:`ordinary_least_squares` by imposing a penalty on the size of "
"coefficients. The ridge coefficients minimize a penalized residual sum of"
" squares,"
msgstr ""

#: ../../modules/linear_model.rst:89
msgid ""
"Here, :math:`\\alpha \\geq 0` is a complexity parameter that controls the"
" amount of shrinkage: the larger the value of :math:`\\alpha`, the "
"greater the amount of shrinkage and thus the coefficients become more "
"robust to collinearity."
msgstr ""

#: ../../modules/linear_model.rst:99
msgid ""
"As with other linear models, :class:`Ridge` will take in its ``fit`` "
"method arrays X, y and will store the coefficients :math:`w` of the "
"linear model in its ``coef_`` member::"
msgstr ""

#: ../../modules/linear_model.rst:116
msgid ":ref:`example_linear_model_plot_ridge_path.py`"
msgstr ""

#: ../../modules/linear_model.rst:117
msgid ":ref:`example_text_document_classification_20newsgroups.py`"
msgstr ""

#: ../../modules/linear_model.rst:121
msgid "Ridge Complexity"
msgstr ""

#: ../../modules/linear_model.rst:123
msgid ""
"This method has the same order of complexity than an "
":ref:`ordinary_least_squares`."
msgstr ""

#: ../../modules/linear_model.rst:133
msgid "Setting the regularization parameter: generalized Cross-Validation"
msgstr ""

#: ../../modules/linear_model.rst:135
msgid ""
":class:`RidgeCV` implements ridge regression with built-in cross-"
"validation of the alpha parameter.  The object works in the same way as "
"GridSearchCV except that it defaults to Generalized Cross-Validation "
"(GCV), an efficient form of leave-one-out cross-validation::"
msgstr ""

#: ../../modules/linear_model.rst
msgid "References"
msgstr ""

#: ../../modules/linear_model.rst:150
msgid ""
"\"Notes on Regularized Least Squares\", Rifkin & Lippert (`technical "
"report <http://cbcl.mit.edu/projects/cbcl/publications/ps/MIT-CSAIL-"
"TR-2007-025.pdf>`_, `course slides "
"<http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf>`_)."
msgstr ""

#: ../../modules/linear_model.rst:159
msgid "Lasso"
msgstr ""

#: ../../modules/linear_model.rst:161
msgid ""
"The :class:`Lasso` is a linear model that estimates sparse coefficients. "
"It is useful in some contexts due to its tendency to prefer solutions "
"with fewer parameter values, effectively reducing the number of variables"
" upon which the given solution is dependent. For this reason, the Lasso "
"and its variants are fundamental to the field of compressed sensing. "
"Under certain conditions, it can recover the exact set of non-zero "
"weights (see "
":ref:`example_applications_plot_tomography_l1_reconstruction.py`)."
msgstr ""

#: ../../modules/linear_model.rst:170
msgid ""
"Mathematically, it consists of a linear model trained with "
":math:`\\ell_1` prior as regularizer. The objective function to minimize "
"is:"
msgstr ""

#: ../../modules/linear_model.rst:175
msgid ""
"The lasso estimate thus solves the minimization of the least-squares "
"penalty with :math:`\\alpha ||w||_1` added, where :math:`\\alpha` is a "
"constant and :math:`||w||_1` is the :math:`\\ell_1`-norm of the parameter"
" vector."
msgstr ""

#: ../../modules/linear_model.rst:180
msgid ""
"The implementation in the class :class:`Lasso` uses coordinate descent as"
" the algorithm to fit the coefficients. See :ref:`least_angle_regression`"
" for another implementation::"
msgstr ""

#: ../../modules/linear_model.rst:193
msgid ""
"Also useful for lower-level tasks is the function :func:`lasso_path` that"
" computes the coefficients along the full path of possible values."
msgstr ""

#: ../../modules/linear_model.rst:198 ../../modules/linear_model.rst:304
msgid ":ref:`example_linear_model_plot_lasso_and_elasticnet.py`"
msgstr ""

#: ../../modules/linear_model.rst:199
msgid ":ref:`example_applications_plot_tomography_l1_reconstruction.py`"
msgstr ""

#: ../../modules/linear_model.rst:202
msgid "**Feature selection with Lasso**"
msgstr ""

#: ../../modules/linear_model.rst:204
msgid ""
"As the Lasso regression yields sparse models, it can thus be used to "
"perform feature selection, as detailed in :ref:`l1_feature_selection`."
msgstr ""

#: ../../modules/linear_model.rst:208
msgid "**Randomized sparsity**"
msgstr ""

#: ../../modules/linear_model.rst:210
msgid ""
"For feature selection or sparse recovery, it may be interesting to use "
":ref:`randomized_l1`."
msgstr ""

#: ../../modules/linear_model.rst:215
msgid "Setting regularization parameter"
msgstr ""

#: ../../modules/linear_model.rst:217
msgid ""
"The ``alpha`` parameter controls the degree of sparsity of the "
"coefficients estimated."
msgstr ""

#: ../../modules/linear_model.rst:221
msgid "Using cross-validation"
msgstr ""

#: ../../modules/linear_model.rst:223
msgid ""
"scikit-learn exposes objects that set the Lasso ``alpha`` parameter by "
"cross-validation: :class:`LassoCV` and :class:`LassoLarsCV`. "
":class:`LassoLarsCV` is based on the :ref:`least_angle_regression` "
"algorithm explained below."
msgstr ""

#: ../../modules/linear_model.rst:228
msgid ""
"For high-dimensional datasets with many collinear regressors, "
":class:`LassoCV` is most often preferable. However, :class:`LassoLarsCV` "
"has the advantage of exploring more relevant values of `alpha` parameter,"
" and if the number of samples is very small compared to the number of "
"observations, it is often faster than :class:`LassoCV`."
msgstr ""

#: ../../modules/linear_model.rst:244
msgid "lasso_cv_1 lasso_cv_2"
msgstr ""

#: ../../modules/linear_model.rst:246
msgid "Information-criteria based model selection"
msgstr ""

#: ../../modules/linear_model.rst:248
msgid ""
"Alternatively, the estimator :class:`LassoLarsIC` proposes to use the "
"Akaike information criterion (AIC) and the Bayes Information criterion "
"(BIC). It is a computationally cheaper alternative to find the optimal "
"value of alpha as the regularization path is computed only once instead "
"of k+1 times when using k-fold cross-validation. However, such criteria "
"needs a proper estimation of the degrees of freedom of the solution, are "
"derived for large samples (asymptotic results) and assume the model is "
"correct, i.e. that the data are actually generated by this model. They "
"also tend to break when the problem is badly conditioned (more features "
"than samples)."
msgstr ""

#: ../../modules/linear_model.rst:267
msgid ":ref:`example_linear_model_plot_lasso_model_selection.py`"
msgstr ""

#: ../../modules/linear_model.rst:272
msgid "Elastic Net"
msgstr ""

#: ../../modules/linear_model.rst:273
msgid ""
":class:`ElasticNet` is a linear regression model trained with L1 and L2 "
"prior as regularizer. This combination allows for learning a sparse model"
" where few of the weights are non-zero like :class:`Lasso`, while still "
"maintaining the regularization properties of :class:`Ridge`. We control "
"the convex combination of L1 and L2 using the ``l1_ratio`` parameter."
msgstr ""

#: ../../modules/linear_model.rst:279
msgid ""
"Elastic-net is useful when there are multiple features which are "
"correlated with one another. Lasso is likely to pick one of these at "
"random, while elastic-net is likely to pick both."
msgstr ""

#: ../../modules/linear_model.rst:283
msgid ""
"A practical advantage of trading-off between Lasso and Ridge is it allows"
" Elastic-Net to inherit some of Ridge's stability under rotation."
msgstr ""

#: ../../modules/linear_model.rst:286
msgid "The objective function to minimize is in this case"
msgstr ""

#: ../../modules/linear_model.rst:299
msgid ""
"The class :class:`ElasticNetCV` can be used to set the parameters "
"``alpha`` (:math:`\\alpha`) and ``l1_ratio`` (:math:`\\rho`) by cross-"
"validation."
msgstr ""

#: ../../modules/linear_model.rst:305
msgid ":ref:`example_linear_model_plot_lasso_coordinate_descent_path.py`"
msgstr ""

#: ../../modules/linear_model.rst:311
msgid "Multi-task Lasso"
msgstr ""

#: ../../modules/linear_model.rst:313
msgid ""
"The :class:`MultiTaskLasso` is a linear model that estimates sparse "
"coefficients for multiple regression problems jointly: ``y`` is a 2D "
"array, of shape (n_samples, n_tasks). The constraint is that the selected"
" features are the same for all the regression problems, also called "
"tasks."
msgstr ""

#: ../../modules/linear_model.rst:318
msgid ""
"The following figure compares the location of the non-zeros in W obtained"
" with a simple Lasso or a MultiTaskLasso. The Lasso estimates yields "
"scattered non-zeros while the non-zeros of the MultiTaskLasso are full "
"columns."
msgstr ""

#: ../../modules/linear_model.rst:332
msgid "multi_task_lasso_1 multi_task_lasso_2"
msgstr ""

#: ../../modules/linear_model.rst:334
msgid ""
"Fitting a time-series model, imposing that any active feature be active "
"at all times."
msgstr ""

#: ../../modules/linear_model.rst:337
msgid ":ref:`example_linear_model_plot_multi_task_lasso_support.py`"
msgstr ""

#: ../../modules/linear_model.rst:341
msgid ""
"Mathematically, it consists of a linear model trained with a mixed "
":math:`\\ell_1` :math:`\\ell_2` prior as regularizer. The objective "
"function to minimize is:"
msgstr ""

#: ../../modules/linear_model.rst:347
msgid "where;"
msgstr ""

#: ../../modules/linear_model.rst:352
msgid ""
"The implementation in the class :class:`MultiTaskLasso` uses coordinate "
"descent as the algorithm to fit the coefficients."
msgstr ""

#: ../../modules/linear_model.rst:358
msgid "Least Angle Regression"
msgstr ""

#: ../../modules/linear_model.rst:360
msgid ""
"Least-angle regression (LARS) is a regression algorithm for high-"
"dimensional data, developed by Bradley Efron, Trevor Hastie, Iain "
"Johnstone and Robert Tibshirani."
msgstr ""

#: ../../modules/linear_model.rst:364
msgid "The advantages of LARS are:"
msgstr ""

#: ../../modules/linear_model.rst:366
msgid ""
"It is numerically efficient in contexts where p >> n (i.e., when the "
"number of dimensions is significantly greater than the number of points)"
msgstr ""

#: ../../modules/linear_model.rst:370
msgid ""
"It is computationally just as fast as forward selection and has the same "
"order of complexity as an ordinary least squares."
msgstr ""

#: ../../modules/linear_model.rst:373
msgid ""
"It produces a full piecewise linear solution path, which is useful in "
"cross-validation or similar attempts to tune the model."
msgstr ""

#: ../../modules/linear_model.rst:376
msgid ""
"If two variables are almost equally correlated with the response, then "
"their coefficients should increase at approximately the same rate. The "
"algorithm thus behaves as intuition would expect, and also is more "
"stable."
msgstr ""

#: ../../modules/linear_model.rst:381
msgid ""
"It is easily modified to produce solutions for other estimators, like the"
" Lasso."
msgstr ""

#: ../../modules/linear_model.rst:384
msgid "The disadvantages of the LARS method include:"
msgstr ""

#: ../../modules/linear_model.rst:386
msgid ""
"Because LARS is based upon an iterative refitting of the residuals, it "
"would appear to be especially sensitive to the effects of noise. This "
"problem is discussed in detail by Weisberg in the discussion section of "
"the Efron et al. (2004) Annals of Statistics article."
msgstr ""

#: ../../modules/linear_model.rst:392
msgid ""
"The LARS model can be used using estimator :class:`Lars`, or its low-"
"level implementation :func:`lars_path`."
msgstr ""

#: ../../modules/linear_model.rst:397
msgid "LARS Lasso"
msgstr ""

#: ../../modules/linear_model.rst:399
msgid ""
":class:`LassoLars` is a lasso model implemented using the LARS algorithm,"
" and unlike the implementation based on coordinate_descent, this yields "
"the exact solution, which is piecewise linear as a function of the norm "
"of its coefficients."
msgstr ""

#: ../../modules/linear_model.rst:422
msgid ":ref:`example_linear_model_plot_lasso_lars.py`"
msgstr ""

#: ../../modules/linear_model.rst:424
msgid ""
"The Lars algorithm provides the full path of the coefficients along the "
"regularization parameter almost for free, thus a common operation consist"
" of retrieving the path with function :func:`lars_path`"
msgstr ""

#: ../../modules/linear_model.rst:429
msgid "Mathematical formulation"
msgstr ""

#: ../../modules/linear_model.rst:431
msgid ""
"The algorithm is similar to forward stepwise regression, but instead of "
"including variables at each step, the estimated parameters are increased "
"in a direction equiangular to each one's correlations with the residual."
msgstr ""

#: ../../modules/linear_model.rst:436
msgid ""
"Instead of giving a vector result, the LARS solution consists of a curve "
"denoting the solution for each value of the L1 norm of the parameter "
"vector. The full coefficients path is stored in the array ``coef_path_``,"
" which has size (n_features, max_features+1). The first column is always "
"zero."
msgstr ""

#: ../../modules/linear_model.rst
msgid "References:"
msgstr ""

#: ../../modules/linear_model.rst:444
msgid ""
"Original Algorithm is detailed in the paper `Least Angle Regression <http"
"://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf>`_ by "
"Hastie et al."
msgstr ""

#: ../../modules/linear_model.rst:452
msgid "Orthogonal Matching Pursuit (OMP)"
msgstr ""

#: ../../modules/linear_model.rst:453
msgid ""
":class:`OrthogonalMatchingPursuit` and :func:`orthogonal_mp` implements "
"the OMP algorithm for approximating the fit of a linear model with "
"constraints imposed on the number of non-zero coefficients (ie. the L "
":sub:`0` pseudo-norm)."
msgstr ""

#: ../../modules/linear_model.rst:457
msgid ""
"Being a forward feature selection method like "
":ref:`least_angle_regression`, orthogonal matching pursuit can "
"approximate the optimum solution vector with a fixed number of non-zero "
"elements:"
msgstr ""

#: ../../modules/linear_model.rst:464
msgid ""
"Alternatively, orthogonal matching pursuit can target a specific error "
"instead of a specific number of non-zero coefficients. This can be "
"expressed as:"
msgstr ""

#: ../../modules/linear_model.rst:471
msgid ""
"OMP is based on a greedy algorithm that includes at each step the atom "
"most highly correlated with the current residual. It is similar to the "
"simpler matching pursuit (MP) method, but better in that at each "
"iteration, the residual is recomputed using an orthogonal projection on "
"the space of the previously chosen dictionary elements."
msgstr ""

#: ../../modules/linear_model.rst:480
msgid ":ref:`example_linear_model_plot_omp.py`"
msgstr ""

#: ../../modules/linear_model.rst:484
msgid "http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf"
msgstr ""

#: ../../modules/linear_model.rst:486
msgid ""
"`Matching pursuits with time-frequency dictionaries "
"<http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf>`_, "
"S. G. Mallat, Z. Zhang,"
msgstr ""

#: ../../modules/linear_model.rst:494
msgid "Bayesian Regression"
msgstr ""

#: ../../modules/linear_model.rst:496
msgid ""
"Bayesian regression techniques can be used to include regularization "
"parameters in the estimation procedure: the regularization parameter is "
"not set in a hard sense but tuned to the data at hand."
msgstr ""

#: ../../modules/linear_model.rst:500
msgid ""
"This can be done by introducing `uninformative priors "
"<http://en.wikipedia.org/wiki/Non-"
"informative_prior#Uninformative_priors>`__ over the hyper parameters of "
"the model. The :math:`\\ell_{2}` regularization used in `Ridge "
"Regression`_ is equivalent to finding a maximum a-postiori solution under"
" a Gaussian prior over the parameters :math:`w` with precision "
":math:`\\lambda^-1`.  Instead of setting `\\lambda` manually, it is "
"possible to treat it as a random variable to be estimated from the data."
msgstr ""

#: ../../modules/linear_model.rst:509
msgid ""
"To obtain a fully probabilistic model, the output :math:`y` is assumed to"
" be Gaussian distributed around :math:`X w`:"
msgstr ""

#: ../../modules/linear_model.rst:514
msgid ""
"Alpha is again treated as a random variable that is to be estimated from "
"the data."
msgstr ""

#: ../../modules/linear_model.rst:517
msgid "The advantages of Bayesian Regression are:"
msgstr ""

#: ../../modules/linear_model.rst:519
msgid "It adapts to the data at hand."
msgstr ""

#: ../../modules/linear_model.rst:521
msgid ""
"It can be used to include regularization parameters in the estimation "
"procedure."
msgstr ""

#: ../../modules/linear_model.rst:524
msgid "The disadvantages of Bayesian regression include:"
msgstr ""

#: ../../modules/linear_model.rst:526
msgid "Inference of the model can be time consuming."
msgstr ""

#: ../../modules/linear_model.rst:531
msgid ""
"A good introduction to Bayesian methods is given in C. Bishop: Pattern "
"Recognition and Machine learning"
msgstr ""

#: ../../modules/linear_model.rst:534
msgid ""
"Original Algorithm is detailed in the  book `Bayesian learning for neural"
" networks` by Radford M. Neal"
msgstr ""

#: ../../modules/linear_model.rst:540
msgid "Bayesian Ridge Regression"
msgstr ""

#: ../../modules/linear_model.rst:542
msgid ""
":class:`BayesianRidge` estimates a probabilistic model of the regression "
"problem as described above. The prior for the parameter :math:`w` is "
"given by a spherical Gaussian:"
msgstr ""

#: ../../modules/linear_model.rst:549
msgid ""
"The priors over :math:`\\alpha` and :math:`\\lambda` are chosen to be "
"`gamma distributions "
"<http://en.wikipedia.org/wiki/Gamma_distribution>`__, the conjugate prior"
" for the precision of the Gaussian."
msgstr ""

#: ../../modules/linear_model.rst:553
msgid ""
"The resulting model is called *Bayesian Ridge Regression*, and is similar"
" to the classical :class:`Ridge`.  The parameters :math:`w`, "
":math:`\\alpha` and :math:`\\lambda` are estimated jointly during the fit"
" of the model.  The remaining hyperparameters are the parameters of the "
"gamma priors over :math:`\\alpha` and :math:`\\lambda`.  These are "
"usually chosen to be *non-informative*.  The parameters are estimated by "
"maximizing the *marginal log likelihood*."
msgstr ""

#: ../../modules/linear_model.rst:561
msgid ""
"By default :math:`\\alpha_1 = \\alpha_2 =  \\lambda_1 = \\lambda_2 = "
"1.e^{-6}`."
msgstr ""

#: ../../modules/linear_model.rst:570
msgid "Bayesian Ridge Regression is used for regression::"
msgstr ""

#: ../../modules/linear_model.rst:581
msgid "After being fitted, the model can then be used to predict new values::"
msgstr ""

#: ../../modules/linear_model.rst:587
msgid "The weights :math:`w` of the model can be access::"
msgstr ""

#: ../../modules/linear_model.rst:592
msgid ""
"Due to the Bayesian framework, the weights found are slightly different "
"to the ones found by :ref:`ordinary_least_squares`. However, Bayesian "
"Ridge Regression is more robust to ill-posed problem."
msgstr ""

#: ../../modules/linear_model.rst:598
msgid ":ref:`example_linear_model_plot_bayesian_ridge.py`"
msgstr ""

#: ../../modules/linear_model.rst:602
msgid ""
"More details can be found in the article `Bayesian Interpolation "
"<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.9072&rep=rep1&type=pdf>`_"
" by MacKay, David J. C."
msgstr ""

#: ../../modules/linear_model.rst:609
msgid "Automatic Relevance Determination - ARD"
msgstr ""

#: ../../modules/linear_model.rst:611
msgid ""
":class:`ARDRegression` is very similar to `Bayesian Ridge Regression`_, "
"but can lead to sparser weights :math:`w` [1]_ [2]_. "
":class:`ARDRegression` poses a different prior over :math:`w`, by "
"dropping the assumption of the Gaussian being spherical."
msgstr ""

#: ../../modules/linear_model.rst:616
msgid ""
"Instead, the distribution over :math:`w` is assumed to be an axis-"
"parallel, elliptical Gaussian distribution."
msgstr ""

#: ../../modules/linear_model.rst:619
msgid ""
"This means each weight :math:`w_{i}` is drawn from a Gaussian "
"distribution, centered on zero and with a precision :math:`\\lambda_{i}`:"
msgstr ""

#: ../../modules/linear_model.rst:624
msgid ""
"with :math:`diag \\; (A) = \\lambda = "
"\\{\\lambda_{1},...,\\lambda_{p}\\}`."
msgstr ""

#: ../../modules/linear_model.rst:626
msgid ""
"In contrast to `Bayesian Ridge Regression`_, each coordinate of "
":math:`w_{i}` has its own standard deviation :math:`\\lambda_i`. The "
"prior over all :math:`\\lambda_i` is chosen to be the same gamma "
"distribution given by hyperparameters :math:`\\lambda_1` and "
":math:`\\lambda_2`."
msgstr ""

#: ../../modules/linear_model.rst:639
msgid ":ref:`example_linear_model_plot_ard.py`"
msgstr ""

#: ../../modules/linear_model.rst:643
msgid ""
"Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter "
"7.2.1"
msgstr ""

#: ../../modules/linear_model.rst:645
msgid ""
"David Wipf and Srikantan Nagarajan: `A new view of automatic relevance "
"determination. "
"<http://books.nips.cc/papers/files/nips20/NIPS2007_0976.pdf>`_"
msgstr ""

#: ../../modules/linear_model.rst:650
msgid "Logistic regression"
msgstr ""

#: ../../modules/linear_model.rst:652
msgid ""
"Logistic regression, despite its name, is a linear model for "
"classification rather than regression. Logistic regression is also known "
"in the literature as logit regression, maximum-entropy classification "
"(MaxEnt) or the log-linear classifier. In this model, the probabilities "
"describing the possible outcomes of a single trial are modeled using a "
"`logistic function <http://en.wikipedia.org/wiki/Logistic_function>`_."
msgstr ""

#: ../../modules/linear_model.rst:657
msgid ""
"The implementation of logistic regression in scikit-learn can be accessed"
" from class :class:`LogisticRegression`. This implementation can fit a "
"multiclass (one-vs-rest) logistic regression with optional L2 or L1 "
"regularization."
msgstr ""

#: ../../modules/linear_model.rst:662
msgid ""
"As an optimization problem, binary class L2 penalized logistic regression"
" minimizes the following cost function:"
msgstr ""

#: ../../modules/linear_model.rst:667
msgid ""
"Similarly, L1 regularized logistic regression solves the following "
"optimization problem"
msgstr ""

#: ../../modules/linear_model.rst:671
msgid ""
"The solvers implemented in the class :class:`LogisticRegression` are "
"\"liblinear\" (which is a wrapper around the C++ library, LIBLINEAR), "
"\"newton-cg\", \"lbfgs\" and \"sag\"."
msgstr ""

#: ../../modules/linear_model.rst:675
msgid ""
"The \"lbfgs\" and \"newton-cg\" solvers only support L2 penalization and "
"are found to converge faster for some high dimensional data. L1 "
"penalization yields sparse predicting weights."
msgstr ""

#: ../../modules/linear_model.rst:679
msgid ""
"The solver \"liblinear\" uses a coordinate descent (CD) algorithm based "
"on Liblinear. For L1 penalization :func:`sklearn.svm.l1_min_c` allows to "
"calculate the lower bound for C in order to get a non \"null\" (all "
"feature weights to zero) model. This relies on the excellent `LIBLINEAR "
"library <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`_, which is "
"shipped with scikit-learn. However, the CD algorithm implemented in "
"liblinear cannot learn a true multinomial (multiclass) model; instead, "
"the optimization problem is decomposed in a \"one-vs-rest\" fashion so "
"separate binary classifiers are trained for all classes. This happens "
"under the hood, so :class:`LogisticRegression` instances using this "
"solver behave as multiclass classifiers."
msgstr ""

#: ../../modules/linear_model.rst:691
msgid ""
"Setting `multi_class` to \"multinomial\" with the \"lbfgs\" or \"newton-"
"cg\" solver in :class:`LogisticRegression` learns a true multinomial "
"logistic regression model, which means that its probability estimates "
"should be better calibrated than the default \"one-vs-rest\" setting. "
"\"lbfgs\", \"newton-cg\" and \"sag\" solvers cannot optimize L1-penalized"
" models, though, so the \"multinomial\" setting does not learn sparse "
"models."
msgstr ""

#: ../../modules/linear_model.rst:697
msgid ""
"The solver \"sag\" uses a Stochastic Average Gradient descent [3]_. It "
"does not handle \"multinomial\" case, and is limited to L2-penalized "
"models, yet it is often faster than other solvers for large datasets, "
"when both the number of samples and the number of features are large."
msgstr ""

#: ../../modules/linear_model.rst:702
msgid "In a nutshell, one may choose the solver with the following rules:"
msgstr ""

#: ../../modules/linear_model.rst:705
msgid "Case"
msgstr ""

#: ../../modules/linear_model.rst:705
msgid "Solver"
msgstr ""

#: ../../modules/linear_model.rst:707
msgid "Small dataset or L1 penalty"
msgstr ""

#: ../../modules/linear_model.rst:707
msgid "\"liblinear\""
msgstr ""

#: ../../modules/linear_model.rst:708
msgid "Multinomial loss"
msgstr ""

#: ../../modules/linear_model.rst:708
msgid "\"lbfgs\" or newton-cg\""
msgstr ""

#: ../../modules/linear_model.rst:709
msgid "Large dataset"
msgstr ""

#: ../../modules/linear_model.rst:709
msgid "\"sag\""
msgstr ""

#: ../../modules/linear_model.rst:712
msgid ""
"For large dataset, you may also consider using :class:`SGDClassifier` "
"with 'log' loss."
msgstr ""

#: ../../modules/linear_model.rst:716
msgid ":ref:`example_linear_model_plot_logistic_l1_l2_sparsity.py`"
msgstr ""

#: ../../modules/linear_model.rst:718
msgid ":ref:`example_linear_model_plot_logistic_path.py`"
msgstr ""

#: ../../modules/linear_model.rst
msgid "Differences from liblinear:"
msgstr ""

#: ../../modules/linear_model.rst:724
msgid ""
"There might be a difference in the scores obtained between "
":class:`LogisticRegression` with ``solver=liblinear`` or "
":class:`LinearSVC` and the external liblinear library directly, when "
"``fit_intercept=False`` and the fit ``coef_`` (or) the data to be "
"predicted are zeroes. This is because for the sample(s) with "
"``decision_function`` zero, :class:`LogisticRegression` and "
":class:`LinearSVC` predict the negative class, while liblinear predicts "
"the positive class. Note that a model with ``fit_intercept=False`` and "
"having many samples with ``decision_function`` zero, is likely to be a "
"underfit, bad model and you are advised to set ``fit_intercept=True`` and"
" increase the intercept_scaling."
msgstr ""

#: ../../modules/linear_model.rst:735
msgid "**Feature selection with sparse logistic regression**"
msgstr ""

#: ../../modules/linear_model.rst:737
msgid ""
"A logistic regression with L1 penalty yields sparse models, and can thus "
"be used to perform feature selection, as detailed in "
":ref:`l1_feature_selection`."
msgstr ""

#: ../../modules/linear_model.rst:741
msgid ""
":class:`LogisticRegressionCV` implements Logistic Regression with builtin"
" cross-validation to find out the optimal C parameter. \"newton-cg\", "
"\"sag\" and \"lbfgs\" solvers are found to be faster for high-dimensional"
" dense data, due to warm-starting. For the multiclass case, if "
"`multi_class` option is set to \"ovr\", an optimal C is obtained for each"
" class and if the `multi_class` option is set to \"multinomial\", an "
"optimal C is obtained that minimizes the cross-entropy loss."
msgstr ""

#: ../../modules/linear_model.rst:752
msgid ""
"Mark Schmidt, Nicolas Le Roux, and Francis Bach: `Minimizing Finite Sums "
"with the Stochastic Average Gradient. "
"<http://hal.inria.fr/hal-00860051/PDF/sag_journal.pdf>`_"
msgstr ""

#: ../../modules/linear_model.rst:755
msgid "Stochastic Gradient Descent - SGD"
msgstr ""

#: ../../modules/linear_model.rst:757
msgid ""
"Stochastic gradient descent is a simple yet very efficient approach to "
"fit linear models. It is particularly useful when the number of samples "
"(and the number of features) is very large. The ``partial_fit`` method "
"allows only/out-of-core learning."
msgstr ""

#: ../../modules/linear_model.rst:762
msgid ""
"The classes :class:`SGDClassifier` and :class:`SGDRegressor` provide "
"functionality to fit linear models for classification and regression "
"using different (convex) loss functions and different penalties. E.g., "
"with ``loss=\"log\"``, :class:`SGDClassifier` fits a logistic regression "
"model, while with ``loss=\"hinge\"`` it fits a linear support vector "
"machine (SVM)."
msgstr ""

#: ../../modules/linear_model.rst:771
msgid ":ref:`sgd`"
msgstr ""

#: ../../modules/linear_model.rst:776
msgid "Perceptron"
msgstr ""

#: ../../modules/linear_model.rst:778
msgid ""
"The :class:`Perceptron` is another simple algorithm suitable for large "
"scale learning. By default:"
msgstr ""

#: ../../modules/linear_model.rst:781
msgid "It does not require a learning rate."
msgstr ""

#: ../../modules/linear_model.rst:783
msgid "It is not regularized (penalized)."
msgstr ""

#: ../../modules/linear_model.rst:785
msgid "It updates its model only on mistakes."
msgstr ""

#: ../../modules/linear_model.rst:787
msgid ""
"The last characteristic implies that the Perceptron is slightly faster to"
" train than SGD with the hinge loss and that the resulting models are "
"sparser."
msgstr ""

#: ../../modules/linear_model.rst:794
msgid "Passive Aggressive Algorithms"
msgstr ""

#: ../../modules/linear_model.rst:796
msgid ""
"The passive-aggressive algorithms are a family of algorithms for large-"
"scale learning. They are similar to the Perceptron in that they do not "
"require a learning rate. However, contrary to the Perceptron, they "
"include a regularization parameter ``C``."
msgstr ""

#: ../../modules/linear_model.rst:801
msgid ""
"For classification, :class:`PassiveAggressiveClassifier` can be used with"
" ``loss='hinge'`` (PA-I) or ``loss='squared_hinge'`` (PA-II).  For "
"regression, :class:`PassiveAggressiveRegressor` can be used with "
"``loss='epsilon_insensitive'`` (PA-I) or "
"``loss='squared_epsilon_insensitive'`` (PA-II)."
msgstr ""

#: ../../modules/linear_model.rst:810
msgid ""
"`\"Online Passive-Aggressive Algorithms\" "
"<http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf>`_ K."
" Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 "
"(2006)"
msgstr ""

#: ../../modules/linear_model.rst:816
msgid "Robustness regression: outliers and modeling errors"
msgstr ""

#: ../../modules/linear_model.rst:818
msgid ""
"Robust regression is interested in fitting a regression model in the "
"presence of corrupt data: either outliers, or error in the model."
msgstr ""

#: ../../modules/linear_model.rst:827
msgid "Different scenario and useful concepts"
msgstr ""

#: ../../modules/linear_model.rst:829
msgid ""
"There are different things to keep in mind when dealing with data "
"corrupted by outliers:"
msgstr ""

#: ../../modules/linear_model.rst:844
msgid "**Outliers in X or in y**?"
msgstr ""

#: ../../modules/linear_model.rst:847
msgid "Outliers in the y direction"
msgstr ""

#: ../../modules/linear_model.rst:847
msgid "Outliers in the X direction"
msgstr ""

#: ../../modules/linear_model.rst:849 ../../modules/linear_model.rst:860
msgid "|y_outliers|"
msgstr ""

#: ../../modules/linear_model.rst:849
msgid "|X_outliers|"
msgstr ""

#: ../../modules/linear_model.rst:852
msgid "**Fraction of outliers versus amplitude of error**"
msgstr ""

#: ../../modules/linear_model.rst:854
msgid ""
"The number of outlying points matters, but also how much they are "
"outliers."
msgstr ""

#: ../../modules/linear_model.rst:858
msgid "Small outliers"
msgstr ""

#: ../../modules/linear_model.rst:858
msgid "Large outliers"
msgstr ""

#: ../../modules/linear_model.rst:860
msgid "|large_y_outliers|"
msgstr ""

#: ../../modules/linear_model.rst:863
msgid ""
"An important notion of robust fitting is that of breakdown point: the "
"fraction of data that can be outlying for the fit to start missing the "
"inlying data."
msgstr ""

#: ../../modules/linear_model.rst:867
msgid ""
"Note that in general, robust fitting in high-dimensional setting (large "
"`n_features`) is very hard. The robust models here will probably not work"
" in these settings."
msgstr ""

#: ../../modules/linear_model.rst
msgid "**Trade-offs: which estimator?**"
msgstr ""

#: ../../modules/linear_model.rst:874
msgid ""
"Scikit-learn provides 2 robust regression estimators: :ref:`RANSAC "
"<ransac_regression>` and :ref:`Theil Sen <theil_sen_regression>`"
msgstr ""

#: ../../modules/linear_model.rst:878
msgid ""
":ref:`RANSAC <ransac_regression>` is faster, and scales much better with "
"the number of samples"
msgstr ""

#: ../../modules/linear_model.rst:881
msgid ""
":ref:`RANSAC <ransac_regression>` will deal better with large outliers in"
" the y direction (most common situation)"
msgstr ""

#: ../../modules/linear_model.rst:884
msgid ""
":ref:`Theil Sen <theil_sen_regression>` will cope better with medium-size"
" outliers in the X direction, but this property will disappear in large "
"dimensional settings."
msgstr ""

#: ../../modules/linear_model.rst:888
msgid "When in doubt, use :ref:`RANSAC <ransac_regression>`"
msgstr ""

#: ../../modules/linear_model.rst:893
msgid "RANSAC: RANdom SAmple Consensus"
msgstr ""

#: ../../modules/linear_model.rst:895
msgid ""
"RANSAC (RANdom SAmple Consensus) fits a model from random subsets of "
"inliers from the complete data set."
msgstr ""

#: ../../modules/linear_model.rst:898
msgid ""
"RANSAC is a non-deterministic algorithm producing only a reasonable "
"result with a certain probability, which is dependent on the number of "
"iterations (see `max_trials` parameter). It is typically used for linear "
"and non-linear regression problems and is especially popular in the "
"fields of photogrammetric computer vision."
msgstr ""

#: ../../modules/linear_model.rst:904
msgid ""
"The algorithm splits the complete input sample data into a set of "
"inliers, which may be subject to noise, and outliers, which are e.g. "
"caused by erroneous measurements or invalid hypotheses about the data. "
"The resulting model is then estimated only from the determined inliers."
msgstr ""

#: ../../modules/linear_model.rst:915
msgid "Details of the algorithm"
msgstr ""

#: ../../modules/linear_model.rst:917
msgid "Each iteration performs the following steps:"
msgstr ""

#: ../../modules/linear_model.rst:919
msgid ""
"Select ``min_samples`` random samples from the original data and check "
"whether the set of data is valid (see ``is_data_valid``)."
msgstr ""

#: ../../modules/linear_model.rst:921
msgid ""
"Fit a model to the random subset (``base_estimator.fit``) and check "
"whether the estimated model is valid (see ``is_model_valid``)."
msgstr ""

#: ../../modules/linear_model.rst:923
msgid ""
"Classify all data as inliers or outliers by calculating the residuals to "
"the estimated model (``base_estimator.predict(X) - y``) - all data "
"samples with absolute residuals smaller than the ``residual_threshold`` "
"are considered as inliers."
msgstr ""

#: ../../modules/linear_model.rst:927
msgid ""
"Save fitted model as best model if number of inlier samples is maximal. "
"In case the current estimated model has the same number of inliers, it is"
" only considered as the best model if it has better score."
msgstr ""

#: ../../modules/linear_model.rst:931
msgid ""
"These steps are performed either a maximum number of times "
"(``max_trials``) or until one of the special stop criteria are met (see "
"``stop_n_inliers`` and ``stop_score``). The final model is estimated "
"using all inlier samples (consensus set) of the previously determined "
"best model."
msgstr ""

#: ../../modules/linear_model.rst:936
msgid ""
"The ``is_data_valid`` and ``is_model_valid`` functions allow to identify "
"and reject degenerate combinations of random sub-samples. If the "
"estimated model is not needed for identifying degenerate cases, "
"``is_data_valid`` should be used as it is called prior to fitting the "
"model and thus leading to better computational performance."
msgstr ""

#: ../../modules/linear_model.rst:945
msgid ":ref:`example_linear_model_plot_ransac.py`"
msgstr ""

#: ../../modules/linear_model.rst:946 ../../modules/linear_model.rst:973
msgid ":ref:`example_linear_model_plot_robust_fit.py`"
msgstr ""

#: ../../modules/linear_model.rst:950
msgid "http://en.wikipedia.org/wiki/RANSAC"
msgstr ""

#: ../../modules/linear_model.rst:951
msgid ""
"`\"Random Sample Consensus: A Paradigm for Model Fitting with "
"Applications to Image Analysis and Automated Cartography\" "
"<http://www.cs.columbia.edu/~belhumeur/courses/compPhoto/ransac.pdf>`_ "
"Martin A. Fischler and Robert C. Bolles - SRI International (1981)"
msgstr ""

#: ../../modules/linear_model.rst:955
msgid ""
"`\"Performance Evaluation of RANSAC Family\" "
"<http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf>`_ Sunglok "
"Choi, Taemin Kim and Wonpil Yu - BMVC (2009)"
msgstr ""

#: ../../modules/linear_model.rst:962
msgid "Theil-Sen estimator: generalized-median-based estimator"
msgstr ""

#: ../../modules/linear_model.rst:964
msgid ""
"The :class:`TheilSenRegressor` estimator uses a generalization of the "
"median in multiple dimensions. It is thus robust to multivariate "
"outliers. Note however that the robustness of the estimator decreases "
"quickly with the dimensionality of the problem. It looses its robustness "
"properties and becomes no better than an ordinary least squares in high "
"dimension."
msgstr ""

#: ../../modules/linear_model.rst:972 ../../modules/linear_model.rst:1014
msgid ":ref:`example_linear_model_plot_theilsen.py`"
msgstr ""

#: ../../modules/linear_model.rst:977
#, python-format
msgid "http://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator"
msgstr ""

#: ../../modules/linear_model.rst:980
msgid "Theoretical considerations"
msgstr ""

#: ../../modules/linear_model.rst:982
#, python-format
msgid ""
":class:`TheilSenRegressor` is comparable to the :ref:`Ordinary Least "
"Squares (OLS) <ordinary_least_squares>` in terms of asymptotic efficiency"
" and as an unbiased estimator. In contrast to OLS, Theil-Sen is a non-"
"parametric method which means it makes no assumption about the underlying"
" distribution of the data. Since Theil-Sen is a median-based estimator, "
"it is more robust against corrupted data aka outliers. In univariate "
"setting, Theil-Sen has a breakdown point of about 29.3% in case of a "
"simple linear regression which means that it can tolerate arbitrary "
"corrupted data of up to 29.3%."
msgstr ""

#: ../../modules/linear_model.rst:997
msgid ""
"The implementation of :class:`TheilSenRegressor` in scikit-learn follows "
"a generalization to a multivariate linear regression model [#f1]_ using "
"the spatial median which is a generalization of the median to multiple "
"dimensions [#f2]_."
msgstr ""

#: ../../modules/linear_model.rst:1002
msgid "In terms of time and space complexity, Theil-Sen scales according to"
msgstr ""

#: ../../modules/linear_model.rst:1007
msgid ""
"which makes it infeasible to be applied exhaustively to problems with a "
"large number of samples and features. Therefore, the magnitude of a "
"subpopulation can be chosen to limit the time and space complexity by "
"considering only a random subset of all possible combinations."
msgstr ""

#: ../../modules/linear_model.rst:1018
msgid ""
"Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: `Theil-Sen "
"Estimators in a Multiple Linear Regression Model. "
"<http://www.math.iupui.edu/~hpeng/MTSE_0908.pdf>`_"
msgstr ""

#: ../../modules/linear_model.rst:1020
msgid ""
"Kärkkäinen and S. Äyrämö: `On Computation of Spatial Median for Robust "
"Data Mining. <http://users.jyu.fi/~samiayr/pdf/ayramo_eurogen05.pdf>`_"
msgstr ""

#: ../../modules/linear_model.rst:1025
msgid "Polynomial regression: extending linear models with basis functions"
msgstr ""

#: ../../modules/linear_model.rst:1029
msgid ""
"One common pattern within machine learning is to use linear models "
"trained on nonlinear functions of the data.  This approach maintains the "
"generally fast performance of linear methods, while allowing them to fit "
"a much wider range of data."
msgstr ""

#: ../../modules/linear_model.rst:1034
msgid ""
"For example, a simple linear regression can be extended by constructing "
"**polynomial features** from the coefficients.  In the standard linear "
"regression case, you might have a model that looks like this for two-"
"dimensional data:"
msgstr ""

#: ../../modules/linear_model.rst:1041
msgid ""
"If we want to fit a paraboloid to the data instead of a plane, we can "
"combine the features in second-order polynomials, so that the model looks"
" like this:"
msgstr ""

#: ../../modules/linear_model.rst:1046
msgid ""
"The (sometimes surprising) observation is that this is *still a linear "
"model*: to see this, imagine creating a new variable"
msgstr ""

#: ../../modules/linear_model.rst:1051
msgid "With this re-labeling of the data, our problem can be written"
msgstr ""

#: ../../modules/linear_model.rst:1055
msgid ""
"We see that the resulting *polynomial regression* is in the same class of"
" linear models we'd considered above (i.e. the model is linear in "
":math:`w`) and can be solved by the same techniques.  By considering "
"linear fits within a higher-dimensional space built with these basis "
"functions, the model has the flexibility to fit a much broader range of "
"data."
msgstr ""

#: ../../modules/linear_model.rst:1061
msgid ""
"Here is an example of applying this idea to one-dimensional data, using "
"polynomial features of varying degrees:"
msgstr ""

#: ../../modules/linear_model.rst:1069
msgid ""
"This figure is created using the :class:`PolynomialFeatures` "
"preprocessor. This preprocessor transforms an input data matrix into a "
"new data matrix of a given degree.  It can be used as follows::"
msgstr ""

#: ../../modules/linear_model.rst:1086
msgid ""
"The features of ``X`` have been transformed from :math:`[x_1, x_2]` to "
":math:`[1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]`, and can now be used within "
"any linear model."
msgstr ""

#: ../../modules/linear_model.rst:1090
msgid ""
"This sort of preprocessing can be streamlined with the :ref:`Pipeline "
"<pipeline>` tools. A single object representing a simple polynomial "
"regression can be created and used as follows::"
msgstr ""

#: ../../modules/linear_model.rst:1107
msgid ""
"The linear model trained on polynomial features is able to exactly "
"recover the input polynomial coefficients."
msgstr ""

#: ../../modules/linear_model.rst:1110
msgid ""
"In some cases it's not necessary to include higher powers of any single "
"feature, but only the so-called *interaction features* that multiply "
"together at most :math:`d` distinct features. These can be gotten from "
":class:`PolynomialFeatures` with the setting ``interaction_only=True``."
msgstr ""

#: ../../modules/linear_model.rst:1116
msgid ""
"For example, when dealing with boolean features, :math:`x_i^n = x_i` for "
"all :math:`n` and is therefore useless; but :math:`x_i x_j` represents "
"the conjunction of two booleans. This way, we can solve the XOR problem "
"with a linear classifier::"
msgstr ""

