# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/model_evaluation.rst:7
msgid "Model evaluation: quantifying the quality of predictions"
msgstr ""

#: ../../modules/model_evaluation.rst:9
msgid ""
"There are 3 different approaches to evaluate the quality of predictions "
"of a model:"
msgstr ""

#: ../../modules/model_evaluation.rst:12
msgid ""
"**Estimator score method**: Estimators have a ``score`` method providing "
"a default evaluation criterion for the problem they are designed to "
"solve. This is not discussed on this page, but in each estimator's "
"documentation."
msgstr ""

#: ../../modules/model_evaluation.rst:16
msgid ""
"**Scoring parameter**: Model-evaluation tools using :ref:`cross-"
"validation <cross_validation>` (such as "
":func:`cross_validation.cross_val_score` and "
":class:`grid_search.GridSearchCV`) rely on an internal *scoring* "
"strategy. This is discussed in the section :ref:`scoring_parameter`."
msgstr ""

#: ../../modules/model_evaluation.rst:22
msgid ""
"**Metric functions**: The :mod:`metrics` module implements functions "
"assessing prediction error for specific purposes. These metrics are "
"detailed in sections on :ref:`classification_metrics`, "
":ref:`multilabel_ranking_metrics`, :ref:`regression_metrics` and "
":ref:`clustering_metrics`."
msgstr ""

#: ../../modules/model_evaluation.rst:28
msgid ""
"Finally, :ref:`dummy_estimators` are useful to get a baseline value of "
"those metrics for random predictions."
msgstr ""

#: ../../modules/model_evaluation.rst:33
msgid ""
"For \"pairwise\" metrics, between *samples* and not estimators or "
"predictions, see the :ref:`metrics` section."
msgstr ""

#: ../../modules/model_evaluation.rst:39
msgid "The ``scoring`` parameter: defining model evaluation rules"
msgstr ""

#: ../../modules/model_evaluation.rst:41
msgid ""
"Model selection and evaluation using tools, such as "
":class:`grid_search.GridSearchCV` and "
":func:`cross_validation.cross_val_score`, take a ``scoring`` parameter "
"that controls what metric they apply to the estimators evaluated."
msgstr ""

#: ../../modules/model_evaluation.rst:47
msgid "Common cases: predefined values"
msgstr ""

#: ../../modules/model_evaluation.rst:49
msgid ""
"For the most common use cases, you can designate a scorer object with the"
" ``scoring`` parameter; the table below shows all possible values. All "
"scorer ojects follow the convention that higher return values are better "
"than lower return values.  Thus the returns from mean_absolute_error and "
"mean_squared_error, which measure the distance between the model and the "
"data, are negated."
msgstr ""

#: ../../modules/model_evaluation.rst:58
msgid "Scoring"
msgstr ""

#: ../../modules/model_evaluation.rst:58
msgid "Function"
msgstr ""

#: ../../modules/model_evaluation.rst:58
msgid "Comment"
msgstr ""

#: ../../modules/model_evaluation.rst:60
msgid "**Classification**"
msgstr ""

#: ../../modules/model_evaluation.rst:61
msgid "'accuracy'"
msgstr ""

#: ../../modules/model_evaluation.rst:61
msgid ":func:`metrics.accuracy_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:62
msgid "'average_precision'"
msgstr ""

#: ../../modules/model_evaluation.rst:62
msgid ":func:`metrics.average_precision_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:63
msgid "'f1'"
msgstr ""

#: ../../modules/model_evaluation.rst:63 ../../modules/model_evaluation.rst:64
#: ../../modules/model_evaluation.rst:65 ../../modules/model_evaluation.rst:66
#: ../../modules/model_evaluation.rst:67
msgid ":func:`metrics.f1_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:63
msgid "for binary targets"
msgstr ""

#: ../../modules/model_evaluation.rst:64
msgid "'f1_micro'"
msgstr ""

#: ../../modules/model_evaluation.rst:64
msgid "micro-averaged"
msgstr ""

#: ../../modules/model_evaluation.rst:65
msgid "'f1_macro'"
msgstr ""

#: ../../modules/model_evaluation.rst:65
msgid "macro-averaged"
msgstr ""

#: ../../modules/model_evaluation.rst:66
msgid "'f1_weighted'"
msgstr ""

#: ../../modules/model_evaluation.rst:66
msgid "weighted average"
msgstr ""

#: ../../modules/model_evaluation.rst:67
msgid "'f1_samples'"
msgstr ""

#: ../../modules/model_evaluation.rst:67
msgid "by multilabel sample"
msgstr ""

#: ../../modules/model_evaluation.rst:68
msgid "'log_loss'"
msgstr ""

#: ../../modules/model_evaluation.rst:68
msgid ":func:`metrics.log_loss`"
msgstr ""

#: ../../modules/model_evaluation.rst:68
msgid "requires ``predict_proba`` support"
msgstr ""

#: ../../modules/model_evaluation.rst:69
msgid "'precision' etc."
msgstr ""

#: ../../modules/model_evaluation.rst:69
msgid ":func:`metrics.precision_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:69 ../../modules/model_evaluation.rst:70
msgid "suffixes apply as with 'f1'"
msgstr ""

#: ../../modules/model_evaluation.rst:70
msgid "'recall' etc."
msgstr ""

#: ../../modules/model_evaluation.rst:70
msgid ":func:`metrics.recall_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:71
msgid "'roc_auc'"
msgstr ""

#: ../../modules/model_evaluation.rst:71
msgid ":func:`metrics.roc_auc_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:73
msgid "**Clustering**"
msgstr ""

#: ../../modules/model_evaluation.rst:74
msgid "'adjusted_rand_score'"
msgstr ""

#: ../../modules/model_evaluation.rst:74
msgid ":func:`metrics.adjusted_rand_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:76
msgid "**Regression**"
msgstr ""

#: ../../modules/model_evaluation.rst:77
msgid "'mean_absolute_error'"
msgstr ""

#: ../../modules/model_evaluation.rst:77
msgid ":func:`metrics.mean_absolute_error`"
msgstr ""

#: ../../modules/model_evaluation.rst:78
msgid "'mean_squared_error'"
msgstr ""

#: ../../modules/model_evaluation.rst:78
msgid ":func:`metrics.mean_squared_error`"
msgstr ""

#: ../../modules/model_evaluation.rst:79
msgid "'median_absolute_error'"
msgstr ""

#: ../../modules/model_evaluation.rst:79
msgid ":func:`metrics.median_absolute_error`"
msgstr ""

#: ../../modules/model_evaluation.rst:80
msgid "'r2'"
msgstr ""

#: ../../modules/model_evaluation.rst:80
msgid ":func:`metrics.r2_score`"
msgstr ""

#: ../../modules/model_evaluation.rst:83
msgid "Usage examples:"
msgstr ""

#: ../../modules/model_evaluation.rst:98
msgid ""
"The values listed by the ValueError exception correspond to the functions"
" measuring prediction accuracy described in the following sections. The "
"scorer objects for those functions are stored in the dictionary "
"``sklearn.metrics.SCORERS``."
msgstr ""

#: ../../modules/model_evaluation.rst:108
msgid "Defining your scoring strategy from metric functions"
msgstr ""

#: ../../modules/model_evaluation.rst:110
msgid ""
"The module :mod:`sklearn.metric` also exposes a set of simple functions "
"measuring a prediction error given ground truth and prediction:"
msgstr ""

#: ../../modules/model_evaluation.rst:113
msgid ""
"functions ending with ``_score`` return a value to maximize, the higher "
"the better."
msgstr ""

#: ../../modules/model_evaluation.rst:116
msgid ""
"functions ending with ``_error`` or ``_loss`` return a value to minimize,"
" the lower the better.  When converting into a scorer object using "
":func:`make_scorer`, set the ``greater_is_better`` parameter to False "
"(True by default; see the parameter description below)."
msgstr ""

#: ../../modules/model_evaluation.rst:122
msgid ""
"Metrics available for various machine learning tasks are detailed in "
"sections below."
msgstr ""

#: ../../modules/model_evaluation.rst:125
msgid ""
"Many metrics are not given names to be used as ``scoring`` values, "
"sometimes because they require additional parameters, such as "
":func:`fbeta_score`. In such cases, you need to generate an appropriate "
"scoring object.  The simplest way to generate a callable object for "
"scoring is by using :func:`make_scorer`. That function converts metrics "
"into callables that can be used for model evaluation."
msgstr ""

#: ../../modules/model_evaluation.rst:132
msgid ""
"One typical use case is to wrap an existing metric function from the "
"library with non-default values for its parameters, such as the ``beta`` "
"parameter for the :func:`fbeta_score` function::"
msgstr ""

#: ../../modules/model_evaluation.rst:142
msgid ""
"The second use case is to build a completely custom scorer object from a "
"simple python function using :func:`make_scorer`, which can take several "
"parameters:"
msgstr ""

#: ../../modules/model_evaluation.rst:146
msgid ""
"the python function you want to use (``my_custom_loss_func`` in the "
"example below)"
msgstr ""

#: ../../modules/model_evaluation.rst:149
msgid ""
"whether the python function returns a score (``greater_is_better=True``, "
"the default) or a loss (``greater_is_better=False``).  If a loss, the "
"output of the python function is negated by the scorer object, conforming"
" to the cross validation convention that scorers return higher values for"
" better models."
msgstr ""

#: ../../modules/model_evaluation.rst:154
msgid ""
"for classification metrics only: whether the python function you provided"
" requires continuous decision certainties (``needs_threshold=True``).  "
"The default value is False."
msgstr ""

#: ../../modules/model_evaluation.rst:158
msgid ""
"any additional parameters, such as ``beta`` or ``labels`` in "
":func:`f1_score`."
msgstr ""

#: ../../modules/model_evaluation.rst:160
msgid ""
"Here is an example of building custom scorers, and of using the "
"``greater_is_better`` parameter::"
msgstr ""

#: ../../modules/model_evaluation.rst:187
msgid "Implementing your own scoring object"
msgstr ""

#: ../../modules/model_evaluation.rst:188
msgid ""
"You can generate even more flexible model scorers by constructing your "
"own scoring object from scratch, without using the :func:`make_scorer` "
"factory. For a callable to be a scorer, it needs to meet the protocol "
"specified by the following two rules:"
msgstr ""

#: ../../modules/model_evaluation.rst:193
msgid ""
"It can be called with parameters ``(estimator, X, y)``, where "
"``estimator`` is the model that should be evaluated, ``X`` is validation "
"data, and ``y`` is the ground truth target for ``X`` (in the supervised "
"case) or ``None`` (in the unsupervised case)."
msgstr ""

#: ../../modules/model_evaluation.rst:198
msgid ""
"It returns a floating point number that quantifies the ``estimator`` "
"prediction quality on ``X``, with reference to ``y``. Again, by "
"convention higher numbers are better, so if your scorer returns loss, "
"that value should be negated."
msgstr ""

#: ../../modules/model_evaluation.rst:207
msgid "Classification metrics"
msgstr ""

#: ../../modules/model_evaluation.rst:211
msgid ""
"The :mod:`sklearn.metrics` module implements several loss, score, and "
"utility functions to measure classification performance. Some metrics "
"might require probability estimates of the positive class, confidence "
"values, or binary decisions values. Most implementations allow each "
"sample to provide a weighted contribution to the overall score, through "
"the ``sample_weight`` parameter."
msgstr ""

#: ../../modules/model_evaluation.rst:218
msgid "Some of these are restricted to the binary classification case:"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`matthews_corrcoef <sklearn.metrics.matthews_corrcoef>`\\ (y_true, "
"y_pred)"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute the Matthews correlation coefficient (MCC) for binary classes"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`precision_recall_curve <sklearn.metrics.precision_recall_curve>`\\ "
"(y_true, probas_pred)"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute precision-recall pairs for different probability thresholds"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`roc_curve <sklearn.metrics.roc_curve>`\\ (y_true, y_score[, "
"pos_label, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute Receiver operating characteristic (ROC)"
msgstr ""

#: ../../modules/model_evaluation.rst:228
msgid "Others also work in the multiclass case:"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`confusion_matrix <sklearn.metrics.confusion_matrix>`\\ (y_true, "
"y_pred[, labels])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute confusion matrix to evaluate the accuracy of a classification"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`hinge_loss <sklearn.metrics.hinge_loss>`\\ (y_true, pred_decision[,"
" labels, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Average hinge loss (non-regularized)"
msgstr ""

#: ../../modules/model_evaluation.rst:237
msgid "Some also work in the multilabel case:"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`accuracy_score <sklearn.metrics.accuracy_score>`\\ (y_true, "
"y_pred[, normalize, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Accuracy classification score."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`classification_report <sklearn.metrics.classification_report>`\\ "
"(y_true, y_pred[, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Build a text report showing the main classification metrics"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`f1_score <sklearn.metrics.f1_score>`\\ (y_true, y_pred[, labels, "
"...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute the F1 score, also known as balanced F-score or F-measure"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`fbeta_score <sklearn.metrics.fbeta_score>`\\ (y_true, y_pred, "
"beta[, labels, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute the F-beta score"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`hamming_loss <sklearn.metrics.hamming_loss>`\\ (y_true, y_pred[, "
"classes])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute the average Hamming loss."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`jaccard_similarity_score "
"<sklearn.metrics.jaccard_similarity_score>`\\ (y_true, y_pred[, ...])"
msgstr ""

#: ../../<autosummary>:1 ../../modules/model_evaluation.rst:499
msgid "Jaccard similarity coefficient score"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`log_loss <sklearn.metrics.log_loss>`\\ (y_true, y_pred[, eps, "
"normalize, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Log loss, aka logistic loss or cross-entropy loss."
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`precision_recall_fscore_support "
"<sklearn.metrics.precision_recall_fscore_support>`\\ (y_true, y_pred)"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute precision, recall, F-measure and support for each class"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`precision_score <sklearn.metrics.precision_score>`\\ (y_true, "
"y_pred[, labels, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute the precision"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`recall_score <sklearn.metrics.recall_score>`\\ (y_true, y_pred[, "
"labels, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute the recall"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`zero_one_loss <sklearn.metrics.zero_one_loss>`\\ (y_true, y_pred[, "
"normalize, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Zero-one classification loss."
msgstr ""

#: ../../modules/model_evaluation.rst:254
msgid "And some work with binary and multilabel (but not multiclass) problems:"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`average_precision_score "
"<sklearn.metrics.average_precision_score>`\\ (y_true, y_score[, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute average precision (AP) from prediction scores"
msgstr ""

#: ../../<autosummary>:1
msgid ""
":obj:`roc_auc_score <sklearn.metrics.roc_auc_score>`\\ (y_true, y_score[,"
" average, ...])"
msgstr ""

#: ../../<autosummary>:1
msgid "Compute Area Under the Curve (AUC) from prediction scores"
msgstr ""

#: ../../modules/model_evaluation.rst:263
msgid ""
"In the following sub-sections, we will describe each of those functions, "
"preceded by some notes on common API and metric definition."
msgstr ""

#: ../../modules/model_evaluation.rst:267
msgid "From binary to multiclass and multilabel"
msgstr ""

#: ../../modules/model_evaluation.rst:269
msgid ""
"Some metrics are essentially defined for binary classification tasks "
"(e.g. :func:`f1_score`, :func:`roc_auc_score`). In these cases, by "
"default only the positive label is evaluated, assuming by default that "
"the positive class is labelled ``1`` (though this may be configurable "
"through the ``pos_label`` parameter)."
msgstr ""

#: ../../modules/model_evaluation.rst:277
msgid ""
"In extending a binary metric to multiclass or multilabel problems, the "
"data is treated as a collection of binary problems, one for each class. "
"There are then a number of ways to average binary metric calculations "
"across the set of classes, each of which may be useful in some scenario. "
"Where available, you should select among these using the ``average`` "
"parameter."
msgstr ""

#: ../../modules/model_evaluation.rst:283
msgid ""
"``\"macro\"`` simply calculates the mean of the binary metrics, giving "
"equal weight to each class.  In problems where infrequent classes are "
"nonetheless important, macro-averaging may be a means of highlighting "
"their performance. On the other hand, the assumption that all classes are"
" equally important is often untrue, such that macro-averaging will over-"
"emphasize the typically low performance on an infrequent class."
msgstr ""

#: ../../modules/model_evaluation.rst:289
msgid ""
"``\"weighted\"`` accounts for class imbalance by computing the average of"
" binary metrics in which each class's score is weighted by its presence "
"in the true data sample."
msgstr ""

#: ../../modules/model_evaluation.rst:292
msgid ""
"``\"micro\"`` gives each sample-class pair an equal contribution to the "
"overall metric (except as a result of sample-weight). Rather than summing"
" the metric per class, this sums the dividends and divisors that make up "
"the the per-class metrics to calculate an overall quotient. Micro-"
"averaging may be preferred in multilabel settings, including multiclass "
"classification where a majority class is to be ignored."
msgstr ""

#: ../../modules/model_evaluation.rst:298
msgid ""
"``\"samples\"`` applies only to multilabel problems. It does not "
"calculate a per-class measure, instead calculating the metric over the "
"true and predicted classes for each sample in the evaluation data, and "
"returning their (``sample_weight``-weighted) average."
msgstr ""

#: ../../modules/model_evaluation.rst:302
msgid ""
"Selecting ``average=None`` will return an array with the score for each "
"class."
msgstr ""

#: ../../modules/model_evaluation.rst:305
msgid ""
"While multiclass data is provided to the metric, like binary targets, as "
"an array of class labels, multilabel data is specified as an indicator "
"matrix, in which cell ``[i, j]`` has value 1 if sample ``i`` has label "
"``j`` and value 0 otherwise."
msgstr ""

#: ../../modules/model_evaluation.rst:313
msgid "Accuracy score"
msgstr ""

#: ../../modules/model_evaluation.rst:315
msgid ""
"The :func:`accuracy_score` function computes the `accuracy "
"<http://en.wikipedia.org/wiki/Accuracy_and_precision>`_, either the "
"fraction (default) or the count (normalize=False) of correct predictions."
msgstr ""

#: ../../modules/model_evaluation.rst:320
msgid ""
"In multilabel classification, the function returns the subset accuracy. "
"If the entire set of predicted labels for a sample strictly match with "
"the true set of labels, then the subset accuracy is 1.0; otherwise it is "
"0.0."
msgstr ""

#: ../../modules/model_evaluation.rst:324
msgid ""
"If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample "
"and :math:`y_i` is the corresponding true value, then the fraction of "
"correct predictions over :math:`n_\\text{samples}` is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:333
#: ../../modules/model_evaluation.rst:1007
msgid ""
"where :math:`1(x)` is the `indicator function "
"<http://en.wikipedia.org/wiki/Indicator_function>`_."
msgstr ""

#: ../../modules/model_evaluation.rst:345
#: ../../modules/model_evaluation.rst:480
#: ../../modules/model_evaluation.rst:528
msgid "In the multilabel case with binary label indicators: ::"
msgstr ""

#: ../../modules/model_evaluation.rst
msgid "Example:"
msgstr ""

#: ../../modules/model_evaluation.rst:352
msgid ""
"See "
":ref:`example_feature_selection_plot_permutation_test_for_classification.py`"
" for an example of accuracy score usage using permutations of the "
"dataset."
msgstr ""

#: ../../modules/model_evaluation.rst:359
msgid "Cohen's kappa"
msgstr ""

#: ../../modules/model_evaluation.rst:361
msgid ""
"The function :func:`cohen_kappa_score` computes Cohen's kappa statistic. "
"This measure is intended to compare labelings by different human "
"annotators, not a classifier versus a ground truth."
msgstr ""

#: ../../modules/model_evaluation.rst:365
msgid ""
"The kappa score (see docstring) is a number between -1 and 1. Scores "
"above .8 are generally considered good agreement; zero or lower means no "
"agreement (practically random labels)."
msgstr ""

#: ../../modules/model_evaluation.rst:369
msgid ""
"Kappa scores can be computed for binary or multiclass problems, but not "
"for multilabel problems (except by manually computing a per-label score) "
"and not for more than two annotators."
msgstr ""

#: ../../modules/model_evaluation.rst:376
msgid "Confusion matrix"
msgstr ""

#: ../../modules/model_evaluation.rst:378
msgid ""
"The :func:`confusion_matrix` function evaluates classification accuracy "
"by computing the `confusion matrix "
"<http://en.wikipedia.org/wiki/Confusion_matrix>`_."
msgstr ""

#: ../../modules/model_evaluation.rst:382
msgid ""
"By definition, entry :math:`i, j` in a confusion matrix is the number of "
"observations actually in group :math:`i`, but predicted to be in group "
":math:`j`. Here is an example::"
msgstr ""

#: ../../modules/model_evaluation.rst:394
msgid ""
"Here is a visual representation of such a confusion matrix (this figure "
"comes from the :ref:`example_model_selection_plot_confusion_matrix.py` "
"example):"
msgstr ""

#: ../../modules/model_evaluation.rst:404
msgid ""
"See :ref:`example_model_selection_plot_confusion_matrix.py` for an "
"example of using a confusion matrix to evaluate classifier output "
"quality."
msgstr ""

#: ../../modules/model_evaluation.rst:408
msgid ""
"See :ref:`example_classification_plot_digits_classification.py` for an "
"example of using a confusion matrix to classify hand-written digits."
msgstr ""

#: ../../modules/model_evaluation.rst:412
msgid ""
"See :ref:`example_text_document_classification_20newsgroups.py` for an "
"example of using a confusion matrix to classify text documents."
msgstr ""

#: ../../modules/model_evaluation.rst:419
msgid "Classification report"
msgstr ""

#: ../../modules/model_evaluation.rst:421
msgid ""
"The :func:`classification_report` function builds a text report showing "
"the main classification metrics. Here is a small example with custom "
"``target_names`` and inferred labels::"
msgstr ""

#: ../../modules/model_evaluation.rst:441
msgid ""
"See :ref:`example_classification_plot_digits_classification.py` for an "
"example of classification report usage for hand-written digits."
msgstr ""

#: ../../modules/model_evaluation.rst:445
msgid ""
"See :ref:`example_text_document_classification_20newsgroups.py` for an "
"example of classification report usage for text documents."
msgstr ""

#: ../../modules/model_evaluation.rst:449
msgid ""
"See :ref:`example_model_selection_grid_search_digits.py` for an example "
"of classification report usage for grid search with nested cross-"
"validation."
msgstr ""

#: ../../modules/model_evaluation.rst:456
msgid "Hamming loss"
msgstr ""

#: ../../modules/model_evaluation.rst:458
msgid ""
"The :func:`hamming_loss` computes the average Hamming loss or `Hamming "
"distance <http://en.wikipedia.org/wiki/Hamming_distance>`_ between two "
"sets of samples."
msgstr ""

#: ../../modules/model_evaluation.rst:462
msgid ""
"If :math:`\\hat{y}_j` is the predicted value for the :math:`j`-th label "
"of a given sample, :math:`y_j` is the corresponding true value, and "
":math:`n_\\text{labels}` is the number of classes or labels, then the "
"Hamming loss :math:`L_{Hamming}` between two samples is defined as:"
msgstr ""

#: ../../modules/model_evaluation.rst:471
msgid ""
"where :math:`1(x)` is the `indicator function "
"<http://en.wikipedia.org/wiki/Indicator_function>`_. ::"
msgstr ""

#: ../../modules/model_evaluation.rst:487
msgid ""
"In multiclass classification, the Hamming loss corresponds to the Hamming"
" distance between ``y_true`` and ``y_pred`` which is similar to the "
":ref:`zero_one_loss` function.  However, while zero-one loss penalizes "
"prediction sets that do not strictly match true sets, the Hamming loss "
"penalizes individual labels.  Thus the Hamming loss, upper bounded by the"
" zero-one loss, is always between zero and one, inclusive; and predicting"
" a proper subset or superset of the true labels will give a Hamming loss "
"between zero and one, exclusive."
msgstr ""

#: ../../modules/model_evaluation.rst:501
msgid ""
"The :func:`jaccard_similarity_score` function computes the average "
"(default) or sum of `Jaccard similarity coefficients "
"<http://en.wikipedia.org/wiki/Jaccard_index>`_, also called the Jaccard "
"index, between pairs of label sets."
msgstr ""

#: ../../modules/model_evaluation.rst:506
msgid ""
"The Jaccard similarity coefficient of the :math:`i`-th samples, with a "
"ground truth label set :math:`y_i` and predicted label set "
":math:`\\hat{y}_i`, is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:514
msgid ""
"In binary and multiclass classification, the Jaccard similarity "
"coefficient score is equal to the classification accuracy."
msgstr ""

#: ../../modules/model_evaluation.rst:536
msgid "Precision, recall and F-measures"
msgstr ""

#: ../../modules/model_evaluation.rst:538
msgid ""
"Intuitively, `precision "
"<http://en.wikipedia.org/wiki/Precision_and_recall#Precision>`_ is the "
"ability of the classifier not to label as positive a sample that is "
"negative, and `recall "
"<http://en.wikipedia.org/wiki/Precision_and_recall#Recall>`_ is the "
"ability of the classifier to find all the positive samples."
msgstr ""

#: ../../modules/model_evaluation.rst:544
msgid ""
"The  `F-measure <http://en.wikipedia.org/wiki/F1_score>`_ "
"(:math:`F_\\beta` and :math:`F_1` measures) can be interpreted as a "
"weighted harmonic mean of the precision and recall. A :math:`F_\\beta` "
"measure reaches its best value at 1 and its worst score at 0. With "
":math:`\\beta = 1`,  :math:`F_\\beta` and :math:`F_1`  are equivalent, "
"and the recall and the precision are equally important."
msgstr ""

#: ../../modules/model_evaluation.rst:551
msgid ""
"The :func:`precision_recall_curve` computes a precision-recall curve from"
" the ground truth label and a score given by the classifier by varying a "
"decision threshold."
msgstr ""

#: ../../modules/model_evaluation.rst:555
msgid ""
"The :func:`average_precision_score` function computes the average "
"precision (AP) from prediction scores. This score corresponds to the area"
" under the precision-recall curve."
msgstr ""

#: ../../modules/model_evaluation.rst:559
msgid ""
"Several functions allow you to analyze the precision, recall and "
"F-measures score:"
msgstr ""

#: ../../modules/model_evaluation.rst:573
msgid ""
"Note that the :func:`precision_recall_curve` function is restricted to "
"the binary case. The :func:`average_precision_score` function works only "
"in binary classification and multilabel indicator format."
msgstr ""

#: ../../modules/model_evaluation.rst
msgid "Examples:"
msgstr ""

#: ../../modules/model_evaluation.rst:580
msgid ""
"See :ref:`example_text_document_classification_20newsgroups.py` for an "
"example of :func:`f1_score` usage to classify  text documents."
msgstr ""

#: ../../modules/model_evaluation.rst:584
msgid ""
"See :ref:`example_model_selection_grid_search_digits.py` for an example "
"of :func:`precision_score` and :func:`recall_score` usage to estimate "
"parameters using grid search with nested cross-validation."
msgstr ""

#: ../../modules/model_evaluation.rst:588
msgid ""
"See :ref:`example_model_selection_plot_precision_recall.py` for an "
"example of :func:`precision_recall_curve` usage to evaluate classifier "
"output quality."
msgstr ""

#: ../../modules/model_evaluation.rst:592
msgid ""
"See :ref:`example_linear_model_plot_sparse_recovery.py` for an example of"
" :func:`precision_recall_curve` usage to select features for sparse "
"linear models."
msgstr ""

#: ../../modules/model_evaluation.rst:597
msgid "Binary classification"
msgstr ""

#: ../../modules/model_evaluation.rst:599
msgid ""
"In a binary classification task, the terms ''positive'' and ''negative'' "
"refer to the classifier's prediction, and the terms ''true'' and "
"''false'' refer to whether that prediction corresponds to the external "
"judgment (sometimes known as the ''observation''). Given these "
"definitions, we can formulate the following table:"
msgstr ""

#: ../../modules/model_evaluation.rst:606
msgid "Actual class (observation)"
msgstr ""

#: ../../modules/model_evaluation.rst:608
msgid "Predicted class (expectation)"
msgstr ""

#: ../../modules/model_evaluation.rst:608
msgid "tp (true positive) Correct result"
msgstr ""

#: ../../modules/model_evaluation.rst:608
msgid "fp (false positive) Unexpected result"
msgstr ""

#: ../../modules/model_evaluation.rst:611
msgid "fn (false negative) Missing result"
msgstr ""

#: ../../modules/model_evaluation.rst:611
msgid "tn (true negative) Correct absence of result"
msgstr ""

#: ../../modules/model_evaluation.rst:615
msgid ""
"In this context, we can define the notions of precision, recall and "
"F-measure:"
msgstr ""

#: ../../modules/model_evaluation.rst:629
msgid "Here are some small examples in binary classification::"
msgstr ""

#: ../../modules/model_evaluation.rst:668
msgid "Multiclass and multilabel classification"
msgstr ""

#: ../../modules/model_evaluation.rst:669
msgid ""
"In multiclass and multilabel classification task, the notions of "
"precision, recall, and F-measures can be applied to each label "
"independently. There are a few ways to combine results across labels, "
"specified by the ``average`` argument to the "
":func:`average_precision_score` (multilabel only), :func:`f1_score`, "
":func:`fbeta_score`, :func:`precision_recall_fscore_support`, "
":func:`precision_score` and :func:`recall_score` functions, as described "
":ref:`above <average>`. Note that for \"micro\"-averaging in a multiclass"
" setting with all labels included will produce equal precision, recall "
"and :math:`F`, while \"weighted\" averaging may produce an F-score that "
"is not between precision and recall."
msgstr ""

#: ../../modules/model_evaluation.rst:681
msgid "To make this more explicit, consider the following notation:"
msgstr ""

#: ../../modules/model_evaluation.rst:683
msgid ":math:`y` the set of *predicted* :math:`(sample, label)` pairs"
msgstr ""

#: ../../modules/model_evaluation.rst:684
msgid ":math:`\\hat{y}` the set of *true* :math:`(sample, label)` pairs"
msgstr ""

#: ../../modules/model_evaluation.rst:685
msgid ":math:`L` the set of labels"
msgstr ""

#: ../../modules/model_evaluation.rst:686
msgid ":math:`S` the set of samples"
msgstr ""

#: ../../modules/model_evaluation.rst:687
msgid ""
":math:`y_s` the subset of :math:`y` with sample :math:`s`, i.e. "
":math:`y_s := \\left\\{(s', l) \\in y | s' = s\\right\\}`"
msgstr ""

#: ../../modules/model_evaluation.rst:689
msgid ":math:`y_l` the subset of :math:`y` with label :math:`l`"
msgstr ""

#: ../../modules/model_evaluation.rst:690
msgid ""
"similarly, :math:`\\hat{y}_s` and :math:`\\hat{y}_l` are subsets of "
":math:`\\hat{y}`"
msgstr ""

#: ../../modules/model_evaluation.rst:692
msgid ":math:`P(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|A\\right|}`"
msgstr ""

#: ../../modules/model_evaluation.rst:693
msgid ""
":math:`R(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|B\\right|}` "
"(Conventions vary on handling :math:`B = \\emptyset`; this implementation"
" uses :math:`R(A, B):=0`, and similar for :math:`P`.)"
msgstr ""

#: ../../modules/model_evaluation.rst:696
msgid ""
":math:`F_\\beta(A, B) := \\left(1 + \\beta^2\\right) \\frac{P(A, B) "
"\\times R(A, B)}{\\beta^2 P(A, B) + R(A, B)}`"
msgstr ""

#: ../../modules/model_evaluation.rst:698
msgid "Then the metrics are defined as:"
msgstr ""

#: ../../modules/model_evaluation.rst:701
msgid "``average``"
msgstr ""

#: ../../modules/model_evaluation.rst:701
msgid "Precision"
msgstr ""

#: ../../modules/model_evaluation.rst:701
msgid "Recall"
msgstr ""

#: ../../modules/model_evaluation.rst:701
msgid "F\\_beta"
msgstr ""

#: ../../modules/model_evaluation.rst:703
msgid "``\"micro\"``"
msgstr ""

#: ../../modules/model_evaluation.rst:703
msgid ":math:`P(y, \\hat{y})`"
msgstr ""

#: ../../modules/model_evaluation.rst:703
msgid ":math:`R(y, \\hat{y})`"
msgstr ""

#: ../../modules/model_evaluation.rst:703
msgid ":math:`F_\\beta(y, \\hat{y})`"
msgstr ""

#: ../../modules/model_evaluation.rst:705
msgid "``\"samples\"``"
msgstr ""

#: ../../modules/model_evaluation.rst:705
msgid ":math:`\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} P(y_s, \\hat{y}_s)`"
msgstr ""

#: ../../modules/model_evaluation.rst:705
msgid ":math:`\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} R(y_s, \\hat{y}_s)`"
msgstr ""

#: ../../modules/model_evaluation.rst:705
msgid ""
":math:`\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} F_\\beta(y_s, "
"\\hat{y}_s)`"
msgstr ""

#: ../../modules/model_evaluation.rst:707
msgid "``\"macro\"``"
msgstr ""

#: ../../modules/model_evaluation.rst:707
msgid ":math:`\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} P(y_l, \\hat{y}_l)`"
msgstr ""

#: ../../modules/model_evaluation.rst:707
msgid ":math:`\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} R(y_l, \\hat{y}_l)`"
msgstr ""

#: ../../modules/model_evaluation.rst:707
msgid ""
":math:`\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} F_\\beta(y_l, "
"\\hat{y}_l)`"
msgstr ""

#: ../../modules/model_evaluation.rst:709
msgid "``\"weighted\"``"
msgstr ""

#: ../../modules/model_evaluation.rst:709
msgid ""
":math:`\\frac{1}{\\sum_{l \\in L} \\left|\\hat{y}_l\\right|} \\sum_{l "
"\\in L} \\left|\\hat{y}_l\\right| P(y_l, \\hat{y}_l)`"
msgstr ""

#: ../../modules/model_evaluation.rst:709
msgid ""
":math:`\\frac{1}{\\sum_{l \\in L} \\left|\\hat{y}_l\\right|} \\sum_{l "
"\\in L} \\left|\\hat{y}_l\\right| R(y_l, \\hat{y}_l)`"
msgstr ""

#: ../../modules/model_evaluation.rst:709
msgid ""
":math:`\\frac{1}{\\sum_{l \\in L} \\left|\\hat{y}_l\\right|} \\sum_{l "
"\\in L} \\left|\\hat{y}_l\\right| F_\\beta(y_l, \\hat{y}_l)`"
msgstr ""

#: ../../modules/model_evaluation.rst:711
msgid "``None``"
msgstr ""

#: ../../modules/model_evaluation.rst:711
msgid ":math:`\\langle P(y_l, \\hat{y}_l) | l \\in L \\rangle`"
msgstr ""

#: ../../modules/model_evaluation.rst:711
msgid ":math:`\\langle R(y_l, \\hat{y}_l) | l \\in L \\rangle`"
msgstr ""

#: ../../modules/model_evaluation.rst:711
msgid ":math:`\\langle F_\\beta(y_l, \\hat{y}_l) | l \\in L \\rangle`"
msgstr ""

#: ../../modules/model_evaluation.rst:730
msgid ""
"For multiclass classification with a \"negative class\", it is possible "
"to exclude some labels:"
msgstr ""

#: ../../modules/model_evaluation.rst:736
msgid ""
"Similarly, labels not present in the data sample may be accounted for in "
"macro-averaging."
msgstr ""

#: ../../modules/model_evaluation.rst:745
msgid "Hinge loss"
msgstr ""

#: ../../modules/model_evaluation.rst:747
msgid ""
"The :func:`hinge_loss` function computes the average distance between the"
" model and the data using `hinge loss "
"<http://en.wikipedia.org/wiki/Hinge_loss>`_, a one-sided metric that "
"considers only prediction errors. (Hinge loss is used in maximal margin "
"classifiers such as support vector machines.)"
msgstr ""

#: ../../modules/model_evaluation.rst:753
msgid ""
"If the labels are encoded with +1 and -1,  :math:`y`: is the true value, "
"and :math:`w` is the predicted decisions as output by "
"``decision_function``, then the hinge loss is defined as:"
msgstr ""

#: ../../modules/model_evaluation.rst:761
msgid ""
"If there are more than two labels, :func:`hinge_loss` uses a multiclass "
"variant due to Crammer & Singer. `Here "
"<http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf>`_ is"
" the paper describing it."
msgstr ""

#: ../../modules/model_evaluation.rst:766
msgid ""
"If :math:`y_w` is the predicted decision for true label and :math:`y_t` "
"is the maximum of the predicted decisions for all other labels, where "
"predicted decisions are output by decision function, then multiclass "
"hinge loss is defined by:"
msgstr ""

#: ../../modules/model_evaluation.rst:775
msgid ""
"Here a small example demonstrating the use of the :func:`hinge_loss` "
"function with a svm classifier in a binary class problem::"
msgstr ""

#: ../../modules/model_evaluation.rst:794
msgid ""
"Here is an example demonstrating the use of the :func:`hinge_loss` "
"function with a svm classifier in a multiclass problem::"
msgstr ""

#: ../../modules/model_evaluation.rst:814
msgid "Log loss"
msgstr ""

#: ../../modules/model_evaluation.rst:816
msgid ""
"Log loss, also called logistic regression loss or cross-entropy loss, is "
"defined on probability estimates.  It is commonly used in (multinomial) "
"logistic regression and neural networks, as well as in some variants of "
"expectation-maximization, and can be used to evaluate the probability "
"outputs (``predict_proba``) of a classifier instead of its discrete "
"predictions."
msgstr ""

#: ../../modules/model_evaluation.rst:823
msgid ""
"For binary classification with a true label :math:`y \\in \\{0,1\\}` and "
"a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log "
"loss per sample is the negative log-likelihood of the classifier given "
"the true label:"
msgstr ""

#: ../../modules/model_evaluation.rst:832
msgid ""
"This extends to the multiclass case as follows. Let the true labels for a"
" set of samples be encoded as a 1-of-K binary indicator matrix :math:`Y`,"
" i.e., :math:`y_{i,k} = 1` if sample :math:`i` has label :math:`k` taken "
"from a set of :math:`K` labels. Let :math:`P` be a matrix of probability "
"estimates, with :math:`p_{i,k} = \\operatorname{Pr}(t_{i,k} = 1)`. Then "
"the log loss of the whole set is"
msgstr ""

#: ../../modules/model_evaluation.rst:845
msgid ""
"To see how this generalizes the binary log loss given above, note that in"
" the binary case, :math:`p_{i,0} = 1 - p_{i,1}` and :math:`y_{i,0} = 1 - "
"y_{i,1}`, so expanding the inner sum over :math:`y_{i,k} \\in \\{0,1\\}` "
"gives the binary log loss."
msgstr ""

#: ../../modules/model_evaluation.rst:851
msgid ""
"The :func:`log_loss` function computes log loss given a list of ground-"
"truth labels and a probability matrix, as returned by an estimator's "
"``predict_proba`` method."
msgstr ""

#: ../../modules/model_evaluation.rst:861
msgid ""
"The first ``[.9, .1]`` in ``y_pred`` denotes 90% probability that the "
"first sample has label 0.  The log loss is non-negative."
msgstr ""

#: ../../modules/model_evaluation.rst:867
msgid "Matthews correlation coefficient"
msgstr ""

#: ../../modules/model_evaluation.rst:869
msgid ""
"The :func:`matthews_corrcoef` function computes the `Matthew's "
"correlation coefficient (MCC) "
"<http://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_ for "
"binary classes.  Quoting Wikipedia:"
msgstr ""

#: ../../modules/model_evaluation.rst:874
msgid ""
"\"The Matthews correlation coefficient is used in machine learning as a "
"measure of the quality of binary (two-class) classifications. It takes "
"into account true and false positives and negatives and is generally "
"regarded as a balanced measure which can be used even if the classes are "
"of very different sizes. The MCC is in essence a correlation coefficient "
"value between -1 and +1. A coefficient of +1 represents a perfect "
"prediction, 0 an average random prediction and -1 an inverse prediction. "
"The statistic is also known as the phi coefficient.\""
msgstr ""

#: ../../modules/model_evaluation.rst:883
msgid ""
"If :math:`tp`, :math:`tn`, :math:`fp` and :math:`fn` are respectively the"
" number of true positives, true negatives, false positives and false "
"negatives, the MCC coefficient is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:891
msgid ""
"Here is a small example illustrating the usage of the "
":func:`matthews_corrcoef` function:"
msgstr ""

#: ../../modules/model_evaluation.rst:903
msgid "Receiver operating characteristic (ROC)"
msgstr ""

#: ../../modules/model_evaluation.rst:905
msgid ""
"The function :func:`roc_curve` computes the `receiver operating "
"characteristic curve, or ROC curve "
"<http://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_. "
"Quoting Wikipedia :"
msgstr ""

#: ../../modules/model_evaluation.rst:909
msgid ""
"\"A receiver operating characteristic (ROC), or simply ROC curve, is a "
"graphical plot which illustrates the performance of a binary classifier "
"system as its discrimination threshold is varied. It is created by "
"plotting the fraction of true positives out of the positives (TPR = true "
"positive rate) vs. the fraction of false positives out of the negatives "
"(FPR = false positive rate), at various threshold settings. TPR is also "
"known as sensitivity, and FPR is one minus the specificity or true "
"negative rate.\""
msgstr ""

#: ../../modules/model_evaluation.rst:917
msgid ""
"This function requires the true binary value and the target scores, which"
" can either be probability estimates of the positive class, confidence "
"values, or binary decisions. Here is a small example of how to use the "
":func:`roc_curve` function::"
msgstr ""

#: ../../modules/model_evaluation.rst:934
msgid "This figure shows an example of such an ROC curve:"
msgstr ""

#: ../../modules/model_evaluation.rst:941
msgid ""
"The :func:`roc_auc_score` function computes the area under the receiver "
"operating characteristic (ROC) curve, which is also denoted by AUC or "
"AUROC.  By computing the area under the roc curve, the curve information "
"is summarized in one number. For more information see the `Wikipedia "
"article on AUC "
"<http://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_curve>`_."
msgstr ""

#: ../../modules/model_evaluation.rst:955
msgid ""
"In multi-label classification, the :func:`roc_auc_score` function is "
"extended by averaging over the labels as :ref:`above <average>`."
msgstr ""

#: ../../modules/model_evaluation.rst:958
msgid ""
"Compared to metrics such as the subset accuracy, the Hamming loss, or the"
" F1 score, ROC doesn't require optimizing a threshold for each label. The"
" :func:`roc_auc_score` function can also be used in multi-class "
"classification, if the predicted outputs have been binarized."
msgstr ""

#: ../../modules/model_evaluation.rst:971
msgid ""
"See :ref:`example_model_selection_plot_roc.py` for an example of using "
"ROC to evaluate the quality of the output of a classifier."
msgstr ""

#: ../../modules/model_evaluation.rst:975
msgid ""
"See :ref:`example_model_selection_plot_roc_crossval.py` for an example of"
" using ROC to evaluate classifier output quality, using cross-validation."
msgstr ""

#: ../../modules/model_evaluation.rst:979
msgid ""
"See :ref:`example_applications_plot_species_distribution_modeling.py` for"
" an example of using ROC to model species distribution."
msgstr ""

#: ../../modules/model_evaluation.rst:986
msgid "Zero one loss"
msgstr ""

#: ../../modules/model_evaluation.rst:988
msgid ""
"The :func:`zero_one_loss` function computes the sum or the average of the"
" 0-1 classification loss (:math:`L_{0-1}`) over "
":math:`n_{\\text{samples}}`. By default, the function normalizes over the"
" sample. To get the sum of the :math:`L_{0-1}`, set ``normalize`` to "
"``False``."
msgstr ""

#: ../../modules/model_evaluation.rst:993
msgid ""
"In multilabel classification, the :func:`zero_one_loss` scores a subset "
"as one if its labels strictly match the predictions, and as a zero if "
"there are any errors.  By default, the function returns the percentage of"
" imperfectly predicted subsets.  To get the count of such subsets "
"instead, set ``normalize`` to ``False``"
msgstr ""

#: ../../modules/model_evaluation.rst:999
msgid ""
"If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample "
"and :math:`y_i` is the corresponding true value, then the 0-1 loss "
":math:`L_{0-1}` is defined as:"
msgstr ""

#: ../../modules/model_evaluation.rst:1019
msgid ""
"In the multilabel case with binary label indicators, where the first "
"label set [0,1] has an error: ::"
msgstr ""

#: ../../modules/model_evaluation.rst:1030
msgid ""
"See :ref:`example_feature_selection_plot_rfe_with_cross_validation.py` "
"for an example of zero one loss usage to perform recursive feature "
"elimination with cross-validation."
msgstr ""

#: ../../modules/model_evaluation.rst:1038
msgid "Multilabel ranking metrics"
msgstr ""

#: ../../modules/model_evaluation.rst:1042
msgid ""
"In multilabel learning, each sample can have any number of ground truth "
"labels associated with it. The goal is to give high scores and better "
"rank to the ground truth labels."
msgstr ""

#: ../../modules/model_evaluation.rst:1049
msgid "Coverage error"
msgstr ""

#: ../../modules/model_evaluation.rst:1051
msgid ""
"The :func:`coverage_error` function computes the average number of labels"
" that have to be included in the final prediction such that all true "
"labels are predicted. This is useful if you want to know how many top-"
"scored-labels you have to predict in average without missing any true "
"one. The best value of this metrics is thus the average number of true "
"labels."
msgstr ""

#: ../../modules/model_evaluation.rst:1057
msgid ""
"Formally, given a binary indicator matrix of the ground truth labels "
":math:`y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times "
"n_\\text{labels}}` and the score associated with each label "
":math:`\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times "
"n_\\text{labels}}`, the coverage is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:1067
msgid ""
"with :math:`\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq "
"\\hat{f}_{ij} \\right\\}\\right|`. Given the rank definition, ties in "
"``y_scores`` are broken by giving the maximal rank that would have been "
"assigned to all tied values."
msgstr ""

#: ../../modules/model_evaluation.rst:1071
#: ../../modules/model_evaluation.rst:1114
#: ../../modules/model_evaluation.rst:1147
msgid "Here is a small example of usage of this function::"
msgstr ""

#: ../../modules/model_evaluation.rst:1083
msgid "Label ranking average precision"
msgstr ""

#: ../../modules/model_evaluation.rst:1085
msgid ""
"The :func:`label_ranking_average_precision_score` function implements "
"label ranking average precision (LRAP). This metric is linked to the "
":func:`average_precision_score` function, but is based on the notion of "
"label ranking instead of precision and recall."
msgstr ""

#: ../../modules/model_evaluation.rst:1090
msgid ""
"Label ranking average precision (LRAP) is the average over each ground "
"truth label assigned to each sample, of the ratio of true vs. total "
"labels with lower score. This metric will yield better scores if you are "
"able to give better rank to the labels associated with each sample. The "
"obtained score is always strictly greater than 0, and the best value is "
"1. If there is exactly one relevant label per sample, label ranking "
"average precision is equivalent to the `mean reciprocal rank "
"<http://en.wikipedia.org/wiki/Mean_reciprocal_rank>`_."
msgstr ""

#: ../../modules/model_evaluation.rst:1098
msgid ""
"Formally, given a binary indicator matrix of the ground truth labels "
":math:`y \\in \\mathcal{R}^{n_\\text{samples} \\times n_\\text{labels}}` "
"and the score associated with each label :math:`\\hat{f} \\in "
"\\mathcal{R}^{n_\\text{samples} \\times n_\\text{labels}}`, the average "
"precision is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:1110
msgid ""
"with :math:`\\mathcal{L}_{ij} = \\left\\{k: y_{ik} = 1, \\hat{f}_{ik} "
"\\geq \\hat{f}_{ij} \\right\\}`, :math:`\\text{rank}_{ij} = "
"\\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|` "
"and :math:`|\\cdot|` is the l0 norm or the cardinality of the set."
msgstr ""

#: ../../modules/model_evaluation.rst:1126
msgid "Ranking loss"
msgstr ""

#: ../../modules/model_evaluation.rst:1128
msgid ""
"The :func:`label_ranking_loss` function computes the ranking loss which "
"averages over the samples the number of label pairs that are incorrectly "
"ordered, i.e. true labels have a lower score than false labels, weighted "
"by the the inverse number of false and true labels. The lowest achievable"
" ranking loss is zero."
msgstr ""

#: ../../modules/model_evaluation.rst:1134
msgid ""
"Formally, given a binary indicator matrix of the ground truth labels "
":math:`y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times "
"n_\\text{labels}}` and the score associated with each label "
":math:`\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times "
"n_\\text{labels}}`, the ranking loss is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:1145
msgid ""
"where :math:`|\\cdot|` is the :math:`\\ell_0` norm or the cardinality of "
"the set."
msgstr ""

#: ../../modules/model_evaluation.rst:1163
msgid "Regression metrics"
msgstr ""

#: ../../modules/model_evaluation.rst:1167
msgid ""
"The :mod:`sklearn.metrics` module implements several loss, score, and "
"utility functions to measure regression performance. Some of those have "
"been enhanced to handle the multioutput case: :func:`mean_squared_error`,"
" :func:`mean_absolute_error`, :func:`explained_variance_score` and "
":func:`r2_score`."
msgstr ""

#: ../../modules/model_evaluation.rst:1174
msgid ""
"These functions have an ``multioutput`` keyword argument which specifies "
"the way the scores or losses for each individual target should be "
"averaged. The default is ``'uniform_average'``, which specifies a "
"uniformly weighted mean over outputs. If an ``ndarray`` of shape "
"``(n_outputs,)`` is passed, then its entries are interpreted as weights "
"and an according weighted average is returned. If ``multioutput`` is "
"``'raw_values'`` is specified, then all unaltered individual scores or "
"losses will be returned in an array of shape ``(n_outputs,)``."
msgstr ""

#: ../../modules/model_evaluation.rst:1184
msgid ""
"The :func:`r2_score` and :func:`explained_variance_score` accept an "
"additional value ``'variance_weighted'`` for the ``multioutput`` "
"parameter. This option leads to a weighting of each individual score by "
"the variance of the corresponding target variable. This setting "
"quantifies the globally captured unscaled variance. If the target "
"variables are of different scale, then this score puts more importance on"
" well explaining the higher variance variables. "
"``multioutput='variance_weighted'`` is the default value for "
":func:`r2_score` for backward compatibility. This will be changed to "
"``uniform_average`` in the future."
msgstr ""

#: ../../modules/model_evaluation.rst:1197
msgid "Explained variance score"
msgstr ""

#: ../../modules/model_evaluation.rst:1199
msgid ""
"The :func:`explained_variance_score` computes the `explained variance "
"regression score <http://en.wikipedia.org/wiki/Explained_variation>`_."
msgstr ""

#: ../../modules/model_evaluation.rst:1202
msgid ""
"If :math:`\\hat{y}` is the estimated target output, :math:`y` the "
"corresponding (correct) target output, and :math:`Var` is `Variance "
"<http://en.wikipedia.org/wiki/Variance>`_, the square of the standard "
"deviation, then the explained variance is estimated as follow:"
msgstr ""

#: ../../modules/model_evaluation.rst:1211
msgid "The best possible score is 1.0, lower values are worse."
msgstr ""

#: ../../modules/model_evaluation.rst:1213
msgid ""
"Here is a small example of usage of the :func:`explained_variance_score` "
"function::"
msgstr ""

#: ../../modules/model_evaluation.rst:1233
msgid "Mean absolute error"
msgstr ""

#: ../../modules/model_evaluation.rst:1235
msgid ""
"The :func:`mean_absolute_error` function computes `mean absolute error "
"<http://en.wikipedia.org/wiki/Mean_absolute_error>`_, a risk metric "
"corresponding to the expected value of the absolute error loss or "
":math:`l1`-norm loss."
msgstr ""

#: ../../modules/model_evaluation.rst:1240
msgid ""
"If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample, "
"and :math:`y_i` is the corresponding true value, then the mean absolute "
"error (MAE) estimated over :math:`n_{\\text{samples}}` is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:1248
msgid ""
"Here is a small example of usage of the :func:`mean_absolute_error` "
"function::"
msgstr ""

#: ../../modules/model_evaluation.rst:1268
msgid "Mean squared error"
msgstr ""

#: ../../modules/model_evaluation.rst:1270
msgid ""
"The :func:`mean_squared_error` function computes `mean square error "
"<http://en.wikipedia.org/wiki/Mean_squared_error>`_, a risk metric "
"corresponding to the expected value of the squared (quadratic) error loss"
" or loss."
msgstr ""

#: ../../modules/model_evaluation.rst:1275
msgid ""
"If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample, "
"and :math:`y_i` is the corresponding true value, then the mean squared "
"error (MSE) estimated over :math:`n_{\\text{samples}}` is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:1283
msgid ""
"Here is a small example of usage of the :func:`mean_squared_error` "
"function::"
msgstr ""

#: ../../modules/model_evaluation.rst:1298
msgid ""
"See :ref:`example_ensemble_plot_gradient_boosting_regression.py` for an "
"example of mean squared error usage to evaluate gradient boosting "
"regression."
msgstr ""

#: ../../modules/model_evaluation.rst:1305
msgid "Median absolute error"
msgstr ""

#: ../../modules/model_evaluation.rst:1307
msgid ""
"The :func:`median_absolute_error` is particularly interesting because it "
"is robust to outliers. The loss is calculated by taking the median of all"
" absolute differences between the target and the prediction."
msgstr ""

#: ../../modules/model_evaluation.rst:1311
msgid ""
"If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample "
"and :math:`y_i` is the corresponding true value, then the median absolute"
" error (MedAE) estimated over :math:`n_{\\text{samples}}` is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:1319
msgid "The :func:`median_absolute_error` does not support multioutput."
msgstr ""

#: ../../modules/model_evaluation.rst:1321
msgid ""
"Here is a small example of usage of the :func:`median_absolute_error` "
"function::"
msgstr ""

#: ../../modules/model_evaluation.rst:1333
msgid "R score, the coefficient of determination"
msgstr ""

#: ../../modules/model_evaluation.rst:1335
msgid ""
"The :func:`r2_score` function computes R, the `coefficient of "
"determination "
"<http://en.wikipedia.org/wiki/Coefficient_of_determination>`_. It "
"provides a measure of how well future samples are likely to be predicted "
"by the model. Best possible score is 1.0 and it can be negative (because "
"the model can be arbitrarily worse). A constant model that always "
"predicts the expected value of y, disregarding the input features, would "
"get a R^2 score of 0.0."
msgstr ""

#: ../../modules/model_evaluation.rst:1343
msgid ""
"If :math:`\\hat{y}_i` is the predicted value of the :math:`i`-th sample "
"and :math:`y_i` is the corresponding true value, then the score R "
"estimated over :math:`n_{\\text{samples}}` is defined as"
msgstr ""

#: ../../modules/model_evaluation.rst:1351
msgid ""
"where :math:`\\bar{y} =  \\frac{1}{n_{\\text{samples}}} "
"\\sum_{i=0}^{n_{\\text{samples}} - 1} y_i`."
msgstr ""

#: ../../modules/model_evaluation.rst:1353
msgid "Here is a small example of usage of the :func:`r2_score` function::"
msgstr ""

#: ../../modules/model_evaluation.rst:1380
msgid ""
"See :ref:`example_linear_model_plot_lasso_and_elasticnet.py` for an "
"example of R score usage to evaluate Lasso and Elastic Net on sparse "
"signals."
msgstr ""

#: ../../modules/model_evaluation.rst:1387
msgid "Clustering metrics"
msgstr ""

#: ../../modules/model_evaluation.rst:1391
msgid ""
"The :mod:`sklearn.metrics` module implements several loss, score, and "
"utility functions. For more information see the "
":ref:`clustering_evaluation` section for instance clustering, and "
":ref:`biclustering_evaluation` for biclustering."
msgstr ""

#: ../../modules/model_evaluation.rst:1401
msgid "Dummy estimators"
msgstr ""

#: ../../modules/model_evaluation.rst:1405
msgid ""
"When doing supervised learning, a simple sanity check consists of "
"comparing one's estimator against simple rules of thumb. "
":class:`DummyClassifier` implements three such simple strategies for "
"classification:"
msgstr ""

#: ../../modules/model_evaluation.rst:1409
msgid ""
"``stratified`` generates random predictions by respecting the training "
"set class distribution."
msgstr ""

#: ../../modules/model_evaluation.rst:1411
msgid ""
"``most_frequent`` always predicts the most frequent label in the training"
" set."
msgstr ""

#: ../../modules/model_evaluation.rst:1412
msgid ""
"``prior`` always predicts the class that maximizes the class prior (like "
"``most_frequent`) and ``predict_proba`` returns the class prior."
msgstr ""

#: ../../modules/model_evaluation.rst:1414
msgid "``uniform`` generates predictions uniformly at random."
msgstr ""

#: ../../modules/model_evaluation.rst:1417
msgid ""
"``constant`` always predicts a constant label that is provided by the "
"user."
msgstr ""

#: ../../modules/model_evaluation.rst:1416
msgid ""
"A major motivation of this method is F1-scoring, when the positive class "
"is in the minority."
msgstr ""

#: ../../modules/model_evaluation.rst:1419
msgid ""
"Note that with all these strategies, the ``predict`` method completely "
"ignores the input data!"
msgstr ""

#: ../../modules/model_evaluation.rst:1422
msgid ""
"To illustrate :class:`DummyClassifier`, first let's create an imbalanced "
"dataset::"
msgstr ""

#: ../../modules/model_evaluation.rst:1432
msgid "Next, let's compare the accuracy of ``SVC`` and ``most_frequent``::"
msgstr ""

#: ../../modules/model_evaluation.rst:1445
msgid ""
"We see that ``SVC`` doesn't do much better than a dummy classifier. Now, "
"let's change the kernel::"
msgstr ""

#: ../../modules/model_evaluation.rst:1452
msgid ""
"We see that the accuracy was boosted to almost 100%.  A cross validation "
"strategy is recommended for a better estimate of the accuracy, if it is "
"not too CPU costly. For more information see the :ref:`cross_validation` "
"section. Moreover if you want to optimize over the parameter space, it is"
" highly recommended to use an appropriate methodology; see the "
":ref:`grid_search` section for details."
msgstr ""

#: ../../modules/model_evaluation.rst:1459
msgid ""
"More generally, when the accuracy of a classifier is too close to random,"
" it probably means that something went wrong: features are not helpful, a"
" hyperparameter is not correctly tuned, the classifier is suffering from "
"class imbalance, etc..."
msgstr ""

#: ../../modules/model_evaluation.rst:1464
msgid ""
":class:`DummyRegressor` also implements four simple rules of thumb for "
"regression:"
msgstr ""

#: ../../modules/model_evaluation.rst:1466
msgid "``mean`` always predicts the mean of the training targets."
msgstr ""

#: ../../modules/model_evaluation.rst:1467
msgid "``median`` always predicts the median of the training targets."
msgstr ""

#: ../../modules/model_evaluation.rst:1468
msgid ""
"``quantile`` always predicts a user provided quantile of the training "
"targets."
msgstr ""

#: ../../modules/model_evaluation.rst:1469
msgid ""
"``constant`` always predicts a constant value that is provided by the "
"user."
msgstr ""

#: ../../modules/model_evaluation.rst:1471
msgid ""
"In all these strategies, the ``predict`` method completely ignores the "
"input data."
msgstr ""

