# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/multiclass.rst:6
msgid "Multiclass and multilabel algorithms"
msgstr ""

#: ../../modules/multiclass.rst:11
msgid ""
"All classifiers in scikit-learn do multiclass classification out-of-the-"
"box. You don't need to use the :mod:`sklearn.multiclass` module unless "
"you want to experiment with different multiclass strategies."
msgstr ""

#: ../../modules/multiclass.rst:15
msgid ""
"The :mod:`sklearn.multiclass` module implements *meta-estimators* to "
"solve ``multiclass`` and ``multilabel`` classification problems by "
"decomposing such problems into binary classification problems."
msgstr ""

#: ../../modules/multiclass.rst:19
msgid ""
"**Multiclass classification** means a classification task with more than "
"two classes; e.g., classify a set of images of fruits which may be "
"oranges, apples, or pears. Multiclass classification makes the assumption"
" that each sample is assigned to one and only one label: a fruit can be "
"either an apple or a pear but not both at the same time."
msgstr ""

#: ../../modules/multiclass.rst:25
msgid ""
"**Multilabel classification** assigns to each sample a set of target "
"labels. This can be thought as predicting properties of a data-point that"
" are not mutually exclusive, such as topics that are relevant for a "
"document. A text might be about any of religion, politics, finance or "
"education at the same time or none of these."
msgstr ""

#: ../../modules/multiclass.rst:31
msgid ""
"**Multioutput-multiclass classification** and **multi-task "
"classification** means that a single estimator has to handle several "
"joint classification tasks. This is a generalization of the multi-label "
"classification task, where the set of classification problem is "
"restricted to binary classification, and of the multi-class "
"classification task. *The output format is a 2d numpy array or sparse "
"matrix.*"
msgstr ""

#: ../../modules/multiclass.rst:39
msgid ""
"The set of labels can be different for each output variable. For instance"
" a sample could be assigned \"pear\" for an output variable that takes "
"possible values in a finite set of species such as \"pear\", \"apple\", "
"\"orange\" and \"green\" for a second output variable that takes possible"
" values in a finite set of colors such as \"green\", \"red\", \"orange\","
" \"yellow\"..."
msgstr ""

#: ../../modules/multiclass.rst:45
msgid ""
"This means that any classifiers handling multi-output multiclass or "
"multi-task classification task supports the multi-label classification "
"task as a special case. Multi-task classification is similar to the "
"multi-output classification task with different model formulations. For "
"more information, see the relevant estimator documentation."
msgstr ""

#: ../../modules/multiclass.rst:52
msgid ""
"All scikit-learn classifiers are capable of multiclass classification, "
"but the meta-estimators offered by :mod:`sklearn.multiclass` permit "
"changing the way they handle more than two classes because this may have "
"an effect on classifier performance (either in terms of generalization "
"error or required computational resources)."
msgstr ""

#: ../../modules/multiclass.rst:58
msgid ""
"Below is a summary of the classifiers supported by scikit-learn grouped "
"by strategy; you don't need the meta-estimators in this class if you're "
"using one of these unless you want custom multiclass behavior:"
msgstr ""

#: ../../modules/multiclass.rst:62
msgid ""
"Inherently multiclass: :ref:`Naive Bayes <naive_bayes>`, :ref:`LDA and "
"QDA <lda_qda>`, :ref:`Decision Trees <tree>`, :ref:`Random Forests "
"<forest>`, :ref:`Nearest Neighbors <neighbors>`, setting "
"``multi_class='multinomial'`` in "
":class:`sklearn.linear_model.LogisticRegression`."
msgstr ""

#: ../../modules/multiclass.rst:68
msgid ""
"Support multilabel: :ref:`Decision Trees <tree>`, :ref:`Random Forests "
"<forest>`, :ref:`Nearest Neighbors <neighbors>`, :ref:`Ridge Regression "
"<ridge_regression>`."
msgstr ""

#: ../../modules/multiclass.rst:71
msgid "One-Vs-One: :class:`sklearn.svm.SVC`."
msgstr ""

#: ../../modules/multiclass.rst:72
msgid "One-Vs-All: all linear models except :class:`sklearn.svm.SVC`."
msgstr ""

#: ../../modules/multiclass.rst:74
msgid ""
"Some estimators also support multioutput-multiclass classification tasks "
":ref:`Decision Trees <tree>`, :ref:`Random Forests <forest>`, "
":ref:`Nearest Neighbors <neighbors>`."
msgstr ""

#: ../../modules/multiclass.rst:80
msgid ""
"At present, no metric in :mod:`sklearn.metrics` supports the multioutput-"
"multiclass classification task."
msgstr ""

#: ../../modules/multiclass.rst:84
msgid "Multilabel classification format"
msgstr ""

#: ../../modules/multiclass.rst:86
msgid ""
"In multilabel learning, the joint set of binary classification tasks is "
"expressed with label binary indicator array: each sample is one row of a "
"2d array of shape (n_samples, n_classes) with binary values: the one, "
"i.e. the non zero elements, corresponds to the subset of labels. An array"
" such as ``np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])`` represents label"
" 0 in the first sample, labels 1 and 2 in the second sample, and no "
"labels in the third sample."
msgstr ""

#: ../../modules/multiclass.rst:93
msgid ""
"Producing multilabel data as a list of sets of labels may be more "
"intuitive. The :class:`MultiLabelBinarizer "
"<sklearn.preprocessing.MultiLabelBinarizer>` transformer can be used to "
"convert between a collection of collections of labels and the indicator "
"format."
msgstr ""

#: ../../modules/multiclass.rst:110
msgid "One-Vs-The-Rest"
msgstr ""

#: ../../modules/multiclass.rst:112
msgid ""
"This strategy, also known as **one-vs-all**, is implemented in "
":class:`OneVsRestClassifier`.  The strategy consists in fitting one "
"classifier per class. For each classifier, the class is fitted against "
"all the other classes. In addition to its computational efficiency (only "
"`n_classes` classifiers are needed), one advantage of this approach is "
"its interpretability. Since each class is represented by one and one "
"classifier only, it is possible to gain knowledge about the class by "
"inspecting its corresponding classifier. This is the most commonly used "
"strategy and is a fair default choice."
msgstr ""

#: ../../modules/multiclass.rst:123 ../../modules/multiclass.rst:180
#: ../../modules/multiclass.rst:243
msgid "Multiclass learning"
msgstr ""

#: ../../modules/multiclass.rst:125
msgid "Below is an example of multiclass learning using OvR::"
msgstr ""

#: ../../modules/multiclass.rst:142
msgid "Multilabel learning"
msgstr ""

#: ../../modules/multiclass.rst:144
msgid ""
":class:`OneVsRestClassifier` also supports multilabel classification. To "
"use this feature, feed the classifier an indicator matrix, in which cell "
"[i, j] indicates the presence of label j in sample i."
msgstr ""

#: ../../modules/multiclass.rst
msgid "Examples:"
msgstr ""

#: ../../modules/multiclass.rst:157
msgid ":ref:`example_plot_multilabel.py`"
msgstr ""

#: ../../modules/multiclass.rst:162
msgid "One-Vs-One"
msgstr ""

#: ../../modules/multiclass.rst:164
msgid ""
":class:`OneVsOneClassifier` constructs one classifier per pair of "
"classes. At prediction time, the class which received the most votes is "
"selected. In the event of a tie (among two classes with an equal number "
"of votes), it selects the class with the highest aggregate classification"
" confidence by summing over the pair-wise classification confidence "
"levels computed by the underlying binary classifiers."
msgstr ""

#: ../../modules/multiclass.rst:171
msgid ""
"Since it requires to fit ``n_classes * (n_classes - 1) / 2`` classifiers,"
" this method is usually slower than one-vs-the-rest, due to its "
"O(n_classes^2) complexity. However, this method may be advantageous for "
"algorithms such as kernel algorithms which don't scale well with "
"``n_samples``. This is because each individual learning problem only "
"involves a small subset of the data whereas, with one-vs-the-rest, the "
"complete dataset is used ``n_classes`` times."
msgstr ""

#: ../../modules/multiclass.rst:182
msgid "Below is an example of multiclass learning using OvO::"
msgstr ""

#: ../../modules/multiclass.rst
msgid "References:"
msgstr ""

#: ../../modules/multiclass.rst:201
msgid ""
"\"Pattern Recognition and Machine Learning. Springer\", Christopher M. "
"Bishop, page 183, (First Edition)"
msgstr ""

#: ../../modules/multiclass.rst:207
msgid "Error-Correcting Output-Codes"
msgstr ""

#: ../../modules/multiclass.rst:209
msgid ""
"Output-code based strategies are fairly different from one-vs-the-rest "
"and one-vs-one. With these strategies, each class is represented in a "
"euclidean space, where each dimension can only be 0 or 1. Another way to "
"put it is that each class is represented by a binary code (an array of 0 "
"and 1). The matrix which keeps track of the location/code of each class "
"is called the code book. The code size is the dimensionality of the "
"aforementioned space. Intuitively, each class should be represented by a "
"code as unique as possible and a good code book should be designed to "
"optimize classification accuracy. In this implementation, we simply use a"
" randomly-generated code book as advocated in [3]_ although more "
"elaborate methods may be added in the future."
msgstr ""

#: ../../modules/multiclass.rst:221
msgid ""
"At fitting time, one binary classifier per bit in the code book is "
"fitted. At prediction time, the classifiers are used to project new "
"points in the class space and the class closest to the points is chosen."
msgstr ""

#: ../../modules/multiclass.rst:225
msgid ""
"In :class:`OutputCodeClassifier`, the ``code_size`` attribute allows the "
"user to control the number of classifiers which will be used. It is a "
"percentage of the total number of classes."
msgstr ""

#: ../../modules/multiclass.rst:229
msgid ""
"A number between 0 and 1 will require fewer classifiers than one-vs-the-"
"rest. In theory, ``log2(n_classes) / n_classes`` is sufficient to "
"represent each class unambiguously. However, in practice, it may not lead"
" to good accuracy since ``log2(n_classes)`` is much smaller than "
"n_classes."
msgstr ""

#: ../../modules/multiclass.rst:234
msgid ""
"A number greater than than 1 will require more classifiers than one-vs-"
"the-rest. In this case, some classifiers will in theory correct for the "
"mistakes made by other classifiers, hence the name \"error-correcting\". "
"In practice, however, this may not happen as classifier mistakes will "
"typically be correlated. The error-correcting output codes have a similar"
" effect to bagging."
msgstr ""

#: ../../modules/multiclass.rst:245
msgid "Below is an example of multiclass learning using Output-Codes::"
msgstr ""

#: ../../modules/multiclass.rst:265
msgid ""
"\"Solving multiclass learning problems via error-correcting output "
"codes\", Dietterich T., Bakiri G., Journal of Artificial Intelligence "
"Research 2, 1995."
msgstr ""

#: ../../modules/multiclass.rst:270
msgid ""
"\"The error coding method and PICTs\", James G., Hastie T., Journal of "
"Computational and Graphical statistics 7, 1998."
msgstr ""

#: ../../modules/multiclass.rst:275
msgid ""
"\"The Elements of Statistical Learning\", Hastie T., Tibshirani R., "
"Friedman J., page 606 (second-edition) 2008."
msgstr ""

