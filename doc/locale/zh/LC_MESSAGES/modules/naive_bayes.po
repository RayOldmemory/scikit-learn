# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/naive_bayes.rst:5
msgid "Naive Bayes"
msgstr ""

#: ../../modules/naive_bayes.rst:10
msgid ""
"Naive Bayes methods are a set of supervised learning algorithms based on "
"applying Bayes' theorem with the \"naive\" assumption of independence "
"between every pair of features. Given a class variable :math:`y` and a "
"dependent feature vector :math:`x_1` through :math:`x_n`, Bayes' theorem "
"states the following relationship:"
msgstr ""

#: ../../modules/naive_bayes.rst:21
msgid "Using the naive independence assumption that"
msgstr ""

#: ../../modules/naive_bayes.rst:27
msgid "for all :math:`i`, this relationship is simplified to"
msgstr ""

#: ../../modules/naive_bayes.rst:34
msgid ""
"Since :math:`P(x_1, \\dots, x_n)` is constant given the input, we can use"
" the following classification rule:"
msgstr ""

#: ../../modules/naive_bayes.rst:45
msgid ""
"and we can use Maximum A Posteriori (MAP) estimation to estimate "
":math:`P(y)` and :math:`P(x_i \\mid y)`; the former is then the relative "
"frequency of class :math:`y` in the training set."
msgstr ""

#: ../../modules/naive_bayes.rst:50
msgid ""
"The different naive Bayes classifiers differ mainly by the assumptions "
"they make regarding the distribution of :math:`P(x_i \\mid y)`."
msgstr ""

#: ../../modules/naive_bayes.rst:53
msgid ""
"In spite of their apparently over-simplified assumptions, naive Bayes "
"classifiers have worked quite well in many real-world situations, "
"famously document classification and spam filtering. They require a small"
" amount of training data to estimate the necessary parameters. (For "
"theoretical reasons why naive Bayes works well, and on which types of "
"data it does, see the references below.)"
msgstr ""

#: ../../modules/naive_bayes.rst:60
msgid ""
"Naive Bayes learners and classifiers can be extremely fast compared to "
"more sophisticated methods. The decoupling of the class conditional "
"feature distributions means that each distribution can be independently "
"estimated as a one dimensional distribution. This in turn helps to "
"alleviate problems stemming from the curse of dimensionality."
msgstr ""

#: ../../modules/naive_bayes.rst:67
msgid ""
"On the flip side, although naive Bayes is known as a decent classifier, "
"it is known to be a bad estimator, so the probability outputs from "
"``predict_proba`` are not to be taken too seriously."
msgstr ""

#: ../../modules/naive_bayes.rst
msgid "References:"
msgstr ""

#: ../../modules/naive_bayes.rst:73
msgid ""
"H. Zhang (2004). `The optimality of Naive Bayes. "
"<http://www.cs.unb.ca/profs/hzhang/publications/FLAIRS04ZhangH.pdf>`_ "
"Proc. FLAIRS."
msgstr ""

#: ../../modules/naive_bayes.rst:80
msgid "Gaussian Naive Bayes"
msgstr ""

#: ../../modules/naive_bayes.rst:82
msgid ""
":class:`GaussianNB` implements the Gaussian Naive Bayes algorithm for "
"classification. The likelihood of the features is assumed to be Gaussian:"
msgstr ""

#: ../../modules/naive_bayes.rst:89
msgid ""
"The parameters :math:`\\sigma_y` and :math:`\\mu_y` are estimated using "
"maximum likelihood."
msgstr ""

#: ../../modules/naive_bayes.rst:104
msgid "Multinomial Naive Bayes"
msgstr ""

#: ../../modules/naive_bayes.rst:106
msgid ""
":class:`MultinomialNB` implements the naive Bayes algorithm for "
"multinomially distributed data, and is one of the two classic naive Bayes"
" variants used in text classification (where the data are typically "
"represented as word vector counts, although tf-idf vectors are also known"
" to work well in practice). The distribution is parametrized by vectors "
":math:`\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})` for each class "
":math:`y`, where :math:`n` is the number of features (in text "
"classification, the size of the vocabulary) and :math:`\\theta_{yi}` is "
"the probability :math:`P(x_i \\mid y)` of feature :math:`i` appearing in "
"a sample belonging to class :math:`y`."
msgstr ""

#: ../../modules/naive_bayes.rst:117
msgid ""
"The parameters :math:`\\theta_y` is estimated by a smoothed version of "
"maximum likelihood, i.e. relative frequency counting:"
msgstr ""

#: ../../modules/naive_bayes.rst:124
msgid ""
"where :math:`N_{yi} = \\sum_{x \\in T} x_i` is the number of times "
"feature :math:`i` appears in a sample of class :math:`y` in the training "
"set :math:`T`, and :math:`N_{y} = \\sum_{i=1}^{|T|} N_{yi}` is the total "
"count of all features for class :math:`y`."
msgstr ""

#: ../../modules/naive_bayes.rst:130
msgid ""
"The smoothing priors :math:`\\alpha \\ge 0` accounts for features not "
"present in the learning samples and prevents zero probabilities in "
"further computations. Setting :math:`\\alpha = 1` is called Laplace "
"smoothing, while :math:`\\alpha < 1` is called Lidstone smoothing."
msgstr ""

#: ../../modules/naive_bayes.rst:140
msgid "Bernoulli Naive Bayes"
msgstr ""

#: ../../modules/naive_bayes.rst:142
msgid ""
":class:`BernoulliNB` implements the naive Bayes training and "
"classification algorithms for data that is distributed according to "
"multivariate Bernoulli distributions; i.e., there may be multiple "
"features but each one is assumed to be a binary-valued (Bernoulli, "
"boolean) variable. Therefore, this class requires samples to be "
"represented as binary-valued feature vectors; if handed any other kind of"
" data, a ``BernoulliNB`` instance may binarize its input (depending on "
"the ``binarize`` parameter)."
msgstr ""

#: ../../modules/naive_bayes.rst:150
msgid "The decision rule for Bernoulli naive Bayes is based on"
msgstr ""

#: ../../modules/naive_bayes.rst:156
msgid ""
"which differs from multinomial NB's rule in that it explicitly penalizes "
"the non-occurrence of a feature :math:`i` that is an indicator for class "
":math:`y`, where the multinomial variant would simply ignore a non-"
"occurring feature."
msgstr ""

#: ../../modules/naive_bayes.rst:161
msgid ""
"In the case of text classification, word occurrence vectors (rather than "
"word count vectors) may be used to train and use this classifier. "
"``BernoulliNB`` might perform better on some datasets, especially those "
"with shorter documents. It is advisable to evaluate both models, if time "
"permits."
msgstr ""

#: ../../modules/naive_bayes.rst:168
msgid ""
"C.D. Manning, P. Raghavan and H. SchÃ¼tze (2008). Introduction to "
"Information Retrieval. Cambridge University Press, pp. 234-265."
msgstr ""

#: ../../modules/naive_bayes.rst:171
msgid ""
"A. McCallum and K. Nigam (1998). `A comparison of event models for Naive "
"Bayes text classification. "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.46.1529>`_ Proc."
" AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48."
msgstr ""

#: ../../modules/naive_bayes.rst:176
msgid ""
"V. Metsis, I. Androutsopoulos and G. Paliouras (2006). `Spam filtering "
"with Naive Bayes -- Which Naive Bayes? "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.5542>`_ 3rd "
"Conf. on Email and Anti-Spam (CEAS)."
msgstr ""

#: ../../modules/naive_bayes.rst:183
msgid "Out-of-core naive Bayes model fitting"
msgstr ""

#: ../../modules/naive_bayes.rst:185
msgid ""
"Naive Bayes models can be used to tackle large scale classification "
"problems for which the full training set might not fit in memory. To "
"handle this case, :class:`MultinomialNB`, :class:`BernoulliNB`, and "
":class:`GaussianNB` expose a ``partial_fit`` method that can be used "
"incrementally as done with other classifiers as demonstrated in "
":ref:`example_applications_plot_out_of_core_classification.py`. Both "
"discrete classifiers support sample weighting; :class:`GaussianNB` does "
"not."
msgstr ""

#: ../../modules/naive_bayes.rst:193
msgid ""
"Contrary to the ``fit`` method, the first call to ``partial_fit`` needs "
"to be passed the list of all the expected class labels."
msgstr ""

#: ../../modules/naive_bayes.rst:196
msgid ""
"For an overview of available strategies in scikit-learn, see also the "
":ref:`out-of-core learning <scaling_strategies>` documentation."
msgstr ""

#: ../../modules/naive_bayes.rst:201
msgid ""
"The ``partial_fit`` method call of naive Bayes models introduces some "
"computational overhead. It is recommended to use data chunk sizes that "
"are as large as possible, that is as the available RAM allows."
msgstr ""

