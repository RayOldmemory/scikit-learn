# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/neural_networks.rst:5
msgid "Neural network models (unsupervised)"
msgstr ""

#: ../../modules/neural_networks.rst:13
msgid "Restricted Boltzmann machines"
msgstr ""

#: ../../modules/neural_networks.rst:15
msgid ""
"Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature "
"learners based on a probabilistic model. The features extracted by an RBM"
" or a hierarchy of RBMs often give good results when fed into a linear "
"classifier such as a linear SVM or a perceptron."
msgstr ""

#: ../../modules/neural_networks.rst:20
msgid ""
"The model makes assumptions regarding the distribution of inputs. At the "
"moment, scikit-learn only provides :class:`BernoulliRBM`, which assumes "
"the inputs are either binary values or values between 0 and 1, each "
"encoding the probability that the specific feature would be turned on."
msgstr ""

#: ../../modules/neural_networks.rst:25
msgid ""
"The RBM tries to maximize the likelihood of the data using a particular "
"graphical model. The parameter learning algorithm used (:ref:`Stochastic "
"Maximum Likelihood <sml>`) prevents the representations from straying far"
" from the input data, which makes them capture interesting regularities, "
"but makes the model less useful for small datasets, and usually not "
"useful for density estimation."
msgstr ""

#: ../../modules/neural_networks.rst:32
msgid ""
"The method gained popularity for initializing deep neural networks with "
"the weights of independent RBMs. This method is known as unsupervised "
"pre-training."
msgstr ""

#: ../../modules/neural_networks.rst
msgid "Examples:"
msgstr ""

#: ../../modules/neural_networks.rst:42
msgid ":ref:`example_neural_networks_plot_rbm_logistic_classification.py`"
msgstr ""

#: ../../modules/neural_networks.rst:46
msgid "Graphical model and parametrization"
msgstr ""

#: ../../modules/neural_networks.rst:48
msgid "The graphical model of an RBM is a fully-connected bipartite graph."
msgstr ""

#: ../../modules/neural_networks.rst:53
msgid ""
"The nodes are random variables whose states depend on the state of the "
"other nodes they are connected to. The model is therefore parameterized "
"by the weights of the connections, as well as one intercept (bias) term "
"for each visible and hidden unit, ommited from the image for simplicity."
msgstr ""

#: ../../modules/neural_networks.rst:58
msgid "The energy function measures the quality of a joint assignment:"
msgstr ""

#: ../../modules/neural_networks.rst:65
msgid ""
"In the formula above, :math:`\\mathbf{b}` and :math:`\\mathbf{c}` are the"
" intercept vectors for the visible and hidden layers, respectively. The "
"joint probability of the model is defined in terms of the energy:"
msgstr ""

#: ../../modules/neural_networks.rst:74
msgid ""
"The word *restricted* refers to the bipartite structure of the model, "
"which prohibits direct interaction between hidden units, or between "
"visible units. This means that the following conditional independencies "
"are assumed:"
msgstr ""

#: ../../modules/neural_networks.rst:83
msgid ""
"The bipartite structure allows for the use of efficient block Gibbs "
"sampling for inference."
msgstr ""

#: ../../modules/neural_networks.rst:87
msgid "Bernoulli Restricted Boltzmann machines"
msgstr ""

#: ../../modules/neural_networks.rst:89
msgid ""
"In the :class:`BernoulliRBM`, all units are binary stochastic units. This"
" means that the input data should either be binary, or real-valued "
"between 0 and 1 signifying the probability that the visible unit would "
"turn on or off. This is a good model for character recognition, where the"
" interest is on which pixels are active and which aren't. For images of "
"natural scenes it no longer fits because of background, depth and the "
"tendency of neighbouring pixels to take the same values."
msgstr ""

#: ../../modules/neural_networks.rst:97
msgid ""
"The conditional probability distribution of each unit is given by the "
"logistic sigmoid activation function of the input it receives:"
msgstr ""

#: ../../modules/neural_networks.rst:105
msgid "where :math:`\\sigma` is the logistic sigmoid function:"
msgstr ""

#: ../../modules/neural_networks.rst:114
msgid "Stochastic Maximum Likelihood learning"
msgstr ""

#: ../../modules/neural_networks.rst:116
msgid ""
"The training algorithm implemented in :class:`BernoulliRBM` is known as "
"Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence "
"(PCD). Optimizing maximum likelihood directly is infeasible because of "
"the form of the data likelihood:"
msgstr ""

#: ../../modules/neural_networks.rst:125
msgid ""
"For simplicity the equation above is written for a single training "
"example. The gradient with respect to the weights is formed of two terms "
"corresponding to the ones above. They are usually known as the positive "
"gradient and the negative gradient, because of their respective signs.  "
"In this implementation, the gradients are estimated over mini-batches of "
"samples."
msgstr ""

#: ../../modules/neural_networks.rst:131
msgid ""
"In maximizing the log-likelihood, the positive gradient makes the model "
"prefer hidden states that are compatible with the observed training data."
" Because of the bipartite structure of RBMs, it can be computed "
"efficiently. The negative gradient, however, is intractable. Its goal is "
"to lower the energy of joint states that the model prefers, therefore "
"making it stay true to the data. It can be approximated by Markov chain "
"Monte Carlo using block Gibbs sampling by iteratively sampling each of "
":math:`v` and :math:`h` given the other, until the chain mixes. Samples "
"generated in this way are sometimes refered as fantasy particles. This is"
" inefficient and it is difficult to determine whether the Markov chain "
"mixes."
msgstr ""

#: ../../modules/neural_networks.rst:142
msgid ""
"The Contrastive Divergence method suggests to stop the chain after a "
"small number of iterations, :math:`k`, usually even 1. This method is "
"fast and has low variance, but the samples are far from the model "
"distribution."
msgstr ""

#: ../../modules/neural_networks.rst:146
msgid ""
"Persistent Contrastive Divergence addresses this. Instead of starting a "
"new chain each time the gradient is needed, and performing only one Gibbs"
" sampling step, in PCD we keep a number of chains (fantasy particles) "
"that are updated :math:`k` Gibbs steps after each weight update. This "
"allows the particles to explore the space more thoroughly."
msgstr ""

#: ../../modules/neural_networks.rst
msgid "References:"
msgstr ""

#: ../../modules/neural_networks.rst:154
msgid ""
"`\"A fast learning algorithm for deep belief nets\" "
"<http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf>`_ G. Hinton, S. "
"Osindero, Y.-W. Teh, 2006"
msgstr ""

#: ../../modules/neural_networks.rst:158
msgid ""
"`\"Training Restricted Boltzmann Machines using Approximations to the "
"Likelihood Gradient\" <http://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf>`_ "
"T. Tieleman, 2008"
msgstr ""

