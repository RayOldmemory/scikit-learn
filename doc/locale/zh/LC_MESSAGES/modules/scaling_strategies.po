# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/scaling_strategies.rst:5
msgid "Strategies to scale computationally: bigger data"
msgstr ""

#: ../../modules/scaling_strategies.rst:7
msgid ""
"For some applications the amount of examples, features (or both) and/or "
"the speed at which they need to be processed are challenging for "
"traditional approaches. In these cases scikit-learn has a number of "
"options you can consider to make your system scale."
msgstr ""

#: ../../modules/scaling_strategies.rst:13
msgid "Scaling with instances using out-of-core learning"
msgstr ""

#: ../../modules/scaling_strategies.rst:15
msgid ""
"Out-of-core (or \"external memory\") learning is a technique used to "
"learn from data that cannot fit in a computer's main memory (RAM)."
msgstr ""

#: ../../modules/scaling_strategies.rst:18
msgid "Here is sketch of a system designed to achieve this goal:"
msgstr ""

#: ../../modules/scaling_strategies.rst:20
msgid "a way to stream instances"
msgstr ""

#: ../../modules/scaling_strategies.rst:21
msgid "a way to extract features from instances"
msgstr ""

#: ../../modules/scaling_strategies.rst:22
msgid "an incremental algorithm"
msgstr ""

#: ../../modules/scaling_strategies.rst:25
msgid "Streaming instances"
msgstr ""

#: ../../modules/scaling_strategies.rst:26
msgid ""
"Basically, 1. may be a reader that yields instances from files on a hard "
"drive, a database, from a network stream etc. However, details on how to "
"achieve this are beyond the scope of this documentation."
msgstr ""

#: ../../modules/scaling_strategies.rst:31
msgid "Extracting features"
msgstr ""

#: ../../modules/scaling_strategies.rst:32
msgid ""
"\\2. could be any relevant way to extract features among the different "
":ref:`feature extraction <feature_extraction>` methods supported by "
"scikit-learn. However, when working with data that needs vectorization "
"and where the set of features or values is not known in advance one "
"should take explicit care. A good example is text classification where "
"unknown terms are likely to be found during training. It is possible to "
"use a statefull vectorizer if making multiple passes over the data is "
"reasonable from an application point of view. Otherwise, one can turn up "
"the difficulty by using a stateless feature extractor. Currently the "
"preferred way to do this is to use the so-called :ref:`hashing "
"trick<feature_hashing>` as implemented by "
":class:`sklearn.feature_extraction.FeatureHasher` for datasets with "
"categorical variables represented as list of Python dicts or "
":class:`sklearn.feature_extraction.text.HashingVectorizer` for text "
"documents."
msgstr ""

#: ../../modules/scaling_strategies.rst:47
msgid "Incremental learning"
msgstr ""

#: ../../modules/scaling_strategies.rst:48
msgid ""
"Finally, for 3. we have a number of options inside scikit-learn. Although"
" all algorithms cannot learn incrementally (i.e. without seeing all the "
"instances at once), all estimators implementing the ``partial_fit`` API "
"are candidates. Actually, the ability to learn incrementally from a mini-"
"batch of instances (sometimes called \"online learning\") is key to out-"
"of-core learning as it guarantees that at any given time there will be "
"only a small amount of instances in the main memory. Choosing a good size"
" for the mini-batch that balances relevancy and memory footprint could "
"involve some tuning [1]_."
msgstr ""

#: ../../modules/scaling_strategies.rst:57
msgid "Here is a list of incremental estimators for different tasks:"
msgstr ""

#: ../../modules/scaling_strategies.rst:63
msgid "Classification"
msgstr ""

#: ../../modules/scaling_strategies.rst:60
msgid ":class:`sklearn.naive_bayes.MultinomialNB`"
msgstr ""

#: ../../modules/scaling_strategies.rst:61
msgid ":class:`sklearn.naive_bayes.BernoulliNB`"
msgstr ""

#: ../../modules/scaling_strategies.rst:62
msgid ":class:`sklearn.linear_model.Perceptron`"
msgstr ""

#: ../../modules/scaling_strategies.rst:63
msgid ":class:`sklearn.linear_model.SGDClassifier`"
msgstr ""

#: ../../modules/scaling_strategies.rst:64
msgid ":class:`sklearn.linear_model.PassiveAggressiveClassifier`"
msgstr ""

#: ../../modules/scaling_strategies.rst:66
msgid "Regression"
msgstr ""

#: ../../modules/scaling_strategies.rst:66
msgid ":class:`sklearn.linear_model.SGDRegressor`"
msgstr ""

#: ../../modules/scaling_strategies.rst:67
msgid ":class:`sklearn.linear_model.PassiveAggressiveRegressor`"
msgstr ""

#: ../../modules/scaling_strategies.rst:68
msgid "Clustering"
msgstr ""

#: ../../modules/scaling_strategies.rst:69
#: ../../modules/scaling_strategies.rst:73
msgid ":class:`sklearn.cluster.MiniBatchKMeans`"
msgstr ""

#: ../../modules/scaling_strategies.rst:73
msgid "Decomposition / feature Extraction"
msgstr ""

#: ../../modules/scaling_strategies.rst:71
msgid ":class:`sklearn.decomposition.MiniBatchDictionaryLearning`"
msgstr ""

#: ../../modules/scaling_strategies.rst:72
msgid ":class:`sklearn.decomposition.IncrementalPCA`"
msgstr ""

#: ../../modules/scaling_strategies.rst:75
msgid ""
"For classification, a somewhat important thing to note is that although a"
" stateless feature extraction routine may be able to cope with new/unseen"
" attributes, the incremental learner itself may be unable to cope with "
"new/unseen targets classes. In this case you have to pass all the "
"possible classes to the first ``partial_fit`` call using the ``classes=``"
" parameter."
msgstr ""

#: ../../modules/scaling_strategies.rst:81
msgid ""
"Another aspect to consider when choosing a proper algorithm is that all "
"of them don't put the same importance on each example over time. Namely, "
"the ``Perceptron`` is still sensitive to badly labeled examples even "
"after many examples whereas the ``SGD*`` and ``PassiveAggressive*`` "
"families are more robust to this kind of artifacts. Conversely, the later"
" also tend to give less importance to remarkably different, yet properly "
"labeled examples when they come late in the stream as their learning rate"
" decreases over time."
msgstr ""

#: ../../modules/scaling_strategies.rst:90
msgid "Examples"
msgstr ""

#: ../../modules/scaling_strategies.rst:91
msgid ""
"Finally, we have a full-fledged example of "
":ref:`example_applications_plot_out_of_core_classification.py`. It is "
"aimed at providing a starting point for people wanting to build out-of-"
"core learning systems and demonstrates most of the notions discussed "
"above."
msgstr ""

#: ../../modules/scaling_strategies.rst:96
msgid ""
"Furthermore, it also shows the evolution of the performance of different "
"algorithms with the number of processed examples."
msgstr ""

#: ../../modules/scaling_strategies.rst:104
msgid "accuracy_over_time"
msgstr ""

#: ../../modules/scaling_strategies.rst:105
msgid ""
"Now looking at the computation time of the different parts, we see that "
"the vectorization is much more expensive than learning itself. From the "
"different algorithms, ``MultinomialNB`` is the most expensive, but its "
"overhead can be mitigated by increasing the size of the mini-batches "
"(exercise: change ``minibatch_size`` to 100 and 10000 in the program and "
"compare)."
msgstr ""

#: ../../modules/scaling_strategies.rst:117
msgid "computation_time"
msgstr ""

#: ../../modules/scaling_strategies.rst:119
msgid "Notes"
msgstr ""

#: ../../modules/scaling_strategies.rst:121
msgid ""
"Depending on the algorithm the mini-batch size can influence results or "
"not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly online "
"and are not affected by batch size. Conversely, MiniBatchKMeans "
"convergence rate is affected by the batch size. Also, its memory "
"footprint can vary dramatically with batch size."
msgstr ""

