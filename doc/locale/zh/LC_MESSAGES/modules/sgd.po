# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../modules/sgd.rst:5
msgid "Stochastic Gradient Descent"
msgstr ""

#: ../../modules/sgd.rst:9
msgid ""
"**Stochastic Gradient Descent (SGD)** is a simple yet very efficient "
"approach to discriminative learning of linear classifiers under convex "
"loss functions such as (linear) `Support Vector Machines "
"<http://en.wikipedia.org/wiki/Support_vector_machine>`_ and `Logistic "
"Regression <http://en.wikipedia.org/wiki/Logistic_regression>`_. Even "
"though SGD has been around in the machine learning community for a long "
"time, it has received a considerable amount of attention just recently in"
" the context of large-scale learning."
msgstr ""

#: ../../modules/sgd.rst:18
msgid ""
"SGD has been successfully applied to large-scale and sparse machine "
"learning problems often encountered in text classification and natural "
"language processing.  Given that the data is sparse, the classifiers in "
"this module easily scale to problems with more than 10^5 training "
"examples and more than 10^5 features."
msgstr ""

#: ../../modules/sgd.rst:24
msgid "The advantages of Stochastic Gradient Descent are:"
msgstr ""

#: ../../modules/sgd.rst:26
msgid "Efficiency."
msgstr ""

#: ../../modules/sgd.rst:28
msgid "Ease of implementation (lots of opportunities for code tuning)."
msgstr ""

#: ../../modules/sgd.rst:30
msgid "The disadvantages of Stochastic Gradient Descent include:"
msgstr ""

#: ../../modules/sgd.rst:32
msgid ""
"SGD requires a number of hyperparameters such as the regularization "
"parameter and the number of iterations."
msgstr ""

#: ../../modules/sgd.rst:35
msgid "SGD is sensitive to feature scaling."
msgstr ""

#: ../../modules/sgd.rst:38
msgid "Classification"
msgstr ""

#: ../../modules/sgd.rst:42
msgid ""
"Make sure you permute (shuffle) your training data before fitting the "
"model or use ``shuffle=True`` to shuffle after each iterations."
msgstr ""

#: ../../modules/sgd.rst:45
msgid ""
"The class :class:`SGDClassifier` implements a plain stochastic gradient "
"descent learning routine which supports different loss functions and "
"penalties for classification."
msgstr ""

#: ../../modules/sgd.rst:54
msgid ""
"As other classifiers, SGD has to be fitted with two arrays: an array X of"
" size [n_samples, n_features] holding the training samples, and an array "
"Y of size [n_samples] holding the target values (class labels) for the "
"training samples::"
msgstr ""

#: ../../modules/sgd.rst:71
msgid "After being fitted, the model can then be used to predict new values::"
msgstr ""

#: ../../modules/sgd.rst:76
msgid ""
"SGD fits a linear model to the training data. The member ``coef_`` holds "
"the model parameters::"
msgstr ""

#: ../../modules/sgd.rst:82
msgid "Member ``intercept_`` holds the intercept (aka offset or bias)::"
msgstr ""

#: ../../modules/sgd.rst:87
msgid ""
"Whether or not the model should use an intercept, i.e. a biased "
"hyperplane, is controlled by the parameter ``fit_intercept``."
msgstr ""

#: ../../modules/sgd.rst:90
msgid ""
"To get the signed distance to the hyperplane use "
":meth:`SGDClassifier.decision_function`::"
msgstr ""

#: ../../modules/sgd.rst:95
msgid ""
"The concrete loss function can be set via the ``loss`` parameter. "
":class:`SGDClassifier` supports the following loss functions:"
msgstr ""

#: ../../modules/sgd.rst:98
msgid "``loss=\"hinge\"``: (soft-margin) linear Support Vector Machine,"
msgstr ""

#: ../../modules/sgd.rst:99
msgid "``loss=\"modified_huber\"``: smoothed hinge loss,"
msgstr ""

#: ../../modules/sgd.rst:100
msgid "``loss=\"log\"``: logistic regression,"
msgstr ""

#: ../../modules/sgd.rst:101
msgid "and all regression losses below."
msgstr ""

#: ../../modules/sgd.rst:103
msgid ""
"The first two loss functions are lazy, they only update the model "
"parameters if an example violates the margin constraint, which makes "
"training very efficient and may result in sparser models, even when L2 "
"penalty is used."
msgstr ""

#: ../../modules/sgd.rst:108
msgid ""
"Using ``loss=\"log\"`` or ``loss=\"modified_huber\"`` enables the "
"``predict_proba`` method, which gives a vector of probability estimates "
":math:`P(y|x)` per sample :math:`x`::"
msgstr ""

#: ../../modules/sgd.rst:116
msgid ""
"The concrete penalty can be set via the ``penalty`` parameter. SGD "
"supports the following penalties:"
msgstr ""

#: ../../modules/sgd.rst:119
msgid "``penalty=\"l2\"``: L2 norm penalty on ``coef_``."
msgstr ""

#: ../../modules/sgd.rst:120
msgid "``penalty=\"l1\"``: L1 norm penalty on ``coef_``."
msgstr ""

#: ../../modules/sgd.rst:121
msgid ""
"``penalty=\"elasticnet\"``: Convex combination of L2 and L1; ``(1 - "
"l1_ratio) * L2 + l1_ratio * L1``."
msgstr ""

#: ../../modules/sgd.rst:124
msgid ""
"The default setting is ``penalty=\"l2\"``. The L1 penalty leads to sparse"
" solutions, driving most coefficients to zero. The Elastic Net solves "
"some deficiencies of the L1 penalty in the presence of highly correlated "
"attributes. The parameter ``l1_ratio`` controls the convex combination of"
" L1 and L2 penalty."
msgstr ""

#: ../../modules/sgd.rst:130
msgid ""
":class:`SGDClassifier` supports multi-class classification by combining "
"multiple binary classifiers in a \"one versus all\" (OVA) scheme. For "
"each of the :math:`K` classes, a binary classifier is learned that "
"discriminates between that and all other :math:`K-1` classes. At testing "
"time, we compute the confidence score (i.e. the signed distances to the "
"hyperplane) for each classifier and choose the class with the highest "
"confidence. The Figure below illustrates the OVA approach on the iris "
"dataset.  The dashed lines represent the three OVA classifiers; the "
"background colors show the decision surface induced by the three "
"classifiers."
msgstr ""

#: ../../modules/sgd.rst:145
msgid ""
"In the case of multi-class classification ``coef_`` is a two-"
"dimensionally array of ``shape=[n_classes, n_features]`` and "
"``intercept_`` is a one dimensional array of ``shape=[n_classes]``. The "
"i-th row of ``coef_`` holds the weight vector of the OVA classifier for "
"the i-th class; classes are indexed in ascending order (see attribute "
"``classes_``). Note that, in principle, since they allow to create a "
"probability model, ``loss=\"log\"`` and ``loss=\"modified_huber\"`` are "
"more suitable for one-vs-all classification."
msgstr ""

#: ../../modules/sgd.rst:154
msgid ""
":class:`SGDClassifier` supports both weighted classes and weighted "
"instances via the fit parameters ``class_weight`` and ``sample_weight``. "
"See the examples below and the doc string of :meth:`SGDClassifier.fit` "
"for further information."
msgstr ""

#: ../../modules/sgd.rst
msgid "Examples:"
msgstr ""

#: ../../modules/sgd.rst:161
msgid ":ref:`example_linear_model_plot_sgd_separating_hyperplane.py`,"
msgstr ""

#: ../../modules/sgd.rst:162
msgid ":ref:`example_linear_model_plot_sgd_iris.py`"
msgstr ""

#: ../../modules/sgd.rst:163
msgid ":ref:`example_linear_model_plot_sgd_weighted_samples.py`"
msgstr ""

#: ../../modules/sgd.rst:164
msgid ":ref:`example_linear_model_plot_sgd_comparison.py`"
msgstr ""

#: ../../modules/sgd.rst:165
msgid ""
":ref:`example_svm_plot_separating_hyperplane_unbalanced.py` (See the "
"`Note`)"
msgstr ""

#: ../../modules/sgd.rst:167
msgid ""
":class:`SGDClassifier` supports averaged SGD (ASGD). Averaging can be "
"enabled by setting ```average=True```. ASGD works by averaging the "
"coefficients of the plain SGD over each iteration over a sample. When "
"using ASGD the learning rate can be larger and even constant leading on "
"some datasets to a speed up in training time."
msgstr ""

#: ../../modules/sgd.rst:173
msgid ""
"For classification with a logistic loss, another variant of SGD with an "
"averaging strategy is available with Stochastic Average Gradient (SAG) "
"algorithm, available as a solver in :class:`LogisticRegression`."
msgstr ""

#: ../../modules/sgd.rst:178
msgid "Regression"
msgstr ""

#: ../../modules/sgd.rst:180
msgid ""
"The class :class:`SGDRegressor` implements a plain stochastic gradient "
"descent learning routine which supports different loss functions and "
"penalties to fit linear regression models. :class:`SGDRegressor` is well "
"suited for regression problems with a large number of training samples (>"
" 10.000), for other problems we recommend :class:`Ridge`, :class:`Lasso`,"
" or :class:`ElasticNet`."
msgstr ""

#: ../../modules/sgd.rst:187
msgid ""
"The concrete loss function can be set via the ``loss`` parameter. "
":class:`SGDRegressor` supports the following loss functions:"
msgstr ""

#: ../../modules/sgd.rst:190
msgid "``loss=\"squared_loss\"``: Ordinary least squares,"
msgstr ""

#: ../../modules/sgd.rst:191
msgid "``loss=\"huber\"``: Huber loss for robust regression,"
msgstr ""

#: ../../modules/sgd.rst:192
msgid "``loss=\"epsilon_insensitive\"``: linear Support Vector Regression."
msgstr ""

#: ../../modules/sgd.rst:194
msgid ""
"The Huber and epsilon-insensitive loss functions can be used for robust "
"regression. The width of the insensitive region has to be specified via "
"the parameter ``epsilon``. This parameter depends on the scale of the "
"target variables."
msgstr ""

#: ../../modules/sgd.rst:199
msgid ""
":class:`SGDRegressor` supports averaged SGD as :class:`SGDClassifier`. "
"Averaging can be enabled by setting ```average=True```."
msgstr ""

#: ../../modules/sgd.rst:202
msgid ""
"For regression with a squared loss and a l2 penalty, another variant of "
"SGD with an averaging strategy is available with Stochastic Average "
"Gradient (SAG) algorithm, available as a solver in :class:`Ridge`."
msgstr ""

#: ../../modules/sgd.rst:208
msgid "Stochastic Gradient Descent for sparse data"
msgstr ""

#: ../../modules/sgd.rst:210
msgid ""
"The sparse implementation produces slightly different results than the "
"dense implementation due to a shrunk learning rate for the intercept."
msgstr ""

#: ../../modules/sgd.rst:214
msgid ""
"There is built-in support for sparse data given in any matrix in a format"
" supported by `scipy.sparse "
"<http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.html>`_."
" For maximum efficiency, however, use the CSR matrix format as defined in"
" `scipy.sparse.csr_matrix "
"<http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>`_."
msgstr ""

#: ../../modules/sgd.rst:221
msgid ":ref:`example_text_document_classification_20newsgroups.py`"
msgstr ""

#: ../../modules/sgd.rst:224
msgid "Complexity"
msgstr ""

#: ../../modules/sgd.rst:226
msgid ""
"The major advantage of SGD is its efficiency, which is basically linear "
"in the number of training examples. If X is a matrix of size (n, p) "
"training has a cost of :math:`O(k n \\bar p)`, where k is the number of "
"iterations (epochs) and :math:`\\bar p` is the average number of non-zero"
" attributes per sample."
msgstr ""

#: ../../modules/sgd.rst:232
msgid ""
"Recent theoretical results, however, show that the runtime to get some "
"desired optimization accuracy does not increase as the training set size "
"increases."
msgstr ""

#: ../../modules/sgd.rst:236
msgid "Tips on Practical Use"
msgstr ""

#: ../../modules/sgd.rst:238
msgid ""
"Stochastic Gradient Descent is sensitive to feature scaling, so it is "
"highly recommended to scale your data. For example, scale each attribute "
"on the input vector X to [0,1] or [-1,+1], or standardize it to have mean"
" 0 and variance 1. Note that the *same* scaling must be applied to the "
"test vector to obtain meaningful results. This can be easily done using "
":class:`StandardScaler`::"
msgstr ""

#: ../../modules/sgd.rst:251
msgid ""
"If your attributes have an intrinsic scale (e.g. word frequencies or "
"indicator features) scaling is not needed."
msgstr ""

#: ../../modules/sgd.rst:254
msgid ""
"Finding a reasonable regularization term :math:`\\alpha` is best done "
"using :class:`GridSearchCV`, usually in the range "
"``10.0**-np.arange(1,7)``."
msgstr ""

#: ../../modules/sgd.rst:258
msgid ""
"Empirically, we found that SGD converges after observing approx. 10^6 "
"training samples. Thus, a reasonable first guess for the number of "
"iterations is ``n_iter = np.ceil(10**6 / n)``, where ``n`` is the size of"
" the training set."
msgstr ""

#: ../../modules/sgd.rst:263
msgid ""
"If you apply SGD to features extracted using PCA we found that it is "
"often wise to scale the feature values by some constant `c` such that the"
" average L2 norm of the training data equals one."
msgstr ""

#: ../../modules/sgd.rst:267
msgid ""
"We found that Averaged SGD works best with a larger number of features "
"and a higher eta0"
msgstr ""

#: ../../modules/sgd.rst
msgid "References:"
msgstr ""

#: ../../modules/sgd.rst:272
msgid ""
"`\"Efficient BackProp\" <yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>`_ "
"Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of "
"the Trade 1998."
msgstr ""

#: ../../modules/sgd.rst:279
msgid "Mathematical formulation"
msgstr ""

#: ../../modules/sgd.rst:281
msgid ""
"Given a set of training examples :math:`(x_1, y_1), \\ldots, (x_n, y_n)` "
"where :math:`x_i \\in \\mathbf{R}^n` and :math:`y_i \\in \\{-1,1\\}`, our"
" goal is to learn a linear scoring function :math:`f(x) = w^T x + b` with"
" model parameters :math:`w \\in \\mathbf{R}^m` and intercept :math:`b "
"\\in \\mathbf{R}`. In order to make predictions, we simply look at the "
"sign of :math:`f(x)`. A common choice to find the model parameters is by "
"minimizing the regularized training error given by"
msgstr ""

#: ../../modules/sgd.rst:293
msgid ""
"where :math:`L` is a loss function that measures model (mis)fit and "
":math:`R` is a regularization term (aka penalty) that penalizes model "
"complexity; :math:`\\alpha > 0` is a non-negative hyperparameter."
msgstr ""

#: ../../modules/sgd.rst:297
msgid "Different choices for :math:`L` entail different classifiers such as"
msgstr ""

#: ../../modules/sgd.rst:299
msgid "Hinge: (soft-margin) Support Vector Machines."
msgstr ""

#: ../../modules/sgd.rst:300
msgid "Log:   Logistic Regression."
msgstr ""

#: ../../modules/sgd.rst:301
msgid "Least-Squares: Ridge Regression."
msgstr ""

#: ../../modules/sgd.rst:302
msgid "Epsilon-Insensitive: (soft-margin) Support Vector Regression."
msgstr ""

#: ../../modules/sgd.rst:304
msgid ""
"All of the above loss functions can be regarded as an upper bound on the "
"misclassification error (Zero-one loss) as shown in the Figure below."
msgstr ""

#: ../../modules/sgd.rst:312
msgid "Popular choices for the regularization term :math:`R` include:"
msgstr ""

#: ../../modules/sgd.rst:314
msgid "L2 norm: :math:`R(w) := \\frac{1}{2} \\sum_{i=1}^{n} w_i^2`,"
msgstr ""

#: ../../modules/sgd.rst:315
msgid ""
"L1 norm: :math:`R(w) := \\sum_{i=1}^{n} |w_i|`, which leads to sparse "
"solutions."
msgstr ""

#: ../../modules/sgd.rst:317
msgid ""
"Elastic Net: :math:`R(w) := \\frac{\\rho}{2} \\sum_{i=1}^{n} w_i^2 + "
"(1-\\rho) \\sum_{i=1}^{n} |w_i|`, a convex combination of L2 and L1, "
"where :math:`\\rho` is given by ``1 - l1_ratio``."
msgstr ""

#: ../../modules/sgd.rst:319
msgid ""
"The Figure below shows the contours of the different regularization terms"
" in the parameter space when :math:`R(w) = 1`."
msgstr ""

#: ../../modules/sgd.rst:328
msgid "SGD"
msgstr ""

#: ../../modules/sgd.rst:330
msgid ""
"Stochastic gradient descent is an optimization method for unconstrained "
"optimization problems. In contrast to (batch) gradient descent, SGD "
"approximates the true gradient of :math:`E(w,b)` by considering a single "
"training example at a time."
msgstr ""

#: ../../modules/sgd.rst:335
msgid ""
"The class :class:`SGDClassifier` implements a first-order SGD learning "
"routine.  The algorithm iterates over the training examples and for each "
"example updates the model parameters according to the update rule given "
"by"
msgstr ""

#: ../../modules/sgd.rst:344
msgid ""
"where :math:`\\eta` is the learning rate which controls the step-size in "
"the parameter space.  The intercept :math:`b` is updated similarly but "
"without regularization."
msgstr ""

#: ../../modules/sgd.rst:348
msgid ""
"The learning rate :math:`\\eta` can be either constant or gradually "
"decaying. For classification, the default learning rate schedule "
"(``learning_rate='optimal'``) is given by"
msgstr ""

#: ../../modules/sgd.rst:356
msgid ""
"where :math:`t` is the time step (there are a total of `n_samples * "
"n_iter` time steps), :math:`t_0` is determined based on a heuristic "
"proposed by Léon Bottou such that the expected initial updates are "
"comparable with the expected size of the weights (this assuming that the "
"norm of the training samples is approx. 1). The exact definition can be "
"found in ``_init_t`` in :class:`BaseSGD`."
msgstr ""

#: ../../modules/sgd.rst:363
msgid ""
"For regression the default learning rate schedule is inverse scaling "
"(``learning_rate='invscaling'``), given by"
msgstr ""

#: ../../modules/sgd.rst:370
msgid ""
"where :math:`eta_0` and :math:`power\\_t` are hyperparameters chosen by "
"the user via ``eta0`` and ``power_t``, resp."
msgstr ""

#: ../../modules/sgd.rst:373
msgid ""
"For a constant learning rate use ``learning_rate='constant'`` and use "
"``eta0`` to specify the learning rate."
msgstr ""

#: ../../modules/sgd.rst:376
msgid ""
"The model parameters can be accessed through the members ``coef_`` and "
"``intercept_``:"
msgstr ""

#: ../../modules/sgd.rst:379
msgid "Member ``coef_`` holds the weights :math:`w`"
msgstr ""

#: ../../modules/sgd.rst:381
msgid "Member ``intercept_`` holds :math:`b`"
msgstr ""

#: ../../modules/sgd.rst:385
msgid ""
"`\"Solving large scale linear prediction problems using stochastic "
"gradient descent algorithms\" "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377>`_ T. "
"Zhang - In Proceedings of ICML '04."
msgstr ""

#: ../../modules/sgd.rst:390
msgid ""
"`\"Regularization and variable selection via the elastic net\" "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696>`_ H. "
"Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 "
"(2), 301-320."
msgstr ""

#: ../../modules/sgd.rst:395
msgid ""
"`\"Towards Optimal One Pass Large Scale Learning with Averaged Stochastic"
" Gradient Descent\" <http://arxiv.org/pdf/1107.2490v2.pdf>`_ Xu, Wei"
msgstr ""

#: ../../modules/sgd.rst:402
msgid "Implementation details"
msgstr ""

#: ../../modules/sgd.rst:404
msgid ""
"The implementation of SGD is influenced by the `Stochastic Gradient SVM "
"<http://leon.bottou.org/projects/sgd>`_  of Léon Bottou. Similar to "
"SvmSGD, the weight vector is represented as the product of a scalar and a"
" vector which allows an efficient weight update in the case of L2 "
"regularization. In the case of sparse feature vectors, the intercept is "
"updated with a smaller learning rate (multiplied by 0.01) to account for "
"the fact that it is updated more frequently. Training examples are picked"
" up sequentially and the learning rate is lowered after each observed "
"example. We adopted the learning rate schedule from Shalev-Shwartz et al."
" 2007. For multi-class classification, a \"one versus all\" approach is "
"used. We use the truncated gradient algorithm proposed by Tsuruoka et al."
" 2009 for L1 regularization (and the Elastic Net). The code is written in"
" Cython."
msgstr ""

#: ../../modules/sgd.rst:420
msgid ""
"`\"Stochastic Gradient Descent\" <http://leon.bottou.org/projects/sgd>`_ "
"L. Bottou - Website, 2010."
msgstr ""

#: ../../modules/sgd.rst:422
msgid ""
"`\"The Tradeoffs of Large Scale Machine Learning\" "
"<http://leon.bottou.org/slides/largescale/lstut.pdf>`_ L. Bottou - "
"Website, 2011."
msgstr ""

#: ../../modules/sgd.rst:424
msgid ""
"`\"Pegasos: Primal estimated sub-gradient solver for svm\" "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513>`_ S. "
"Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07."
msgstr ""

#: ../../modules/sgd.rst:428
msgid ""
"`\"Stochastic gradient descent training for l1-regularized log-linear "
"models with cumulative penalty\" "
"<http://www.aclweb.org/anthology/P/P09/P09-1054.pdf>`_ Y. Tsuruoka, J. "
"Tsujii, S. Ananiadou -  In Proceedings of the AFNLP/ACL '09."
msgstr ""

