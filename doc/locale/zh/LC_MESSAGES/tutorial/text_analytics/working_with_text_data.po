# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2010 - 2014, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2016.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.17\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-02-16 21:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.2.0\n"

#: ../../tutorial/text_analytics/working_with_text_data.rst:5
msgid "Working With Text Data"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:7
msgid ""
"The goal of this guide is to explore some of the main ``scikit-learn`` "
"tools on a single practical task: analysing a collection of text "
"documents (newsgroups posts) on twenty different topics."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:11
msgid "In this section we will see how to:"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:13
msgid "load the file contents and the categories"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:15
msgid "extract feature vectors suitable for machine learning"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:17
msgid "train a linear model to perform categorization"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:19
msgid ""
"use a grid search strategy to find a good configuration of both the "
"feature extraction components and the classifier"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:25
msgid "Tutorial setup"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:27
msgid ""
"To get started with this tutorial, you firstly must have the *scikit-"
"learn* and all of its required dependencies installed."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:30
msgid ""
"Please refer to the :ref:`installation instructions <installation-"
"instructions>` page for more information and for per-system instructions."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:33
msgid "The source of this tutorial can be found within your scikit-learn folder::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:38
msgid "The tutorial folder, should contain the following folders:"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:40
msgid "``*.rst files`` - the source of the tutorial document written with sphinx"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:42
msgid "``data`` - folder to put the datasets used during the tutorial"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:44
msgid "``skeletons`` - sample incomplete scripts for the exercises"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:46
msgid "``solutions`` - solutions of the exercises"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:49
msgid ""
"You can already copy the skeletons into a new folder somewhere on your "
"hard-drive named ``sklearn_tut_workspace`` where you will edit your own "
"files for the exercises while keeping the original skeletons intact::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:56
msgid ""
"Machine Learning algorithms need data. Go to each ``$TUTORIAL_HOME/data``"
" sub-folder and run the ``fetch_data.py`` script from there (after having"
" read them first)."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:60
msgid "For instance::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:68
msgid "Loading the 20 newsgroups dataset"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:70
msgid ""
"The dataset is called \"Twenty Newsgroups\". Here is the official "
"description, quoted from the `website "
"<http://people.csail.mit.edu/jrennie/20Newsgroups/>`_:"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:74
msgid ""
"The 20 Newsgroups data set is a collection of approximately 20,000 "
"newsgroup documents, partitioned (nearly) evenly across 20 different "
"newsgroups. To the best of our knowledge, it was originally collected by "
"Ken Lang, probably for his paper \"Newsweeder: Learning to filter "
"netnews,\" though he does not explicitly mention this collection. The 20 "
"newsgroups collection has become a popular data set for experiments in "
"text applications of machine learning techniques, such as text "
"classification and text clustering."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:83
msgid ""
"In the following we will use the built-in dataset loader for 20 "
"newsgroups from scikit-learn. Alternatively, it is possible to download "
"the dataset manually from the web-site and use the "
":func:`sklearn.datasets.load_files` function by pointing it to the "
"``20news-bydate-train`` subfolder of the uncompressed archive folder."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:89
msgid ""
"In order to get faster execution times for this first example we will "
"work on a partial dataset with only 4 categories out of the 20 available "
"in the dataset::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:96
msgid "We can now load the list of files matching those categories as follows::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:102
msgid ""
"The returned dataset is a ``scikit-learn`` \"bunch\": a simple holder "
"object with fields that can be both accessed as python ``dict`` keys or "
"``object`` attributes for convenience, for instance the ``target_names`` "
"holds the list of the requested category names::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:110
msgid ""
"The files themselves are loaded in memory in the ``data`` attribute. For "
"reference the filenames are also available::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:118
msgid "Let's print the first lines of the first loaded file::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:128
msgid ""
"Supervised learning algorithms will require a category label for each "
"document in the training set. In this case the category is the name of "
"the newsgroup which also happens to be the name of the folder holding the"
" individual documents."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:133
msgid ""
"For speed and space efficiency reasons ``scikit-learn`` loads the target "
"attribute as an array of integers that corresponds to the index of the "
"category name in the ``target_names`` list. The category integer id of "
"each sample is stored in the ``target`` attribute::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:141
msgid "It is possible to get back the category names as follows::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:157
msgid ""
"You can notice that the samples have been shuffled randomly (with a fixed"
" RNG seed): this is useful if you select only the first samples to "
"quickly train a model and get a first idea of the results before re-"
"training on the complete dataset later."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:164
msgid "Extracting features from text files"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:166
msgid ""
"In order to perform machine learning on text documents, we first need to "
"turn the text content into numerical feature vectors."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:173
msgid "Bags of words"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:175
msgid "The most intuitive way to do so is the bags of words representation:"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:177
msgid ""
"assign a fixed integer id to each word occurring in any document of the "
"training set (for instance by building a dictionary from words to integer"
" indices)."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:181
msgid ""
"for each document ``#i``, count the number of occurrences of each word "
"``w`` and store it in ``X[i, j]`` as the value of feature ``#j`` where "
"``j`` is the index of word ``w`` in the dictionary"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:185
msgid ""
"The bags of words representation implies that ``n_features`` is the "
"number of distinct words in the corpus: this number is typically larger "
"that 100,000."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:189
msgid ""
"If ``n_samples == 10000``, storing ``X`` as a numpy array of type float32"
" would require 10000 x 100000 x 4 bytes = **4GB in RAM** which is barely "
"manageable on today's computers."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:193
msgid ""
"Fortunately, **most values in X will be zeros** since for a given "
"document less than a couple thousands of distinct words will be used. For"
" this reason we say that bags of words are typically **high-dimensional "
"sparse datasets**. We can save a lot of memory by only storing the non-"
"zero parts of the feature vectors in memory."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:199
msgid ""
"``scipy.sparse`` matrices are data structures that do exactly this, and "
"``scikit-learn`` has built-in support for these structures."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:204
msgid "Tokenizing text with ``scikit-learn``"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:206
msgid ""
"Text preprocessing, tokenizing and filtering of stopwords are included in"
" a high level component that is able to build a dictionary of features "
"and transform documents to feature vectors::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:215
msgid ""
":class:`CountVectorizer` supports counts of N-grams of words or "
"consequective characters. Once fitted, the vectorizer has built a "
"dictionary of feature indices::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:221
msgid ""
"The index value of a word in the vocabulary is linked to its frequency in"
" the whole training corpus."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:235
msgid "From occurrences to frequencies"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:237
msgid ""
"Occurrence count is a good start but there is an issue: longer documents "
"will have higher average count values than shorter documents, even though"
" they might talk about the same topics."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:241
msgid ""
"To avoid these potential discrepancies it suffices to divide the number "
"of occurrences of each word in a document by the total number of words in"
" the document: these new features are called ``tf`` for Term Frequencies."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:246
msgid ""
"Another refinement on top of tf is to downscale weights for words that "
"occur in many documents in the corpus and are therefore less informative "
"than those that occur only in a smaller portion of the corpus."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:251
msgid ""
"This downscaling is called `tf–idf`_ for \"Term Frequency times Inverse "
"Document Frequency\"."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:257
msgid "Both **tf** and **tf–idf** can be computed as follows::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:265
msgid ""
"In the above example-code, we firstly use the ``fit(..)`` method to fit "
"our estimator to the data and secondly the ``transform(..)`` method to "
"transform our count-matrix to a tf-idf representation. These two steps "
"can be combined to achieve the same end result faster by skipping "
"redundant processing. This is done through using the "
"``fit_transform(..)`` method as shown below, and as mentioned in the note"
" in the previous section::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:280
msgid "Training a classifier"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:282
msgid ""
"Now that we have our features, we can train a classifier to try to "
"predict the category of a post. Let's start with a :ref:`naïve Bayes "
"<naive_bayes>` classifier, which provides a nice baseline for this task. "
"``scikit-learn`` includes several variants of this classifier; the one "
"most suitable for word counts is the multinomial variant::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:292
msgid ""
"To try to predict the outcome on a new document we need to extract the "
"features using almost the same feature extracting chain as before. The "
"difference is that we call ``transform`` instead of ``fit_transform`` on "
"the transformers, since they have already been fit to the training set::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:311
msgid "Building a pipeline"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:313
msgid ""
"In order to make the vectorizer => transformer => classifier easier to "
"work with, ``scikit-learn`` provides a ``Pipeline`` class that behaves "
"like a compound classifier::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:323
msgid ""
"The names ``vect``, ``tfidf`` and ``clf`` (classifier) are arbitrary. We "
"shall see their use in the section on grid search, below. We can now "
"train the model with a single command::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:331
msgid "Evaluation of the performance on the test set"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:333
msgid "Evaluating the predictive accuracy of the model is equally easy::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:343
msgid ""
"I.e., we achieved 83.4% accuracy. Let's see if we can do better with a "
"linear :ref:`support vector machine (SVM) <svm>`, which is widely "
"regarded as one of the best text classification algorithms (although it's"
" also a bit slower than naïve Bayes). We can change the learner by just "
"plugging a different classifier object into our pipeline::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:361
msgid ""
"``scikit-learn`` further provides utilities for more detailed performance"
" analysis of the results::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:385
msgid ""
"As expected the confusion matrix shows that posts from the newsgroups on "
"atheism and christian are more often confused for one another than with "
"computer graphics."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:408
msgid "Parameter tuning using grid search"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:410
msgid ""
"We've already encountered some parameters such as ``use_idf`` in the "
"``TfidfTransformer``. Classifiers tend to have many parameters as well; "
"e.g., ``MultinomialNB`` includes a smoothing parameter ``alpha`` and "
"``SGDClassifier`` has a penalty parameter ``alpha`` and configurable loss"
" and penalty terms in the objective function (see the module "
"documentation, or use the Python ``help`` function, to get a description "
"of these)."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:417
msgid ""
"Instead of tweaking the parameters of the various components of the "
"chain, it is possible to run an exhaustive search of the best parameters "
"on a grid of possible values. We try out all classifiers on either words "
"or bigrams, with or without idf, and with a penalty parameter of either "
"0.01 or 0.001 for the linear SVM::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:429
msgid ""
"Obviously, such an exhaustive search can be expensive. If we have "
"multiple CPU cores at our disposal, we can tell the grid searcher to try "
"these eight parameter combinations in parallel with the ``n_jobs`` "
"parameter. If we give this parameter a value of ``-1``, grid search will "
"detect how many cores are installed and uses them all::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:437
msgid ""
"The grid search instance behaves like a normal ``scikit-learn`` model. "
"Let's perform the search on a smaller subset of the training data to "
"speed up the computation::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:443
msgid ""
"The result of calling ``fit`` on a ``GridSearchCV`` object is a "
"classifier that we can use to ``predict``::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:449
msgid ""
"but otherwise, it's a pretty large and clumsy object. We can, however, "
"get the optimal parameters out by inspecting the object's "
"``grid_scores_`` attribute, which is a list of parameters/score pairs. To"
" get the best scoring attributes, we can do::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:473
msgid "Exercises"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:475
msgid ""
"To do the exercises, copy the content of the 'skeletons' folder as a new "
"folder named 'workspace'::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:480
msgid ""
"You can then edit the content of the workspace without fear of loosing "
"the original exercise instructions."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:483
msgid "Then fire an ipython shell and run the work-in-progress script with::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:487
#, python-format
msgid ""
"If an exception is triggered, use ``%debug`` to fire-up a post mortem "
"ipdb session."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:490
msgid "Refine the implementation and iterate until the exercise is solved."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:492
msgid ""
"**For each exercise, the skeleton file provides all the necessary import "
"statements, boilerplate code to load the data and sample code to evaluate"
" the predictive accurracy of the model.**"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:498
msgid "Exercise 1: Language identification"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:500
msgid ""
"Write a text classification pipeline using a custom preprocessor and "
"``CharNGramAnalyzer`` using data from Wikipedia articles as training set."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:503
msgid "Evaluate the performance on some held out test set."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:505
#: ../../tutorial/text_analytics/working_with_text_data.rst:520
msgid "ipython command line::"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:511
msgid "Exercise 2: Sentiment Analysis on movie reviews"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:513
msgid ""
"Write a text classification pipeline to classify movie reviews as either "
"positive or negative."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:516
msgid "Find a good set of parameters using grid search."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:518
msgid "Evaluate the performance on a held out test set."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:526
msgid "Exercise 3: CLI text classification utility"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:528
msgid ""
"Using the results of the previous exercises and the ``cPickle`` module of"
" the standard library, write a command line utility that detects the "
"language of some text provided on ``stdin`` and estimate the polarity "
"(positive or negative) if the text is written in English."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:534
msgid ""
"Bonus point if the utility is able to give a confidence level for its "
"predictions."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:539
msgid "Where to from here"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:541
msgid ""
"Here are a few suggestions to help further your scikit-learn intuition "
"upon the completion of this tutorial:"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:545
msgid ""
"Try playing around with the ``analyzer`` and ``token normalisation`` "
"under :class:`CountVectorizer`"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:548
msgid ""
"If you don't have labels, try using :ref:`Clustering "
"<example_text_document_clustering.py>` on your problem."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:552
msgid ""
"If you have multiple labels per document, e.g categories, have a look at "
"the :ref:`Multiclass and multilabel section <multiclass>`"
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:555
msgid ""
"Try using :ref:`Truncated SVD <LSA>` for `latent semantic analysis "
"<http://en.wikipedia.org/wiki/Latent_semantic_analysis>`_."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:558
msgid ""
"Have a look at using :ref:`Out-of-core Classification "
"<example_applications_plot_out_of_core_classification.py>` to learn from "
"data that would not fit into the computer main memory."
msgstr ""

#: ../../tutorial/text_analytics/working_with_text_data.rst:563
msgid ""
"Have a look at the :ref:`Hashing Vectorizer <hashing_vectorizer>` as a "
"memory efficient alternative to :class:`CountVectorizer`."
msgstr ""

